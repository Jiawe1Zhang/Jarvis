import os
from typing import Any, Dict, List, Optional

from openai import OpenAI

from utils import ToolCall, log_title
from utils.ui import BaseUI
from utils.tracer import RunTracer
from utils.session_store import SessionStore


class ChatOpenAI:
    """
    Minimal wrapper around the OpenAI Chat Completions API with tool support.
    """

    def __init__(
        self,
        model: str,
        system_prompt: str = "",
        tools: Optional[List[Dict[str, Any]]] = None,
        context: str = "",
        base_url: Optional[str] = None,
        api_key: Optional[str] = None,
        tracer: Optional[RunTracer] = None,
        session_store: Optional[SessionStore] = None,
        session_id: Optional[str] = None,
        max_history_turns: Optional[int] = None,
        ui: Optional[BaseUI] = None,
    ) -> None:
        # Support either cloud (OpenAI-compatible) or local (e.g., Ollama) endpoints.
        resolved_base_url = (
            base_url
            or os.environ.get("OPENAI_BASE_URL")
            or os.environ.get("OLLAMA_BASE_URL")
        )
        resolved_api_key = api_key or os.environ.get("OPENAI_API_KEY")
        if not resolved_api_key and resolved_base_url:
            resolved_api_key = "ollama"
        if not resolved_api_key:
            raise RuntimeError("OPENAI_API_KEY must be set (or provide api_key manually)")
        self.client = OpenAI(
            api_key=resolved_api_key,
            base_url=resolved_base_url,
        )
        self.model = model
        self.messages: List[Dict[str, Any]] = []
        self.tools = tools or []
        self.tracer = tracer
        self.session_store = session_store
        self.session_id = session_id or "default"
        self.max_history_turns = max_history_turns if max_history_turns and max_history_turns > 0 else None
        self.ui = ui or BaseUI()
        # buffer for current turn
        self._pending_turn: List[Dict[str, Any]] = []
        # system prompt first
        # æ¶ˆæ¯æœ€å¼€å§‹ä¸€å®šæ˜¯ system prompt
        if system_prompt:
            self.messages.append({"role": "system", "content": system_prompt})
        # preload history turns (after system, before new context); each turn is a list of messages
        # ç»™æ¶ˆæ¯åˆ—è¡¨å…ˆåŠ è½½åŽ†å²å¯¹è¯
        if self.session_store:
            turns = self.session_store.load_turns(self.session_id, limit=self.max_history_turns or 0)
            for turn in turns:
                self.messages.extend(turn)
        # current run context (e.g., RAG)
        # æ¶ˆæ¯åˆ—è¡¨å†åŠ å…¥ragçš„ä¸Šä¸‹æ–‡
        if context:
            self.messages.append({"role": "user", "content": context})

    def chat(self, prompt: Optional[str] = None) -> Dict[str, Any]:
        if not self.ui.enabled:
            log_title("CHAT")
        if prompt:
            user_msg = {"role": "user", "content": prompt}
            # æ¶ˆæ¯åˆ—è¡¨æœ€åŽåŠ å…¥ç”¨æˆ·çš„ prompt
            self.messages.append(user_msg)

            # è¿™é‡Œæ˜¯æŠŠå½“å‰ç”¨æˆ·æ¶ˆæ¯ä¹Ÿæ”¾åˆ° pending_turn é‡Œï¼Œç­‰ä¼šå„¿ flush_history çš„æ—¶å€™ä¼šä¸€èµ·å­˜å‚¨
            # è¿™æ˜¯ä¸ºäº†ä¿è¯ç”¨æˆ·çš„æ¶ˆæ¯å’ŒåŽç»­æ¨¡åž‹çš„å›žå¤ã€å·¥å…·è°ƒç”¨ç»“æžœéƒ½èƒ½ä½œä¸ºä¸€ä¸ªå®Œæ•´çš„å¯¹è¯è½®æ¬¡å­˜å‚¨ä¸‹æ¥
            # åŽ†å²æ¶ˆæ¯é‡Œ, ç»Ÿä¸€å­˜å‚¨ç”¨æˆ·æ¶ˆæ¯, æ¨¡åž‹å›žå¤, å·¥å…·è°ƒç”¨ç»“æžœ, è€Œä¸å­˜å‚¨å‰é¢codeé‡Œsystem promptå’Œragä¸Šä¸‹æ–‡
            # è¿™é‡Œå…ˆåŠ å…¥ç”¨æˆ·query
            self._pending_turn.append(user_msg)

        
        response = self.client.chat.completions.create(
            model=self.model,
            messages=self.messages,
            # tools æ˜¯ç”¨æ¥å®šä¹‰å¯ç”¨çš„å·¥å…·, 
            tools=self._get_tools_definition() or None,
        )

        choice = response.choices[0].message
        content = choice.content or ""
        tool_calls = [
            ToolCall(
                id=tool_call.id or "",
                name=tool_call.function.name,
                arguments=tool_call.function.arguments,
            )
            for tool_call in choice.tool_calls or []
        ]

        if not self.ui.enabled:
            log_title("RESPONSE")
            if content:
                print(content)

        if self.tracer:
            self.tracer.log_event(
                {
                    "type": "llm_call",
                    "model": self.model,
                    "messages": self.messages,
                    "tool_calls": [tc.__dict__ for tc in tool_calls],
                    "response_content": content,
                }
            )

        # ðŸ‘‡æ˜¯ä¸ºäº†åœ¨åŽç»­çš„å¯¹è¯ä¸­ä¿ç•™ä¸Šä¸‹æ–‡å’Œå·¥å…·è°ƒç”¨ç»“æžœ
        assistant_message: Dict[str, Any] = {"role": "assistant", "content": content}
        if tool_calls:
            # tool_calls æ˜¯ OpenAI Chat Completions å“åº”é‡Œæš´éœ²çš„ä¸€ä¸ªå­—æ®µï¼Œç”¨æ¥æ‰¿è½½æ¨¡åž‹ç”Ÿæˆçš„â€œå‡½æ•°è°ƒç”¨â€æŒ‡ä»¤ï¼Œç±»ä¼¼ role/content é‚£æ ·æ˜¯åè®®çš„ä¸€éƒ¨åˆ†
            # å½“ä½ åœ¨è¯·æ±‚ä¸­æä¾› toolsï¼ˆfunction-calling å®šä¹‰ï¼‰æ—¶ï¼Œæ¨¡åž‹å¦‚æžœå†³å®šè°ƒç”¨å·¥å…·ï¼Œä¼šåœ¨è¿”å›žçš„ choices[0].message.tool_calls é‡Œç»™å‡ºè°ƒç”¨æ¸…å•
            # æ¯ä¸ª tool_call åŒ…å« idã€type: "function"ã€function.name å’Œ function.argumentsï¼ˆå­—ç¬¦ä¸²å½¢å¼ï¼‰ã€‚
            assistant_message["tool_calls"] = [
                {
                    "id": call.id,
                    "type": "function",
                    "function": {"name": call.name, "arguments": call.arguments},
                }
                for call in tool_calls
            ]

        # æ¶ˆæ¯åˆ—è¡¨é‡ŒåŠ å…¥, assistant çš„å›žå¤,ä¸»è¦å†…å®¹æ˜¯ content å’Œ tool_calls, eg:
        # {"role": "assistant", "content": "...", "tool_calls": [...]}
        # butä¸€èˆ¬æ¥è¯´content å¯èƒ½æ˜¯ç©ºçš„, å› ä¸ºæ¨¡åž‹å¯èƒ½ç›´æŽ¥è°ƒç”¨å·¥å…·è€Œä¸è¾“å‡ºæ–‡æœ¬
        # ç›®å‰çœ‹æ¥, è¾“å‡ºcontentçš„æ—¶å€™ä¸€èˆ¬æ˜¯æ²¡æœ‰tool_callsçš„, ä¹Ÿå·®ä¸å¤šå°±æ˜¯ç»“æŸloopäº†
        self.messages.append(assistant_message)
        # è¿™æ¡æ¶ˆæ¯ä¹Ÿæ”¾åˆ° pending_turn é‡Œï¼Œç­‰ä¼šå„¿ flush_history çš„æ—¶å€™ä¼šä¸€èµ·å­˜å‚¨
        self._pending_turn.append(assistant_message)

        return {"content": content, "tool_calls": tool_calls}

    # è¿™ä¸ªå‡½æ•°ç”¨äºŽå°†å·¥å…·è°ƒç”¨çš„ç»“æžœé™„åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨ä¸­
    def append_tool_result(self, tool_call_id: str, tool_output: str) -> None:
        tool_msg = {
            "role": "tool",
            "content": tool_output,
            "tool_call_id": tool_call_id,
        }

        self.messages.append(tool_msg)
    
        # è¿™é‡ŒåŠ å…¥å·¥å…·è°ƒç”¨ç»“æžœ, å‡†å¤‡æ”¾åˆ°åŽ†å²é‡Œ
        self._pending_turn.append(tool_msg)

    def flush_history(self) -> None:
        """
        Persist current turn when explicitly called (e.g., after a successful run).
        """
        if self.session_store and self._pending_turn:
            self.session_store.append_turn(
                self.session_id,
                self._pending_turn,
                max_turns=self.max_history_turns or 0,
            )
            self._pending_turn.clear()

    # è¿™ä¸ªå‡½æ•°ç”¨äºŽå°†å·¥å…·åˆ—è¡¨è½¬æ¢ä¸ºOpenAI APIæ‰€éœ€çš„æ ¼å¼
    def _get_tools_definition(self) -> List[Dict[str, Any]]:
        return [
            {
                "type": "function",
                "function": {
                    "name": tool["name"],
                    "description": tool.get("description", ""),
                    "parameters": tool.get("inputSchema", {}),
                },
            }
            for tool in self.tools
        ]


class SimpleLLMClient:
    """
    A stateless, lightweight LLM client for single-turn tasks like Query Rewriting.
    """

    def __init__(
        self,
        model: str = "gpt-4o",
        base_url: Optional[str] = None,
        api_key: Optional[str] = None,
    ) -> None:
        resolved_base_url = (
            base_url
            or os.environ.get("OPENAI_BASE_URL")
            or os.environ.get("OLLAMA_BASE_URL")
        )
        resolved_api_key = api_key or os.environ.get("OPENAI_API_KEY")
        if not resolved_api_key and resolved_base_url:
            resolved_api_key = "ollama"

        self.client = OpenAI(
            api_key=resolved_api_key,
            base_url=resolved_base_url,
        )
        self.model = model

    def generate(
        self,
        prompt: str,
        system_prompt: str = "",
        response_format: Optional[Dict[str, Any]] = None,
    ) -> str:
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})

        try:
            request_args = {"model": self.model, "messages": messages}
            if response_format is not None:
                request_args["response_format"] = response_format
            response = self.client.chat.completions.create(**request_args)
            return response.choices[0].message.content or ""
        except Exception as e:
            print(f"SimpleLLMClient error: {e}")
            return ""
