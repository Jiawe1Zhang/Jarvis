{"timestamp": "2025-12-09T09:13:56.458067Z", "type": "info", "query": "hi", "message": "run_start"}
{"timestamp": "2025-12-09T09:13:56.458608Z", "type": "info", "intent": "CHAT", "message": "intent_router"}
{"timestamp": "2025-12-09T09:13:56.458798Z", "type": "info", "reason": "config.disabled", "message": "knowledge_disabled"}
{"timestamp": "2025-12-09T09:13:56.464975Z", "type": "info", "reason": "config.disabled", "message": "rag_disabled"}
{"timestamp": "2025-12-09T09:14:16.379790Z", "type": "llm_call", "model": "gpt-5", "messages": [{"role": "system", "content": "You are an intelligent assistant equipped with specific tools.\n\n### WORKFLOW\n1. **Analyze Request**: Determine if the user's request requires performing actions (e.g., file I/O, web fetching) that your tools can handle.\n2. **Select Tool**: If an action is needed, select the appropriate tool from your available list.\n3. **Execute via Function Call**:\n   - You MUST use the native **function calling** mechanism to invoke the tool.\n   - Do NOT output the JSON arguments inside the conversation text (content).\n   - The system will only execute tools passed through the dedicated `tool_calls` field, NOT those written in the message body.\n\n### STRICT RULES\n- **NO JSON IN TEXT**: Never output raw JSON or code blocks containing tool arguments in your response text.\n- **Context Awareness**: Rely on the provided context. Do not invent file paths or data that do not exist.\n- **Fabrication Prohibited**: Do not make up file paths or information not present in the context.\n\nYou are a stateful assistant with access to prior conversation history. Use it to stay consistent, avoid repetition, and reference past context when helpful."}, {"role": "user", "content": "帮我在Arxiv找些最新的agent的文章, 并且把他们的一些核心实验方法总结到我的一个新的Notion页面里, 标题要加上"}, {"role": "assistant", "content": "", "tool_calls": [{"id": "call_Cg4aQpfNCwUctIieOBtknwMV", "type": "function", "function": {"name": "search_papers", "arguments": "{\"query\":\"(ti:\\\"agent\\\" OR ti:\\\"agents\\\" OR ti:\\\"agentic\\\" OR abs:\\\"agentic\\\" OR abs:\\\"LLM agent\\\" OR abs:\\\"autonomous agent\\\") AND (cat:cs.AI OR cat:cs.LG OR cat:cs.CL)\", \"max_results\": 25}"}}]}, {"role": "tool", "content": "[\"Search Results:\\n\\n1. The Adoption and Usage of AI Agents: Early Evidence from Perplexity\\n   Authors: Jeremy Yang, Noah Yonack, Kate Zyskowski, Denis Yarats, Johnny Ho, Jerry Ma\\n   ID: 2512.07828v1\\n   Categories: Primary: cs.LG, Additional: econ.GN\\n   Published: 2025-12-08T18:56:10Z\\n   Preview: This paper presents the first large-scale field study of the adoption, usage intensity, and use cases of general-purpose AI agents operating in open-world web environments.\\n\\n2. An Adaptive Multi-Layered Honeynet Architecture for Threat Behavior Analysis via Deep Learning\\n   Authors: Lukas Johannes M\\u00f6ller\\n   ID: 2512.07827v1\\n   Categories: Primary: cs.CR, Additional: cs.DC, cs.LG\\n   Published: 2025-12-08T18:55:26Z\\n   Preview: The escalating sophistication and variety of cyber threats have rendered static honeypots inadequate, necessitating adaptive, intelligence-driven deception.\\n\\n3. WorldReel: 4D Video Generation with Consistent Geometry and Motion Modeling\\n   Authors: Shaoheng Fang, Hanwen Jiang, Yunpeng Bai, Niloy J. Mitra, Qixing Huang\\n   ID: 2512.07821v1\\n   Categories: Primary: cs.CV, Additional: cs.AI\\n   Published: 2025-12-08T18:54:12Z\\n   Preview: Recent video generators achieve striking photorealism, yet remain fundamentally inconsistent in 3D.\\n\\n4. Collaborative Causal Sensemaking: Closing the Complementarity Gap in Human-AI Decision Support\\n   Authors: Raunak Jain, Mudita Khurana\\n   ID: 2512.07801v1\\n   Categories: Primary: cs.CL, Additional: cs.AI, cs.HC, cs.LG\\n   Published: 2025-12-08T18:30:41Z\\n   Preview: LLM-based agents are rapidly being plugged into expert decision-support, yet in messy, high-stakes settings they rarely make the team smarter: human-AI teams often underperform the best individual, ex...\\n\\n5. The Agent Capability Problem: Predicting Solvability Through Information-Theoretic Bounds\\n   Authors: Shahar Lutati\\n   ID: 2512.07631v1\\n   Categories: Primary: cs.AI, Additional: cs.CC, cs.IT, cs.LG\\n   Published: 2025-12-08T15:21:52Z\\n   Preview: When should an autonomous agent commit resources to a task? We introduce the Agent Capability Problem (ACP), a framework for predicting whether an agent can solve a problem under resource constraints.\\n\\n6. VulnLLM-R: Specialized Reasoning LLM with Agent Scaffold for Vulnerability Detection\\n   Authors: Yuzhou Nie, Hongwei Li, Chengquan Guo, Ruizhe Jiang, Zhun Wang, Bo Li, Dawn Song, Wenbo Guo\\n   ID: 2512.07533v1\\n   Categories: Primary: cs.CR, Additional: cs.AI\\n   Published: 2025-12-08T13:06:23Z\\n   Preview: We propose VulnLLM-R, the~\\\\emph{first specialized reasoning LLM} for vulnerability detection.\\n\\n7. AutoICE: Automatically Synthesizing Verifiable C Code via LLM-driven Evolution\\n   Authors: Weilin Luo, Xueyi Liang, Haotian Deng, Yanan Liu, Hai Wan\\n   ID: 2512.07501v1\\n   Categories: Primary: cs.SE, Additional: cs.AI\\n   Published: 2025-12-08T12:35:10Z\\n   Preview: Automatically synthesizing verifiable code from natural language requirements ensures software correctness and reliability while significantly lowering the barrier to adopting the techniques of formal...\\n\\n8. How Do LLMs Fail In Agentic Scenarios? A Qualitative Analysis of Success and Failure Scenarios of Various LLMs in Agentic Simulations\\n   Authors: JV Roig\\n   ID: 2512.07497v1\\n   Categories: Primary: cs.AI, Additional: cs.SE\\n   Published: 2025-12-08T12:27:15Z\\n   Preview: We investigate how large language models (LLMs) fail when operating as autonomous agents with tool-use capabilities.\\n\\n9. Enhancing Agentic RL with Progressive Reward Shaping and Value-based Sampling Policy Optimization\\n   Authors: Zhuoran Zhuang, Ye Chen, Jianghao Su, Chao Luo, Luhui Liu, Xia Zeng\\n   ID: 2512.07478v1\\n   Categories: Primary: cs.CL\\n   Published: 2025-12-08T11:59:25Z\\n   Preview: Large Language Models (LLMs) empowered with Tool-Integrated Reasoning (TIR) can iteratively plan, call external tools, and integrate returned information to solve complex, long-horizon reasoning tasks...\\n\\n10. Living the Novel: A System for Generating Self-Training Timeline-Aware Conversational Agents from Novels\\n   Authors: Yifei Huang, Tianyu Yan, Sitong Gong, Xiwei Gao, Caixin Kang, Ruicong Liu, Huchuan Lu, Bo Zheng\\n   ID: 2512.07474v1\\n   Categories: Primary: cs.HC, Additional: cs.CL\\n   Published: 2025-12-08T11:57:46Z\\n   Preview: We present the Living Novel, an end-to-end system that transforms any literary work into an immersive, multi-character conversational experience.\\n\\n11. Understanding LLM Agent Behaviours via Game Theory: Strategy Recognition, Biases and Multi-Agent Dynamics\\n   Authors: Trung-Kiet Huynh, Duy-Minh Dao-Sy, Thanh-Bang Cao, Phong-Hao Le, Hong-Dan Nguyen, Phu-Quy Nguyen-Lam, Minh-Luan Nguyen-Vo, Hong-Phat Pham, Phu-Hoa Pham, Thien-Kim Than, Chi-Nguyen Tran, Huy Tran, Gia-Thoai Tran-Le, Alessio Buscemi, Le Hong Trang, The Anh Han\\n   ID: 2512.07462v1\\n   Categories: Primary: cs.MA, Additional: cs.AI, cs.GT, cs.LG, math.DS\\n   Published: 2025-12-08T11:40:03Z\\n   Preview: As Large Language Models (LLMs) increasingly operate as autonomous decision-makers in interactive and multi-agent systems and human societies, understanding their strategic behaviour has profound impl...\\n\\n12. Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning\\n   Authors: Tong Wu, Yang Liu, Jun Bai, Zixia Jia, Shuyi Zhang, Ziyong Lin, Yanting Wang, Song-Chun Zhu, Zilong Zheng\\n   ID: 2512.07461v1\\n   Categories: Primary: cs.CL\\n   Published: 2025-12-08T11:39:43Z\\n   Preview: We introduce Native Parallel Reasoner (NPR), a teacher-free framework that enables Large Language Models (LLMs) to self-evolve genuine parallel reasoning capabilities.\\n\\n13. Social welfare optimisation in well-mixed and structured populations\\n   Authors: Van An Nguyen, Vuong Khang Huynh, Ho Nam Duong, Huu Loi Bui, Hai Anh Ha, Quang Dung Le, Le Quoc Dung Ngo, Tan Dat Nguyen, Ngoc Ngu Nguyen, Hoai Thuong Nguyen, Zhao Song, Le Hong Trang, The Anh Han\\n   ID: 2512.07453v1\\n   Categories: Primary: physics.soc-ph, Additional: cs.AI, cs.MA, math.OC, nlin.AO\\n   Published: 2025-12-08T11:27:43Z\\n   Preview: Research on promoting cooperation among autonomous, self-regarding agents has often focused on the bi-objective optimisation problem: minimising the total incentive cost while maximising the frequency...\\n\\n14. LocalSearchBench: Benchmarking Agentic Search in Real-World Local Life Services\\n   Authors: Hang He, Chuhuai Yue, Chengqi Dong, Mingxue Tian, Zhenfeng Liu, Jiajun Chai, Xiaohan Wang, Yufei Zhang, Qun Liao, Guojun Yin, Wei Lin, Chengcheng Wan, Haiying Sun, Ting Su\\n   ID: 2512.07436v1\\n   Categories: Primary: cs.AI\\n   Published: 2025-12-08T11:12:39Z\\n   Preview: Recent advances in large reasoning models (LRMs) have enabled agentic search systems to perform complex multi-step reasoning across multiple sources.\\n\\n15. Adaptive Tuning of Parameterized Traffic Controllers via Multi-Agent Reinforcement Learning\\n   Authors: Giray \\u00d6n\\u00fcr, Azita Dabiri, Bart De Schutter\\n   ID: 2512.07417v1\\n   Categories: Primary: cs.LG\\n   Published: 2025-12-08T10:52:00Z\\n   Preview: Effective traffic control is essential for mitigating congestion in transportation networks.\\n\\n16. Training Language Models to Use Prolog as a Tool\\n   Authors: Niklas Mellgren, Peter Schneider-Kamp, Lukas Galke Poech\\n   ID: 2512.07407v1\\n   Categories: Primary: cs.CL\\n   Published: 2025-12-08T10:39:38Z\\n   Preview: Ensuring reliable tool use is critical for safe agentic AI systems.\\n\\n17. DeepAgent: A Dual Stream Multi Agent Fusion for Robust Multimodal Deepfake Detection\\n   Authors: Sayeem Been Zaman, Wasimul Karim, Arefin Ittesafun Abian, Reem E. Mohamed, Md Rafiqul Islam, Asif Karim, Sami Azam\\n   ID: 2512.07351v1\\n   Categories: Primary: cs.CV, Additional: cs.AI, cs.SD\\n   Published: 2025-12-08T09:43:30Z\\n   Preview: The increasing use of synthetic media, particularly deepfakes, is an emerging challenge for digital content verification.\\n\\n18. PrivORL: Differentially Private Synthetic Dataset for Offline Reinforcement Learning\\n   Authors: Chen Gong, Zheng Liu, Kecen Li, Tianhao Wang\\n   ID: 2512.07342v1\\n   Categories: Primary: cs.CR, Additional: cs.LG\\n   Published: 2025-12-08T09:29:24Z\\n   Preview: Recently, offline reinforcement learning (RL) has become a popular RL paradigm.\\n\\n19. Towards Accurate UAV Image Perception: Guiding Vision-Language Models with Stronger Task Prompts\\n   Authors: Mingning Guo, Mengwei Wu, Shaoxian Li, Haifeng Li, Chao Tao\\n   ID: 2512.07302v1\\n   Categories: Primary: cs.CV, Additional: cs.AI\\n   Published: 2025-12-08T08:44:57Z\\n   Preview: Existing image perception methods based on VLMs generally follow a paradigm wherein models extract and analyze image content based on user-provided textual task prompts.\\n\\n20. SIT-Graph: State Integrated Tool Graph for Multi-Turn Agents\\n   Authors: Sijia Li, Yuchen Huang, Zifan Liu, Zijian Li, Jingjing fu, Lei Song, Jiang Bian, Jun Zhang, Rui Wang\\n   ID: 2512.07287v1\\n   Categories: Primary: cs.LG, Additional: cs.AI\\n   Published: 2025-12-08T08:27:24Z\\n   Preview: Despite impressive advances in agent systems, multi-turn tool-use scenarios remain challenging.\\n\\n21. MASim: Multilingual Agent-Based Simulation for Social Science\\n   Authors: Xuan Zhang, Wenxuan Zhang, Anxu Wang, See-Kiong Ng, Yang Deng\\n   ID: 2512.07195v1\\n   Categories: Primary: cs.CL, Additional: cs.AI, cs.CY, cs.MA, cs.SI\\n   Published: 2025-12-08T06:12:48Z\\n   Preview: Multi-agent role-playing has recently shown promise for studying social behavior with language agents, but existing simulations are mostly monolingual and fail to model cross-lingual interaction, an e...\\n\\n22. DART: Leveraging Multi-Agent Disagreement for Tool Recruitment in Multimodal Reasoning\\n   Authors: Nithin Sivakumaran, Justin Chih-Yao Chen, David Wan, Yue Zhang, Jaehong Yoon, Elias Stengel-Eskin, Mohit Bansal\\n   ID: 2512.07132v1\\n   Categories: Primary: cs.CL, Additional: cs.AI, cs.CV\\n   Published: 2025-12-08T03:33:38Z\\n   Preview: Specialized visual tools can augment large language models or vision language models with expert knowledge (e.g., grounding, spatial reasoning, medical knowledge, etc.), but knowing which tools to cal...\\n\\n23. VIGIL: A Reflective Runtime for Self-Healing Agents\\n   Authors: Christopher Cruz\\n   ID: 2512.07094v1\\n   Categories: Primary: cs.AI\\n   Published: 2025-12-08T02:18:41Z\\n   Preview: Agentic LLM frameworks promise autonomous behavior via task decomposition, tool use, and iterative planning, but most deployed systems remain brittle.\\n\\n24. ClinNoteAgents: An LLM Multi-Agent System for Predicting and Interpreting Heart Failure 30-Day Readmission from Clinical Notes\\n   Authors: Rongjia Zhou, Chengzhuo Li, Carl Yang, Jiaying Lu\\n   ID: 2512.07081v1\\n   Categories: Primary: cs.AI\\n   Published: 2025-12-08T01:32:14Z\\n   Preview: Heart failure (HF) is one of the leading causes of rehospitalization among older adults in the United States.\\n\\n25. Reformulate, Retrieve, Localize: Agents for Repository-Level Bug Localization\\n   Authors: Genevieve Caumartin, Glaucia Melo\\n   ID: 2512.07022v1\\n   Categories: Primary: cs.SE, Additional: cs.AI, cs.IR\\n   Published: 2025-12-07T22:25:11Z\\n   Preview: Bug localization remains a critical yet time-consuming challenge in large-scale software repositories.\\n\\n\"]", "tool_call_id": "call_Cg4aQpfNCwUctIieOBtknwMV"}, {"role": "assistant", "content": "", "tool_calls": [{"id": "call_0PcX6oZReHzxkG328H0psOOl", "type": "function", "function": {"name": "get_paper_data", "arguments": "{\"paper_id\": \"2512.07497v1\"}"}}, {"id": "call_8XGh4sX7zPGmyn3f5CCjY5n6", "type": "function", "function": {"name": "get_paper_data", "arguments": "{\"paper_id\": \"2512.07478v1\"}"}}, {"id": "call_NplMtNeVlcycAaWxeRyoWupz", "type": "function", "function": {"name": "get_paper_data", "arguments": "{\"paper_id\": \"2512.07436v1\"}"}}, {"id": "call_WkK8OQoUmSwV6iQszjGgYxPL", "type": "function", "function": {"name": "get_paper_data", "arguments": "{\"paper_id\": \"2512.07287v1\"}"}}, {"id": "call_wD154lptaGKNhjMqVuIx22jJ", "type": "function", "function": {"name": "get_paper_data", "arguments": "{\"paper_id\": \"2512.07132v1\"}"}}, {"id": "call_6EmUW8jJ39C3k3KU1C8Ksjbj", "type": "function", "function": {"name": "get_paper_data", "arguments": "{\"paper_id\": \"2512.07094v1\"}"}}, {"id": "call_lLdnBLrjLAmzgk7Dh8xVlvfs", "type": "function", "function": {"name": "get_paper_data", "arguments": "{\"paper_id\": \"2512.07195v1\"}"}}, {"id": "call_Wu6eySzmhbG1uXu5D4eFUX3U", "type": "function", "function": {"name": "get_paper_data", "arguments": "{\"paper_id\": \"2512.07533v1\"}"}}, {"id": "call_Ieh68m5zATvNa4f7XlwkAwcQ", "type": "function", "function": {"name": "get_paper_data", "arguments": "{\"paper_id\": \"2512.07022v1\"}"}}, {"id": "call_3fh2vGu2Rj0YMkI4IRz6dEOW", "type": "function", "function": {"name": "get_paper_data", "arguments": "{\"paper_id\": \"2512.07462v1\"}"}}, {"id": "call_5axPXrqyV2X6Wm80HoPfLrlP", "type": "function", "function": {"name": "get_paper_data", "arguments": "{\"paper_id\": \"2512.07407v1\"}"}}, {"id": "call_nui3JlRBg1EeahUasPtNDsDR", "type": "function", "function": {"name": "get_paper_data", "arguments": "{\"paper_id\": \"2512.07828v1\"}"}}]}, {"role": "tool", "content": "[\"Title: How Do LLMs Fail In Agentic Scenarios? A Qualitative Analysis of Success and Failure Scenarios of Various LLMs in Agentic Simulations\\n\\nMetadata:\\n- Authors: JV Roig\\n- Published: 2025-12-08T12:27:15Z\\n- Last Updated: 2025-12-08T12:27:15Z\\n- Categories: Primary: cs.AI, Additional: cs.SE\\n\\nAbstract:\\nWe investigate how large language models (LLMs) fail when operating as autonomous agents with tool-use capabilities. Using the Kamiwaza Agentic Merit Index (KAMI) v0.1 benchmark, we analyze 900 execution traces from three representative models - Granite 4 Small, Llama 4 Maverick, and DeepSeek V3.1 - across filesystem, text extraction, CSV analysis, and SQL scenarios. Rather than focusing on aggregate scores, we perform fine-grained, per-trial behavioral analysis to surface the strategies that enable successful multi-step tool execution and the recurrent failure modes that undermine reliability. Our findings show that model scale alone does not predict agentic robustness: Llama 4 Maverick (400B) performs only marginally better than Granite 4 Small (32B) in some uncertainty-driven tasks, while DeepSeek V3.1's superior reliability derives primarily from post-training reinforcement learning rather than architecture or size. Across models, we identify four recurring failure archetypes: premature action without grounding, over-helpfulness that substitutes missing entities, vulnerability to distractor-induced context pollution, and fragile execution under load. These patterns highlight the need for agentic evaluation methods that emphasize interactive grounding, recovery behavior, and environment-aware adaptation, suggesting that reliable enterprise deployment requires not just stronger models but deliberate training and design choices that reinforce verification, constraint discovery, and adherence to source-of-truth data.\\n\\nAccess Options:\\n- Abstract page: https://arxiv.org/abs/2512.07497v1\\n- Full text HTML version: https://arxiv.org/html/2512.07497v1\\n- PDF version: https://arxiv.org/pdf/2512.07497v1\\n\\nAdditional Information:\\n- Comment: 48 pages, 3 tables, 2 listings\\n\"]", "tool_call_id": "call_0PcX6oZReHzxkG328H0psOOl"}, {"role": "tool", "content": "[\"Title: Enhancing Agentic RL with Progressive Reward Shaping and Value-based Sampling Policy Optimization\\n\\nMetadata:\\n- Authors: Zhuoran Zhuang, Ye Chen, Jianghao Su, Chao Luo, Luhui Liu, Xia Zeng\\n- Published: 2025-12-08T11:59:25Z\\n- Last Updated: 2025-12-08T11:59:25Z\\n- Categories: Primary: cs.CL\\n\\nAbstract:\\nLarge Language Models (LLMs) empowered with Tool-Integrated Reasoning (TIR) can iteratively plan, call external tools, and integrate returned information to solve complex, long-horizon reasoning tasks. Agentic Reinforcement Learning (Agentic RL) optimizes such models over full tool-interaction trajectories, but two key challenges hinder effectiveness: (1) Sparse, non-instructive rewards, such as binary 0-1 verifiable signals, provide limited guidance for intermediate steps and slow convergence; (2) Gradient degradation in Group Relative Policy Optimization (GRPO), where identical rewards within a rollout group yield zero advantage, reducing sample efficiency and destabilizing training. To address these challenges, we propose two complementary techniques: Progressive Reward Shaping (PRS) and Value-based Sampling Policy Optimization (VSPO). PRS is a curriculum-inspired reward design that introduces dense, stage-wise feedback - encouraging models to first master parseable and properly formatted tool calls, then optimize for factual correctness and answer quality. We instantiate PRS for short-form QA (with a length-aware BLEU to fairly score concise answers) and long-form QA (with LLM-as-a-Judge scoring to prevent reward hacking). VSPO is an enhanced GRPO variant that replaces low-value samples with prompts selected by a task-value metric balancing difficulty and uncertainty, and applies value-smoothing clipping to stabilize gradient updates. Experiments on multiple short-form and long-form QA benchmarks show that PRS consistently outperforms traditional binary rewards, and VSPO achieves superior stability, faster convergence, and higher final performance compared to PPO, GRPO, CISPO, and SFT-only baselines. Together, PRS and VSPO yield LLM-based TIR agents that generalize better across domains.\\n\\nAccess Options:\\n- Abstract page: https://arxiv.org/abs/2512.07478v1\\n- Full text HTML version: https://arxiv.org/html/2512.07478v1\\n- PDF version: https://arxiv.org/pdf/2512.07478v1\\n\"]", "tool_call_id": "call_8XGh4sX7zPGmyn3f5CCjY5n6"}, {"role": "tool", "content": "[\"Title: LocalSearchBench: Benchmarking Agentic Search in Real-World Local Life Services\\n\\nMetadata:\\n- Authors: Hang He, Chuhuai Yue, Chengqi Dong, Mingxue Tian, Zhenfeng Liu, Jiajun Chai, Xiaohan Wang, Yufei Zhang, Qun Liao, Guojun Yin, Wei Lin, Chengcheng Wan, Haiying Sun, Ting Su\\n- Published: 2025-12-08T11:12:39Z\\n- Last Updated: 2025-12-08T11:12:39Z\\n- Categories: Primary: cs.AI\\n\\nAbstract:\\nRecent advances in large reasoning models (LRMs) have enabled agentic search systems to perform complex multi-step reasoning across multiple sources. However, most studies focus on general information retrieval and rarely explores vertical domains with unique challenges. In this work, we focus on local life services and introduce LocalSearchBench, which encompass diverse and complex business scenarios. Real-world queries in this domain are often ambiguous and require multi-hop reasoning across merchants and products, remaining challenging and not fully addressed. As the first comprehensive benchmark for agentic search in local life services, LocalSearchBench includes over 150,000 high-quality entries from various cities and business types. We construct 300 multi-hop QA tasks based on real user queries, challenging agents to understand questions and retrieve information in multiple steps. We also developed LocalPlayground, a unified environment integrating multiple tools for agent interaction. Experiments show that even state-of-the-art LRMs struggle on LocalSearchBench: the best model (DeepSeek-V3.1) achieves only 34.34% correctness, and most models have issues with completeness (average 77.33%) and faithfulness (average 61.99%). This highlights the need for specialized benchmarks and domain-specific agent training in local life services. Code, Benchmark, and Leaderboard are available at localsearchbench.github.io.\\n\\nAccess Options:\\n- Abstract page: https://arxiv.org/abs/2512.07436v1\\n- Full text HTML version: https://arxiv.org/html/2512.07436v1\\n- PDF version: https://arxiv.org/pdf/2512.07436v1\\n\"]", "tool_call_id": "call_NplMtNeVlcycAaWxeRyoWupz"}, {"role": "tool", "content": "[\"Title: SIT-Graph: State Integrated Tool Graph for Multi-Turn Agents\\n\\nMetadata:\\n- Authors: Sijia Li, Yuchen Huang, Zifan Liu, Zijian Li, Jingjing fu, Lei Song, Jiang Bian, Jun Zhang, Rui Wang\\n- Published: 2025-12-08T08:27:24Z\\n- Last Updated: 2025-12-08T08:27:24Z\\n- Categories: Primary: cs.LG, Additional: cs.AI\\n\\nAbstract:\\nDespite impressive advances in agent systems, multi-turn tool-use scenarios remain challenging. It is mainly because intent is clarified progressively and the environment evolves with each tool call. While reusing past experience is natural, current LLM agents either treat entire trajectories or pre-defined subtasks as indivisible units, or solely exploit tool-to-tool dependencies, hindering adaptation as states and information evolve across turns. In this paper, we propose a State Integrated Tool Graph (SIT-Graph), which enhances multi-turn tool use by exploiting partially overlapping experience. Inspired by human decision-making that integrates episodic and procedural memory, SIT-Graph captures both compact state representations (episodic-like fragments) and tool-to-tool dependencies (procedural-like routines) from historical trajectories. Specifically, we first build a tool graph from accumulated tool-use sequences, and then augment each edge with a compact state summary of the dialog and tool history that may shape the next action. At inference time, SIT-Graph enables a human-like balance between episodic recall and procedural execution: when the next decision requires recalling prior context, the agent retrieves the state summaries stored on relevant edges and uses them to guide its next action; when the step is routine, it follows high-confidence tool dependencies without explicit recall. Experiments across multiple stateful multi-turn tool-use benchmarks show that SIT-Graph consistently outperforms strong memory- and graph-based baselines, delivering more robust tool selection and more effective experience transfer.\\n\\nAccess Options:\\n- Abstract page: https://arxiv.org/abs/2512.07287v1\\n- Full text HTML version: https://arxiv.org/html/2512.07287v1\\n- PDF version: https://arxiv.org/pdf/2512.07287v1\\n\"]", "tool_call_id": "call_WkK8OQoUmSwV6iQszjGgYxPL"}, {"role": "tool", "content": "[\"Title: DART: Leveraging Multi-Agent Disagreement for Tool Recruitment in Multimodal Reasoning\\n\\nMetadata:\\n- Authors: Nithin Sivakumaran, Justin Chih-Yao Chen, David Wan, Yue Zhang, Jaehong Yoon, Elias Stengel-Eskin, Mohit Bansal\\n- Published: 2025-12-08T03:33:38Z\\n- Last Updated: 2025-12-08T03:33:38Z\\n- Categories: Primary: cs.CL, Additional: cs.AI, cs.CV\\n\\nAbstract:\\nSpecialized visual tools can augment large language models or vision language models with expert knowledge (e.g., grounding, spatial reasoning, medical knowledge, etc.), but knowing which tools to call (and when to call them) can be challenging. We introduce DART, a multi-agent framework that uses disagreements between multiple debating visual agents to identify useful visual tools (e.g., object detection, OCR, spatial reasoning, etc.) that can resolve inter-agent disagreement. These tools allow for fruitful multi-agent discussion by introducing new information, and by providing tool-aligned agreement scores that highlight agents in agreement with expert tools, thereby facilitating discussion. We utilize an aggregator agent to select the best answer by providing the agent outputs and tool information. We test DART on four diverse benchmarks and show that our approach improves over multi-agent debate as well as over single agent tool-calling frameworks, beating the next-strongest baseline (multi-agent debate with a judge model) by 3.4% and 2.4% on A-OKVQA and MMMU respectively. We also find that DART adapts well to new tools in applied domains, with a 1.3% improvement on the M3D medical dataset over other strong tool-calling, single agent, and multi-agent baselines. Additionally, we measure text overlap across rounds to highlight the rich discussion in DART compared to existing multi-agent methods. Finally, we study the tool call distribution, finding that diverse tools are reliably used to help resolve disagreement.\\n\\nAccess Options:\\n- Abstract page: https://arxiv.org/abs/2512.07132v1\\n- Full text HTML version: https://arxiv.org/html/2512.07132v1\\n- PDF version: https://arxiv.org/pdf/2512.07132v1\\n\\nAdditional Information:\\n- Comment: Code: https://github.com/nsivaku/dart\\n\"]", "tool_call_id": "call_wD154lptaGKNhjMqVuIx22jJ"}, {"role": "tool", "content": "[\"Title: VIGIL: A Reflective Runtime for Self-Healing Agents\\n\\nMetadata:\\n- Authors: Christopher Cruz\\n- Published: 2025-12-08T02:18:41Z\\n- Last Updated: 2025-12-08T02:18:41Z\\n- Categories: Primary: cs.AI\\n\\nAbstract:\\nAgentic LLM frameworks promise autonomous behavior via task decomposition, tool use, and iterative planning, but most deployed systems remain brittle. They lack runtime introspection, cannot diagnose their own failure modes, and do not improve over time without human intervention. In practice, many agent stacks degrade into decorated chains of LLM calls with no structural mechanisms for reliability. We present VIGIL (Verifiable Inspection and Guarded Iterative Learning), a reflective runtime that supervises a sibling agent and performs autonomous maintenance rather than task execution. VIGIL ingests behavioral logs, appraises each event into a structured emotional representation, maintains a persistent EmoBank with decay and contextual policies, and derives an RBT diagnosis that sorts recent behavior into strengths, opportunities, and failures. From this analysis, VIGIL generates both guarded prompt updates that preserve core identity semantics and read only code proposals produced by a strategy engine that operates on log evidence and code hotspots. VIGIL functions as a state gated pipeline. Illegal transitions produce explicit errors rather than allowing the LLM to improvise. In a reminder latency case study, VIGIL identified elevated lag, proposed prompt and code repairs, and when its own diagnostic tool failed due to a schema conflict, it surfaced the internal error, produced a fallback diagnosis, and emitted a repair plan. This demonstrates meta level self repair in a deployed agent runtime.\\n\\nAccess Options:\\n- Abstract page: https://arxiv.org/abs/2512.07094v1\\n- Full text HTML version: https://arxiv.org/html/2512.07094v1\\n- PDF version: https://arxiv.org/pdf/2512.07094v1\\n\"]", "tool_call_id": "call_6EmUW8jJ39C3k3KU1C8Ksjbj"}, {"role": "tool", "content": "[\"Title: MASim: Multilingual Agent-Based Simulation for Social Science\\n\\nMetadata:\\n- Authors: Xuan Zhang, Wenxuan Zhang, Anxu Wang, See-Kiong Ng, Yang Deng\\n- Published: 2025-12-08T06:12:48Z\\n- Last Updated: 2025-12-08T06:12:48Z\\n- Categories: Primary: cs.CL, Additional: cs.AI, cs.CY, cs.MA, cs.SI\\n\\nAbstract:\\nMulti-agent role-playing has recently shown promise for studying social behavior with language agents, but existing simulations are mostly monolingual and fail to model cross-lingual interaction, an essential property of real societies. We introduce MASim, the first multilingual agent-based simulation framework that supports multi-turn interaction among generative agents with diverse sociolinguistic profiles. MASim offers two key analyses: (i) global public opinion modeling, by simulating how attitudes toward open-domain hypotheses evolve across languages and cultures, and (ii) media influence and information diffusion, via autonomous news agents that dynamically generate content and shape user behavior. To instantiate simulations, we construct the MAPS benchmark, which combines survey questions and demographic personas drawn from global population distributions. Experiments on calibration, sensitivity, consistency, and cultural case studies show that MASim reproduces sociocultural phenomena and highlights the importance of multilingual simulation for scalable, controlled computational social science.\\n\\nAccess Options:\\n- Abstract page: https://arxiv.org/abs/2512.07195v1\\n- Full text HTML version: https://arxiv.org/html/2512.07195v1\\n- PDF version: https://arxiv.org/pdf/2512.07195v1\\n\"]", "tool_call_id": "call_lLdnBLrjLAmzgk7Dh8xVlvfs"}, {"role": "tool", "content": "[\"Title: VulnLLM-R: Specialized Reasoning LLM with Agent Scaffold for Vulnerability Detection\\n\\nMetadata:\\n- Authors: Yuzhou Nie, Hongwei Li, Chengquan Guo, Ruizhe Jiang, Zhun Wang, Bo Li, Dawn Song, Wenbo Guo\\n- Published: 2025-12-08T13:06:23Z\\n- Last Updated: 2025-12-08T13:06:23Z\\n- Categories: Primary: cs.CR, Additional: cs.AI\\n\\nAbstract:\\nWe propose VulnLLM-R, the~\\\\emph{first specialized reasoning LLM} for vulnerability detection. Our key insight is that LLMs can reason about program states and analyze the potential vulnerabilities, rather than simple pattern matching. This can improve the model's generalizability and prevent learning shortcuts. However, SOTA reasoning LLMs are typically ultra-large, closed-source, or have limited performance in vulnerability detection. To address this, we propose a novel training recipe with specialized data selection, reasoning data generation, reasoning data filtering and correction, and testing-phase optimization. Using our proposed methodology, we train a reasoning model with seven billion parameters. Through extensive experiments on SOTA datasets across Python, C/C++, and Java, we show that VulnLLM-R has superior effectiveness and efficiency than SOTA static analysis tools and both open-source and commercial large reasoning models. We further conduct a detailed ablation study to validate the key designs in our training recipe. Finally, we construct an agent scaffold around our model and show that it outperforms CodeQL and AFL++ in real-world projects. Our agent further discovers a set of zero-day vulnerabilities in actively maintained repositories. This work represents a pioneering effort to enable real-world, project-level vulnerability detection using AI agents powered by specialized reasoning models. The code is available at~\\\\href{https://github.com/ucsb-mlsec/VulnLLM-R}{github}.\\n\\nAccess Options:\\n- Abstract page: https://arxiv.org/abs/2512.07533v1\\n- Full text HTML version: https://arxiv.org/html/2512.07533v1\\n- PDF version: https://arxiv.org/pdf/2512.07533v1\\n\"]", "tool_call_id": "call_Wu6eySzmhbG1uXu5D4eFUX3U"}, {"role": "tool", "content": "[\"Title: Reformulate, Retrieve, Localize: Agents for Repository-Level Bug Localization\\n\\nMetadata:\\n- Authors: Genevieve Caumartin, Glaucia Melo\\n- Published: 2025-12-07T22:25:11Z\\n- Last Updated: 2025-12-07T22:25:11Z\\n- Categories: Primary: cs.SE, Additional: cs.AI, cs.IR\\n\\nAbstract:\\nBug localization remains a critical yet time-consuming challenge in large-scale software repositories. Traditional information retrieval-based bug localization (IRBL) methods rely on unchanged bug descriptions, which often contain noisy information, leading to poor retrieval accuracy. Recent advances in large language models (LLMs) have improved bug localization through query reformulation, yet the effect on agent performance remains unexplored. In this study, we investigate how an LLM-powered agent can improve file-level bug localization via lightweight query reformulation and summarization. We first employ an open-source, non-fine-tuned LLM to extract key information from bug reports, such as identifiers and code snippets, and reformulate queries pre-retrieval. Our agent then orchestrates BM25 retrieval using these preprocessed queries, automating localization workflow at scale. Using the best-performing query reformulation technique, our agent achieves 35% better ranking in first-file retrieval than our BM25 baseline and up to +22% file retrieval performance over SWE-agent.\\n\\nAccess Options:\\n- Abstract page: https://arxiv.org/abs/2512.07022v1\\n- Full text HTML version: https://arxiv.org/html/2512.07022v1\\n- PDF version: https://arxiv.org/pdf/2512.07022v1\\n\\nAdditional Information:\\n- Comment: Accepted at BoatSE 2026\\n\"]", "tool_call_id": "call_Ieh68m5zATvNa4f7XlwkAwcQ"}, {"role": "tool", "content": "[\"Title: Understanding LLM Agent Behaviours via Game Theory: Strategy Recognition, Biases and Multi-Agent Dynamics\\n\\nMetadata:\\n- Authors: Trung-Kiet Huynh, Duy-Minh Dao-Sy, Thanh-Bang Cao, Phong-Hao Le, Hong-Dan Nguyen, Phu-Quy Nguyen-Lam, Minh-Luan Nguyen-Vo, Hong-Phat Pham, Phu-Hoa Pham, Thien-Kim Than, Chi-Nguyen Tran, Huy Tran, Gia-Thoai Tran-Le, Alessio Buscemi, Le Hong Trang, The Anh Han\\n- Published: 2025-12-08T11:40:03Z\\n- Last Updated: 2025-12-08T11:40:03Z\\n- Categories: Primary: cs.MA, Additional: cs.AI, cs.GT, cs.LG, math.DS\\n\\nAbstract:\\nAs Large Language Models (LLMs) increasingly operate as autonomous decision-makers in interactive and multi-agent systems and human societies, understanding their strategic behaviour has profound implications for safety, coordination, and the design of AI-driven social and economic infrastructures. Assessing such behaviour requires methods that capture not only what LLMs output, but the underlying intentions that guide their decisions. In this work, we extend the FAIRGAME framework to systematically evaluate LLM behaviour in repeated social dilemmas through two complementary advances: a payoff-scaled Prisoners Dilemma isolating sensitivity to incentive magnitude, and an integrated multi-agent Public Goods Game with dynamic payoffs and multi-agent histories. These environments reveal consistent behavioural signatures across models and languages, including incentive-sensitive cooperation, cross-linguistic divergence and end-game alignment toward defection. To interpret these patterns, we train traditional supervised classification models on canonical repeated-game strategies and apply them to FAIRGAME trajectories, showing that LLMs exhibit systematic, model- and language-dependent behavioural intentions, with linguistic framing at times exerting effects as strong as architectural differences. Together, these findings provide a unified methodological foundation for auditing LLMs as strategic agents and reveal systematic cooperation biases with direct implications for AI governance, collective decision-making, and the design of safe multi-agent systems.\\n\\nAccess Options:\\n- Abstract page: https://arxiv.org/abs/2512.07462v1\\n- Full text HTML version: https://arxiv.org/html/2512.07462v1\\n- PDF version: https://arxiv.org/pdf/2512.07462v1\\n\"]", "tool_call_id": "call_3fh2vGu2Rj0YMkI4IRz6dEOW"}, {"role": "tool", "content": "[\"Title: Training Language Models to Use Prolog as a Tool\\n\\nMetadata:\\n- Authors: Niklas Mellgren, Peter Schneider-Kamp, Lukas Galke Poech\\n- Published: 2025-12-08T10:39:38Z\\n- Last Updated: 2025-12-08T10:39:38Z\\n- Categories: Primary: cs.CL\\n\\nAbstract:\\nEnsuring reliable tool use is critical for safe agentic AI systems. Language models frequently produce unreliable reasoning with plausible but incorrect solutions that are difficult to verify. To address this, we investigate fine-tuning models to use Prolog as an external tool for verifiable computation. Using Group Relative Policy Optimization (GRPO), we fine-tune Qwen2.5-3B-Instruct on a cleaned GSM8K-Prolog-Prover dataset while varying (i) prompt structure, (ii) reward composition (execution, syntax, semantics, structure), and (iii) inference protocol: single-shot, best-of-N, and two agentic modes where Prolog is invoked internally or independently. Our reinforcement learning approach outperforms supervised fine-tuning, with our 3B model achieving zero-shot MMLU performance comparable to 7B few-shot results. Our findings reveal that: 1) joint tuning of prompt, reward, and inference shapes program syntax and logic; 2) best-of-N with external Prolog verification maximizes accuracy on GSM8K; 3) agentic inference with internal repair yields superior zero-shot generalization on MMLU-Stem and MMLU-Pro. These results demonstrate that grounding model reasoning in formal verification systems substantially improves reliability and auditability for safety-critical applications. The source code for reproducing our experiments is available under https://github.com/niklasmellgren/grpo-prolog-inference\\n\\nAccess Options:\\n- Abstract page: https://arxiv.org/abs/2512.07407v1\\n- Full text HTML version: https://arxiv.org/html/2512.07407v1\\n- PDF version: https://arxiv.org/pdf/2512.07407v1\\n\\nAdditional Information:\\n- Comment: 10 pages\\n\"]", "tool_call_id": "call_5axPXrqyV2X6Wm80HoPfLrlP"}, {"role": "tool", "content": "[\"Title: The Adoption and Usage of AI Agents: Early Evidence from Perplexity\\n\\nMetadata:\\n- Authors: Jeremy Yang, Noah Yonack, Kate Zyskowski, Denis Yarats, Johnny Ho, Jerry Ma\\n- Published: 2025-12-08T18:56:10Z\\n- Last Updated: 2025-12-08T18:56:10Z\\n- Categories: Primary: cs.LG, Additional: econ.GN\\n\\nAbstract:\\nThis paper presents the first large-scale field study of the adoption, usage intensity, and use cases of general-purpose AI agents operating in open-world web environments. Our analysis centers on Comet, an AI-powered browser developed by Perplexity, and its integrated agent, Comet Assistant. Drawing on hundreds of millions of anonymized user interactions, we address three fundamental questions: Who is using AI agents? How intensively are they using them? And what are they using them for? Our findings reveal substantial heterogeneity in adoption and usage across user segments. Earlier adopters, users in countries with higher GDP per capita and educational attainment, and individuals working in digital or knowledge-intensive sectors -- such as digital technology, academia, finance, marketing, and entrepreneurship -- are more likely to adopt or actively use the agent. To systematically characterize the substance of agent usage, we introduce a hierarchical agentic taxonomy that organizes use cases across three levels: topic, subtopic, and task. The two largest topics, Productivity & Workflow and Learning & Research, account for 57% of all agentic queries, while the two largest subtopics, Courses and Shopping for Goods, make up 22%. The top 10 out of 90 tasks represent 55% of queries. Personal use constitutes 55% of queries, while professional and educational contexts comprise 30% and 16%, respectively. In the short term, use cases exhibit strong stickiness, but over time users tend to shift toward more cognitively oriented topics. The diffusion of increasingly capable AI agents carries important implications for researchers, businesses, policymakers, and educators, inviting new lines of inquiry into this rapidly emerging class of AI capabilities.\\n\\nAccess Options:\\n- Abstract page: https://arxiv.org/abs/2512.07828v1\\n- Full text HTML version: https://arxiv.org/html/2512.07828v1\\n- PDF version: https://arxiv.org/pdf/2512.07828v1\\n\"]", "tool_call_id": "call_nui3JlRBg1EeahUasPtNDsDR"}, {"role": "assistant", "content": "", "tool_calls": [{"id": "call_x7bEnyWoeYd6WNFapWODjL48", "type": "function", "function": {"name": "notion-create-pages", "arguments": "{\"pages\":[{\"content\":\"# 概览\\n本页整理了最近 arXiv 上与“Agent/Agentic AI”相关的论文，并提炼各自的核心实验方法与设置，便于快速复用到项目中。\\n<table_of_contents/>\\n\\n# 论文与核心实验方法\\n\\n## 1) How Do LLMs Fail In Agentic Scenarios? (2025-12-08)\\n- 链接: [arXiv](https://arxiv.org/abs/2512.07497v1) · [PDF](https://arxiv.org/pdf/2512.07497v1)\\n- 作者: JV Roig\\n- 任务/场景: KAMI v0.1 基准中的多步工具使用（文件系统/文本抽取/CSV/SQL）。\\n- 核心实验方法:\\n  - 基于900条执行轨迹的细粒度行为分析，而非仅看总分。\\n  - 识别4类复发性失效模式：过早行动缺乏grounding、过度“帮忙”替代缺失实体、受干扰信息污染上下文、负载下脆弱执行。\\n  - 强调面向交互grounding、恢复行为与环境自适应的评测维度。\\n- 评价/结果: 模型规模并非充分条件；可靠性更多来自后训练强化等过程设计。\\n\\n## 2) Enhancing Agentic RL with PRS + VSPO (2025-12-08)\\n- 链接: [arXiv](https://arxiv.org/abs/2512.07478v1) · [PDF](https://arxiv.org/pdf/2512.07478v1)\\n- 作者: Zhuoran Zhuang 等\\n- 任务/场景: 工具集成推理(TIR)的Agentic RL，短/长问答。\\n- 核心实验方法:\\n  - PRS（Progressive Reward Shaping）：课程式密集奖励，先约束可解析格式与工具调用，再优化事实性与答案质量；长问答用 LLM-as-a-Judge 抑制reward hacking。\\n  - VSPO（Value-based Sampling Policy Optimization）：按任务价值选样，替换低价值样本，引入value-smoothing clipping 稳定梯度；改进GRPO的同酬零优势问题。\\n- 评价/结果: 相比PPO/GRPO/CISPO与SFT基线更稳定、更快收敛且更高性能。\\n\\n## 3) LocalSearchBench: Agentic Search in Local Life (2025-12-08)\\n- 链接: [arXiv](https://arxiv.org/abs/2512.07436v1) · [PDF](https://arxiv.org/pdf/2512.07436v1)\\n- 作者: Hang He 等\\n- 任务/场景: 本地生活服务垂直领域的多步检索与推理（15万+数据、300多跳QA）。\\n- 核心实验方法:\\n  - 构建LocalSearchBench基准与LocalPlayground工具环境，覆盖多城市/多业态的真实查询。\\n  - 评估维度：正确性、完备性、忠实性；揭示SOTA在垂直复杂场景下显著不足。\\n- 评价/结果: 最佳模型仅34.34%正确性，凸显领域特定训练与评测需求。\\n\\n## 4) SIT-Graph: State Integrated Tool Graph (2025-12-08)\\n- 链接: [arXiv](https://arxiv.org/abs/2512.07287v1) · [PDF](https://arxiv.org/pdf/2512.07287v1)\\n- 作者: Sijia Li 等\\n- 任务/场景: 多轮工具使用中的状态演化与经验复用。\\n- 核心实验方法:\\n  - 从历史轨迹构建“工具图”，在边上存放压缩状态摘要（对话与工具上下文）。\\n  - 推理时在“情节回忆”（取状态摘要）与“程序化执行”（高置信度工具依赖）之间动态权衡。\\n- 评价/结果: 相比记忆/图基线，更鲁棒的工具选择与经验迁移。\\n\\n## 5) DART: Multi-Agent Disagreement for Tool Recruitment (2025-12-08)\\n- 链接: [arXiv](https://arxiv.org/abs/2512.07132v1) · [PDF](https://arxiv.org/pdf/2512.07132v1) · 代码: https://github.com/nsivaku/dart\\n- 作者: Nithin Sivakumaran 等\\n- 任务/场景: 多智能体视觉问答/多模态推理中的工具调用决策。\\n- 核心实验方法:\\n  - 用“多代理争论的分歧”触发工具招聘（检测/OCR/空间推理等）。\\n  - 利用工具对齐的一致性评分，辅助聚合器代理选择答案。\\n- 评价/结果: 超过“多代理辩论+裁判”等强基线，A-OKVQA/MMMU/M3D均有提升。\\n\\n## 6) VIGIL: A Reflective Runtime for Self-Healing Agents (2025-12-08)\\n- 链接: [arXiv](https://arxiv.org/abs/2512.07094v1) · [PDF](https://arxiv.org/pdf/2512.07094v1)\\n- 作者: Christopher Cruz\\n- 任务/场景: 运行时自省与自修复，提升Agent可靠性。\\n- 核心实验方法:\\n  - 旁路“监护”代理：事件日志→情感化表征→EmoBank记忆→RBT诊断（优势/机会/失败）。\\n  - 生成“受保护”的提示更新与只读代码改动建议；状态门控流程，非法跃迁显式报错。\\n- 评价/结果: 案例展示在真实系统中实现元层面的自修复。\\n\\n## 7) MASim: Multilingual Agent-Based Simulation (2025-12-08)\\n- 链接: [arXiv](https://arxiv.org/abs/2512.07195v1) · [PDF](https://arxiv.org/pdf/2512.07195v1)\\n- 作者: Xuan Zhang 等\\n- 任务/场景: 多语言社会行为仿真，跨语言互动与信息扩散。\\n- 核心实验方法:\\n  - 多语言生成式代理框架；构建MAPS基准（问卷+人口画像）。\\n  - 评估：校准/敏感性/一致性与文化案例，面向计算社会科学。\\n\\n## 8) VulnLLM-R: Reasoning LLM + Agent Scaffold for Vuln Detection (2025-12-08)\\n- 链接: [arXiv](https://arxiv.org/abs/2512.07533v1) · [PDF](https://arxiv.org/pdf/2512.07533v1) · 代码: https://github.com/ucsb-mlsec/VulnLLM-R\\n- 作者: Yuzhou Nie 等\\n- 任务/场景: 跨语言/多语言代码漏洞检测，项目级实战。\\n- 核心实验方法:\\n  - 专项“推理”LLM训练配方：数据筛选→推理数据生成/过滤/纠错→测试期优化。\\n  - 以Agent脚手架集成工具链，优于CodeQL/AFL++并发现多项零日。\\n\\n## 9) Training LMs to Use Prolog as a Tool (2025-12-08)\\n- 链接: [arXiv](https://arxiv.org/abs/2512.07407v1) · [PDF](https://arxiv.org/pdf/2512.07407v1) · 代码: https://github.com/niklasmellgren/grpo-prolog-inference\\n- 作者: Niklas Mellgren 等\\n- 任务/场景: 将Prolog作为可验证外部工具，提高推理可审计性与可靠性。\\n- 核心实验方法:\\n  - 用GRPO强化学习微调3B模型，系统探索提示结构、奖励构成（执行/语法/语义/结构）与推理协议（单发/Best-of-N/两种Agent化模式）。\\n  - “外部验证的Best-of-N”在GSM8K上最准；“内部修复的Agent化推理”在MMLU泛化更好。\\n\\n## 10) The Adoption and Usage of AI Agents (Perplexity) (2025-12-08)\\n- 链接: [arXiv](https://arxiv.org/abs/2512.07828v1) · [PDF](https://arxiv.org/pdf/2512.07828v1)\\n- 作者: Jeremy Yang 等\\n- 任务/场景: 大规模真实世界AI代理使用行为研究（Perplexity Comet）。\\n- 核心实验方法:\\n  - 基于数亿匿名交互构建“分层Agentic任务分类法”，刻画主题/子主题/任务与粘性变化。\\n  - 细分人群/国家/职业差异，辅以长期迁移到更高认知任务的趋势分析。\\n\\n## 11) Reformulate, Retrieve, Localize: Repo-level Bug Localization (2025-12-07)\\n- 链接: [arXiv](https://arxiv.org/abs/2512.07022v1) · [PDF](https://arxiv.org/pdf/2512.07022v1)\\n- 作者: Genevieve Caumartin, Glaucia Melo\\n- 任务/场景: 代码库级缺陷定位代理。\\n- 核心实验方法:\\n  - 轻量级查询重写/摘要→BM25检索编排，自动化本地化工作流。\\n  - 相比BM25与SWE-agent，在Top-1与文件检索指标显著提升。\\n\\n# 复用建议（模板）\\n- 训练/优化:\\n  - 若奖励稀疏：采用PRS式阶段密集奖励；长答案引入LLM-as-a-Judge并防reward hacking。\\n  - 采样与更新：用VSPO思路替换低价值样本，配合value-smoothing clipping 稳定训练。\\n- 记忆/工具图:\\n  - 将历史轨迹解析成“工具图”，在边上缓存压缩状态，按置信度在回忆与例行之间切换。\\n- 多代理决策与工具调度:\\n  - 先行“分歧探测”，再按分歧类型定向招聘工具；引入与工具对齐的评分辅助聚合裁决。\\n- 可靠性/自修复:\\n  - 运行时日志→情感/事件表征→诊断→“受保护”提示与只读改动提案；用状态门控流程约束跃迁。\\n- 可验证工具链:\\n  - 在数学/逻辑等场景把形式系统（如Prolog）接入，采用外部验证+Best-of-N 或内部修复式Agent推理。\\n- 评测与基准:\\n  - 结合领域基准（如LocalSearchBench）与行为分析（如KAMI），增加完整性/忠实性/恢复能力等维度。\"}]}"}}]}, {"role": "tool", "content": "[\"{\\\"pages\\\":[{\\\"id\\\":\\\"2c4b93bb-159c-812c-8e81-ef9e5291d720\\\",\\\"url\\\":\\\"https://www.notion.so/2c4b93bb159c812c8e81ef9e5291d720\\\",\\\"properties\\\":{}}]}\"]", "tool_call_id": "call_x7bEnyWoeYd6WNFapWODjL48"}, {"role": "assistant", "content": "我已为你在 arXiv 搜集了最新的 Agent/Agentic AI 相关论文，并将核心实验方法总结到了一个新的 Notion 页面。你可以在这里查看与继续编辑：\nhttps://www.notion.so/2c4b93bb159c812c8e81ef9e5291d720\n\n需要我把页面标题加上你指定的前缀或标签吗？如果有具体格式（例如“【最新】Agent论文精选 - 2025-12-08”），告诉我，我马上更新。"}, {"role": "user", "content": "hello"}, {"role": "assistant", "content": "你好！我已把最新的 Agent/Agentic AI 论文及核心实验方法整理到你的 Notion 页面：\nhttps://www.notion.so/2c4b93bb159c812c8e81ef9e5291d720\n\n需要我给页面标题加上什么前缀/标签吗？例如：“【最新】Agent 论文精选 - 2025-12-09”。告诉我你想要的格式，我马上更新。"}, {"role": "user", "content": "hi"}], "tool_calls": [], "response_content": "嗨！页面已创建并填好内容。你之前说“标题要加上”，想加什么前缀/格式呢？例如：\n- 【最新】Agent 论文精选 - 2025-12-09\n- [Agentic AI] 最新论文与实验方法\n\n告诉我你偏好的格式，我立刻把 Notion 页面标题改好。需要我后续按天自动追加最新文章吗？"}
