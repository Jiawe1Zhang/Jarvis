{"timestamp": "2025-12-06T08:27:14.736057Z", "type": "info", "task": "帮我查一下最近关于 RAG Agent 的 arXiv 论文，总结一下他们的核心观点。然后把总结写到 Notion 的页面里去。", "message": "run_start"}
{"timestamp": "2025-12-06T08:27:14.740293Z", "type": "info", "reason": "config.disabled", "message": "rag_disabled"}
{"timestamp": "2025-12-06T08:27:58.719893Z", "type": "llm_call", "model": "gpt-5", "messages": [{"role": "system", "content": "You are an intelligent assistant equipped with specific tools.\n\n### WORKFLOW\n1. **Analyze Request**: Determine if the user's request requires performing actions (e.g., file I/O, web fetching) that your tools can handle.\n2. **Select Tool**: If an action is needed, select the appropriate tool from your available list.\n3. **Execute via Function Call**:\n   - You MUST use the native **function calling** mechanism to invoke the tool.\n   - Do NOT output the JSON arguments inside the conversation text (content).\n   - The system will only execute tools passed through the dedicated `tool_calls` field, NOT those written in the message body.\n\n### STRICT RULES\n- **NO JSON IN TEXT**: Never output raw JSON or code blocks containing tool arguments in your response text.\n- **Context Awareness**: Rely on the provided context. Do not invent file paths or data that do not exist.\n- **Fabrication Prohibited**: Do not make up file paths or information not present in the context.\n\nYou are a stateful assistant with access to prior conversation history. Use it to stay consistent, avoid repetition, and reference past context when helpful."}, {"role": "user", "content": "帮我查一下最近关于 RAG Agent 的 arXiv 论文，总结一下他们的核心观点。然后把总结写到 Notion 的页面里去。"}], "tool_calls": [{"id": "call_4aVFbtd5SHoo6jnwSWviCHj0", "name": "search_papers", "arguments": "{\"query\": \"ti:\\\"RAG agent\\\" OR abs:\\\"RAG agent\\\"\", \"max_results\": 25}"}, {"id": "call_BiDHkzDWmSntNwWBNIlWKkQ5", "name": "search_papers", "arguments": "{\"query\": \"(ti:\\\"retrieval-augmented\\\" AND (ti:agent OR abs:agent)) OR abs:\\\"retrieval-augmented agent\\\"\", \"max_results\": 25}"}, {"id": "call_wxFZHag4GpUWkgJCVqzkRRbO", "name": "search_papers", "arguments": "{\"query\": \"ti:\\\"agentic RAG\\\" OR abs:\\\"agentic RAG\\\" OR ti:\\\"agentic retrieval\\\" OR abs:\\\"agentic retrieval\\\"\", \"max_results\": 25}"}], "response_content": ""}
{"timestamp": "2025-12-06T08:28:00.481876Z", "type": "tool_call", "tool": "search_papers", "args": {"query": "ti:\"RAG agent\" OR abs:\"RAG agent\"", "max_results": 25}, "result": ["Search Results:\n\n1. Evaluating Long-Context Reasoning in LLM-Based WebAgents\n   Authors: Andy Chung, Yichi Zhang, Kaixiang Lin, Aditya Rawal, Qiaozi Gao, Joyce Chai\n   ID: 2512.04307v1\n   Categories: Primary: cs.LG, Additional: cs.AI\n   Published: 2025-12-03T22:53:10Z\n   Preview: As large language model (LLM)-based agents become increasingly integrated into daily digital interactions, their ability to reason across long interaction histories becomes crucial for providing perso...\n\n2. A Hierarchical Tree-based approach for creating Configurable and Static Deep Research Agent (Static-DRA)\n   Authors: Saurav Prateek\n   ID: 2512.03887v2\n   Categories: Primary: cs.AI\n   Published: 2025-12-03T15:37:13Z\n   Preview: The advancement in Large Language Models has driven the creation of complex agentic systems, such as Deep Research Agents (DRAs), to overcome the limitations of static Retrieval Augmented Generation (...\n\n3. Think Before You Drive: World Model-Inspired Multimodal Grounding for Autonomous Vehicles\n   Authors: Haicheng Liao, Huanming Shen, Bonan Wang, Yongkang Li, Yihong Tang, Chengyue Wang, Dingyi Zhuang, Kehua Chen, Hai Yang, Chengzhong Xu, Zhenning Li\n   ID: 2512.03454v1\n   Categories: Primary: cs.CV, Additional: cs.AI\n   Published: 2025-12-03T05:14:16Z\n   Preview: Interpreting natural-language commands to localize target objects is critical for autonomous driving (AD).\n\n4. BookRAG: A Hierarchical Structure-aware Index-based Approach for Retrieval-Augmented Generation on Complex Documents\n   Authors: Shu Wang, Yingli Zhou, Yixiang Fang\n   ID: 2512.03413v1\n   Categories: Primary: cs.IR, Additional: cs.AI\n   Published: 2025-12-03T03:40:49Z\n   Preview: As an effective method to boost the performance of Large Language Models (LLMs) on the question answering (QA) task, Retrieval-Augmented Generation (RAG), which queries highly relevant information fro...\n\n5. Aetheria: A multimodal interpretable content safety framework based on multi-agent debate and collaboration\n   Authors: Yuxiang He, Jian Zhao, Yuchen Yuan, Tianle Zhang, Wei Cai, Haojie Cheng, Ziyan Shi, Ming Zhu, Haichuan Tang, Chi Zhang, Xuelong Li\n   ID: 2512.02530v1\n   Categories: Primary: cs.AI\n   Published: 2025-12-02T08:49:54Z\n   Preview: The exponential growth of digital content presents significant challenges for content safety.\n\n6. CogEvo-Edu: Cognitive Evolution Educational Multi-Agent Collaborative System\n   Authors: Yefeng Wu, Yuchen Song, Yecheng Zhao, Ling Wu, Shan Wan\n   ID: 2512.00331v1\n   Categories: Primary: cs.AI, Additional: cs.MA\n   Published: 2025-11-29T05:41:57Z\n   Preview: Large language models (LLMs) are increasingly deployed as conversational tutors in STEM education, yet most systems still rely on a single LLM with a static retrieval-augmented generation (RAG) pipeli...\n\n7. MegaChat: A Synthetic Persian Q&A Dataset for High-Quality Sales Chatbot Evaluation\n   Authors: Mahdi Rahmani, AmirHossein Saffari, Reyhane Rahmani\n   ID: 2511.23397v1\n   Categories: Primary: cs.CL, Additional: cs.AI, cs.MA\n   Published: 2025-11-28T17:44:20Z\n   Preview: Small and medium-sized enterprises (SMEs) in Iran increasingly leverage Telegram for sales, where real-time engagement is essential for conversion.\n\n8. MCP vs RAG vs NLWeb vs HTML: A Comparison of the Effectiveness and Efficiency of Different Agent Interfaces to the Web (Technical Report)\n   Authors: Aaron Steiner, Ralph Peeters, Christian Bizer\n   ID: 2511.23281v1\n   Categories: Primary: cs.CL\n   Published: 2025-11-28T15:32:15Z\n   Preview: Large language model agents are increasingly used to automate web tasks such as product search, offer comparison, and checkout.\n\n9. RAG-Empowered LLM-Driven Dynamic Radio Resource Management in Open 6G RAN\n   Authors: Onur Salan, Burak Çırağ, Onur Sever, İbrahim Hökelek, Ali Görçin, Hakan Ali Çırpan\n   ID: 2511.22933v1\n   Categories: Primary: eess.SP\n   Published: 2025-11-28T07:20:25Z\n   Preview: Implications of the advancements in the area of artificial intelligence to the wireless communications is extremely significant, especially in terms of resource management.\n\n10. Advancing Automated In-Isolation Validation in Repository-Level Code Translation\n   Authors: Kaiyao Ke, Ali Reza Ibrahimzada, Rangeet Pan, Saurabh Sinha, Reyhaneh Jabbarvand\n   ID: 2511.21878v1\n   Categories: Primary: cs.SE, Additional: cs.PL\n   Published: 2025-11-26T19:53:46Z\n   Preview: Repository-level code translation aims to migrate entire repositories across programming languages while preserving functionality automatically.\n\n11. Chatty-KG: A Multi-Agent AI System for On-Demand Conversational Question Answering over Knowledge Graphs\n   Authors: Reham Omar, Abdelghny Orogat, Ibrahim Abdelaziz, Omij Mangukiya, Panos Kalnis, Essam Mansour\n   ID: 2511.20940v1\n   Categories: Primary: cs.CL\n   Published: 2025-11-26T00:18:55Z\n   Preview: Conversational Question Answering over Knowledge Graphs (KGs) combines the factual grounding of KG-based QA with the interactive nature of dialogue systems.\n\n12. HKRAG: Holistic Knowledge Retrieval-Augmented Generation Over Visually-Rich Documents\n   Authors: Anyang Tong, Xiang Niu, ZhiPing Liu, Chang Tian, Yanyan Wei, Zenglin Shi, Meng Wang\n   ID: 2511.20227v1\n   Categories: Primary: cs.IR\n   Published: 2025-11-25T11:59:52Z\n   Preview: Existing multimodal Retrieval-Augmented Generation (RAG) methods for visually rich documents (VRD) are often biased towards retrieving salient knowledge(e.g., prominent text and visual elements), whil...\n\n13. Large Language Model Aided Birt-Hogg-Dube Syndrome Diagnosis with Multimodal Retrieval-Augmented Generation\n   Authors: Haoqing Li, Jun Shi, Xianmeng Chen, Qiwei Jia, Rui Wang, Wei Wei, Hong An, Xiaowen Hu\n   ID: 2511.19834v1\n   Categories: Primary: cs.CV\n   Published: 2025-11-25T01:55:23Z\n   Preview: Deep learning methods face dual challenges of limited clinical samples and low inter-class differentiation among Diffuse Cystic Lung Diseases (DCLDs) in advancing Birt-Hogg-Dube syndrome (BHD) diagnos...\n\n14. Beyond Protein Language Models: An Agentic LLM Framework for Mechanistic Enzyme Design\n   Authors: Bruno Jacob, Khushbu Agarwal, Marcel Baer, Peter Rice, Simone Raugei\n   ID: 2511.19423v1\n   Categories: Primary: q-bio.QM, Additional: cs.AI\n   Published: 2025-11-24T18:57:07Z\n   Preview: We present Genie-CAT, a tool-augmented large-language-model (LLM) system designed to accelerate scientific hypothesis generation in protein design.\n\n15. Cross-Disciplinary Knowledge Retrieval and Synthesis: A Compound AI Architecture for Scientific Discovery\n   Authors: Svitlana Volkova, Peter Bautista, Avinash Hiriyanna, Gabriel Ganberg, Isabel Erickson, Zachary Klinefelter, Nick Abele, Hsien-Te Kao, Grant Engberson\n   ID: 2511.18298v1\n   Categories: Primary: cs.AI\n   Published: 2025-11-23T05:33:11Z\n   Preview: The exponential growth of scientific knowledge has created significant barriers to cross-disciplinary knowledge discovery, synthesis and research collaboration.\n\n16. Rethinking Retrieval: From Traditional Retrieval Augmented Generation to Agentic and Non-Vector Reasoning Systems in the Financial Domain for Large Language Models\n   Authors: Elias Lumer, Matt Melich, Olivia Zino, Elena Kim, Sara Dieter, Pradeep Honaganahalli Basavaraju, Vamse Kumar Subbiah, James A. Burke, Roberto Hernandez\n   ID: 2511.18177v1\n   Categories: Primary: cs.CL\n   Published: 2025-11-22T20:06:25Z\n   Preview: Recent advancements in Retrieval-Augmented Generation (RAG) have enabled Large Language Models to answer financial questions using external knowledge bases of U.S.\n\n17. AI-Augmented Bibliometric Framework: A Paradigm Shift with Agentic AI for Dynamic, Snippet-Based Research Analysis\n   Authors: Adela Bara, Simona-Vasilica Oprea\n   ID: 2511.21745v1\n   Categories: Primary: cs.DL\n   Published: 2025-11-22T08:41:38Z\n   Preview: Our paper introduces a generative, multiagent AI framework designed to overcome the rigidity, limited flexibility and technical barriers of current bibliometric tools.\n\n18. PersonaAgent with GraphRAG: Community-Aware Knowledge Graphs for Personalized LLM\n   Authors: Siqi Liang, Yudi Zhang, Yue Guo\n   ID: 2511.17467v2\n   Categories: Primary: cs.LG, Additional: cs.AI\n   Published: 2025-11-21T18:15:47Z\n   Preview: We propose a novel framework for persona-based language model system, motivated by the need for personalized AI agents that adapt to individual user preferences.\n\n19. REMSA: An LLM Agent for Foundation Model Selection in Remote Sensing\n   Authors: Binger Chen, Tacettin Emre Bök, Behnood Rasti, Volker Markl, Begüm Demir\n   ID: 2511.17442v1\n   Categories: Primary: cs.CV, Additional: cs.AI\n   Published: 2025-11-21T17:41:26Z\n   Preview: Foundation Models (FMs) are increasingly used in remote sensing (RS) for tasks such as environmental monitoring, disaster assessment, and land-use mapping.\n\n20. Beyond Component Strength: Synergistic Integration and Adaptive Calibration in Multi-Agent RAG Systems\n   Authors: Jithin Krishnan\n   ID: 2511.21729v1\n   Categories: Primary: cs.CL, Additional: cs.AI\n   Published: 2025-11-21T07:53:56Z\n   Preview: Building reliable retrieval-augmented generation (RAG) systems requires more than adding powerful components; it requires understanding how they interact.\n\n21. LLM and Agent-Driven Data Analysis: A Systematic Approach for Enterprise Applications and System-level Deployment\n   Authors: Xi Wang, Xianyao Ling, Kun Li, Gang Yin, Liang Zhang, Jiang Wu, Annie Wang, Weizhe Wang\n   ID: 2511.17676v1\n   Categories: Primary: cs.DB, Additional: cs.AI, cs.CL\n   Published: 2025-11-21T07:16:31Z\n   Preview: The rapid progress in Generative AI and Agent technologies is profoundly transforming enterprise data management and analytics.\n\n22. SurvAgent: Hierarchical CoT-Enhanced Case Banking and Dichotomy-Based Multi-Agent System for Multimodal Survival Prediction\n   Authors: Guolin Huang, Wenting Chen, Jiaqi Yang, Xinheng Lyu, Xiaoling Luo, Sen Yang, Xiaohan Xing, Linlin Shen\n   ID: 2511.16635v1\n   Categories: Primary: cs.CV, Additional: cs.CL\n   Published: 2025-11-20T18:41:44Z\n   Preview: Survival analysis is critical for cancer prognosis and treatment planning, yet existing methods lack the transparency essential for clinical adoption.\n\n23. CorrectHDL: Agentic HDL Design with LLMs Leveraging High-Level Synthesis as Reference\n   Authors: Kangwei Xu, Grace Li Zhang, Ulf Schlichtmann, Bing Li\n   ID: 2511.16395v1\n   Categories: Primary: cs.AI, Additional: cs.PL, cs.SE, eess.SY\n   Published: 2025-11-20T14:13:38Z\n   Preview: Large Language Models (LLMs) have demonstrated remarkable potential in hardware front-end design using hardware description languages (HDLs).\n\n24. Operon: Incremental Construction of Ragged Data via Named Dimensions\n   Authors: Sungbin Moon, Jiho Park, Suyoung Hwang, Donghyun Koh, Seunghyun Moon, Minhyeong Lee\n   ID: 2511.16080v1\n   Categories: Primary: cs.PL, Additional: cs.AI, cs.LG\n   Published: 2025-11-20T06:16:31Z\n   Preview: Modern data processing workflows frequently encounter ragged data: collections with variable-length elements that arise naturally in domains like natural language processing, scientific measurements,...\n\n25. KRAL: Knowledge and Reasoning Augmented Learning for LLM-assisted Clinical Antimicrobial Therapy\n   Authors: Zhe Li, Yehan Qiu, Yujie Chen, Xiang Zhou\n   ID: 2511.15974v4\n   Categories: Primary: cs.AI\n   Published: 2025-11-20T02:04:46Z\n   Preview: Clinical antimicrobial therapy requires the dynamic integration of pathogen profiles,host factors, pharmacological properties of antimicrobials,and the severity of infection.\n\n"]}
{"timestamp": "2025-12-06T08:28:03.246631Z", "type": "tool_call", "tool": "search_papers", "args": {"query": "(ti:\"retrieval-augmented\" AND (ti:agent OR abs:agent)) OR abs:\"retrieval-augmented agent\"", "max_results": 25}, "result": ["Search Results:\n\n1. The Personalization Paradox: Semantic Loss vs. Reasoning Gains in Agentic AI Q&A\n   Authors: Satyajit Movidi, Stephen Russell\n   ID: 2512.04343v1\n   Categories: Primary: cs.IR\n   Published: 2025-12-04T00:12:41Z\n   Preview: AIVisor, an agentic retrieval-augmented LLM for student advising, was used to examine how personalization affects system performance across multiple evaluation dimensions.\n\n2. A Hierarchical Tree-based approach for creating Configurable and Static Deep Research Agent (Static-DRA)\n   Authors: Saurav Prateek\n   ID: 2512.03887v2\n   Categories: Primary: cs.AI\n   Published: 2025-12-03T15:37:13Z\n   Preview: The advancement in Large Language Models has driven the creation of complex agentic systems, such as Deep Research Agents (DRAs), to overcome the limitations of static Retrieval Augmented Generation (...\n\n3. OmniDexVLG: Learning Dexterous Grasp Generation from Vision Language Model-Guided Grasp Semantics, Taxonomy and Functional Affordance\n   Authors: Lei Zhang, Diwen Zheng, Kaixin Bai, Zhenshan Bing, Zoltan-Csaba Marton, Zhaopeng Chen, Alois Christian Knoll, Jianwei Zhang\n   ID: 2512.03874v1\n   Categories: Primary: cs.RO, Additional: cs.LG\n   Published: 2025-12-03T15:28:23Z\n   Preview: Dexterous grasp generation aims to produce grasp poses that align with task requirements and human interpretable grasp semantics.\n\n4. Think Before You Drive: World Model-Inspired Multimodal Grounding for Autonomous Vehicles\n   Authors: Haicheng Liao, Huanming Shen, Bonan Wang, Yongkang Li, Yihong Tang, Chengyue Wang, Dingyi Zhuang, Kehua Chen, Hai Yang, Chengzhong Xu, Zhenning Li\n   ID: 2512.03454v1\n   Categories: Primary: cs.CV, Additional: cs.AI\n   Published: 2025-12-03T05:14:16Z\n   Preview: Interpreting natural-language commands to localize target objects is critical for autonomous driving (AD).\n\n5. Multi-Aspect Knowledge-Enhanced Medical Vision-Language Pretraining with Multi-Agent Data Generation\n   Authors: Xieji Li, Siyuan Yan, Yingsheng Liu, H. Peter Soyer, Monika Janda, Victoria Mar, Zongyuan Ge\n   ID: 2512.03445v1\n   Categories: Primary: cs.CV, Additional: cs.AI\n   Published: 2025-12-03T04:55:54Z\n   Preview: Vision-language pretraining (VLP) has emerged as a powerful paradigm in medical image analysis, enabling representation learning from large-scale image-text pairs without relying on expensive manual a...\n\n6. HarnessAgent: Scaling Automatic Fuzzing Harness Construction with Tool-Augmented LLM Pipelines\n   Authors: Kang Yang, Yunhang Zhang, Zichuan Li, GuanHong Tao, Jun Xu, XiaoJing Liao\n   ID: 2512.03420v1\n   Categories: Primary: cs.CR, Additional: cs.SE\n   Published: 2025-12-03T03:55:09Z\n   Preview: Large language model (LLM)-based techniques have achieved notable progress in generating harnesses for program fuzzing.\n\n7. BookRAG: A Hierarchical Structure-aware Index-based Approach for Retrieval-Augmented Generation on Complex Documents\n   Authors: Shu Wang, Yingli Zhou, Yixiang Fang\n   ID: 2512.03413v1\n   Categories: Primary: cs.IR, Additional: cs.AI\n   Published: 2025-12-03T03:40:49Z\n   Preview: As an effective method to boost the performance of Large Language Models (LLMs) on the question answering (QA) task, Retrieval-Augmented Generation (RAG), which queries highly relevant information fro...\n\n8. WorldMM: Dynamic Multimodal Memory Agent for Long Video Reasoning\n   Authors: Woongyeong Yeo, Kangsan Kim, Jaehong Yoon, Sung Ju Hwang\n   ID: 2512.02425v1\n   Categories: Primary: cs.CV, Additional: cs.AI, cs.CL, cs.IR, cs.LG\n   Published: 2025-12-02T05:14:52Z\n   Preview: Recent advances in video large language models have demonstrated strong capabilities in understanding short clips.\n\n9. MMAG: Mixed Memory-Augmented Generation for Large Language Models Applications\n   Authors: Stefano Zeppieri\n   ID: 2512.01710v2\n   Categories: Primary: cs.CL, Additional: cs.IR\n   Published: 2025-12-01T14:16:57Z\n   Preview: Large Language Models (LLMs) excel at generating coherent text within a single prompt but fall short in sustaining relevance, personalization, and continuity across extended interactions.\n\n10. Memory-Integrated Reconfigurable Adapters: A Unified Framework for Settings with Multiple Tasks\n   Authors: Susmit Agrawal, Krishn Vishwas Kher, Saksham Mittal, Swarnim Maheshwari, Vineeth N. Balasubramanian\n   ID: 2512.00940v1\n   Categories: Primary: cs.LG\n   Published: 2025-11-30T15:45:55Z\n   Preview: Organisms constantly pivot between tasks such as evading predators, foraging, traversing rugged terrain, and socializing, often within milliseconds.\n\n11. ML-Tool-Bench: Tool-Augmented Planning for ML Tasks\n   Authors: Yaswanth Chittepu, Raghavendra Addanki, Tung Mai, Anup Rao, Branislav Kveton\n   ID: 2512.00672v1\n   Categories: Primary: cs.LG, Additional: cs.AI\n   Published: 2025-11-29T23:59:40Z\n   Preview: The development of autonomous machine learning (ML) agents capable of end-to-end data science workflows represents a significant frontier in artificial intelligence.\n\n12. CogEvo-Edu: Cognitive Evolution Educational Multi-Agent Collaborative System\n   Authors: Yefeng Wu, Yuchen Song, Yecheng Zhao, Ling Wu, Shan Wan\n   ID: 2512.00331v1\n   Categories: Primary: cs.AI, Additional: cs.MA\n   Published: 2025-11-29T05:41:57Z\n   Preview: Large language models (LLMs) are increasingly deployed as conversational tutors in STEM education, yet most systems still rely on a single LLM with a static retrieval-augmented generation (RAG) pipeli...\n\n13. Words into World: A Task-Adaptive Agent for Language-Guided Spatial Retrieval in AR\n   Authors: Lixing Guo, Tobias Höllerer\n   ID: 2512.00294v1\n   Categories: Primary: cs.CV, Additional: cs.AI, cs.HC\n   Published: 2025-11-29T03:29:15Z\n   Preview: Traditional augmented reality (AR) systems predominantly rely on fixed class detectors or fiducial markers, limiting their ability to interpret complex, open-vocabulary natural language queries.\n\n14. MegaChat: A Synthetic Persian Q&A Dataset for High-Quality Sales Chatbot Evaluation\n   Authors: Mahdi Rahmani, AmirHossein Saffari, Reyhane Rahmani\n   ID: 2511.23397v1\n   Categories: Primary: cs.CL, Additional: cs.AI, cs.MA\n   Published: 2025-11-28T17:44:20Z\n   Preview: Small and medium-sized enterprises (SMEs) in Iran increasingly leverage Telegram for sales, where real-time engagement is essential for conversion.\n\n15. MCP vs RAG vs NLWeb vs HTML: A Comparison of the Effectiveness and Efficiency of Different Agent Interfaces to the Web (Technical Report)\n   Authors: Aaron Steiner, Ralph Peeters, Christian Bizer\n   ID: 2511.23281v1\n   Categories: Primary: cs.CL\n   Published: 2025-11-28T15:32:15Z\n   Preview: Large language model agents are increasingly used to automate web tasks such as product search, offer comparison, and checkout.\n\n16. RAG-Empowered LLM-Driven Dynamic Radio Resource Management in Open 6G RAN\n   Authors: Onur Salan, Burak Çırağ, Onur Sever, İbrahim Hökelek, Ali Görçin, Hakan Ali Çırpan\n   ID: 2511.22933v1\n   Categories: Primary: eess.SP\n   Published: 2025-11-28T07:20:25Z\n   Preview: Implications of the advancements in the area of artificial intelligence to the wireless communications is extremely significant, especially in terms of resource management.\n\n17. Resolving Evidence Sparsity: Agentic Context Engineering for Long-Document Understanding\n   Authors: Keliang Liu, Zizhi Chen, Mingcheng Li, Jingqun Tang, Dingkang Yang, Lihua Zhang\n   ID: 2511.22850v1\n   Categories: Primary: cs.CV\n   Published: 2025-11-28T03:09:40Z\n   Preview: Document understanding is a long standing practical task.\n\n18. Real-Time Procedural Learning From Experience for AI Agents\n   Authors: Dasheng Bi, Yubin Hu, Mohammed N. Nasir\n   ID: 2511.22074v2\n   Categories: Primary: cs.AI, Additional: cs.IR\n   Published: 2025-11-27T03:51:49Z\n   Preview: Learning how to do things from trial and error in real time is a hallmark of biological intelligence, yet most LLM-based agents lack mechanisms to acquire procedural knowledge after deployment.\n\n19. Chatty-KG: A Multi-Agent AI System for On-Demand Conversational Question Answering over Knowledge Graphs\n   Authors: Reham Omar, Abdelghny Orogat, Ibrahim Abdelaziz, Omij Mangukiya, Panos Kalnis, Essam Mansour\n   ID: 2511.20940v1\n   Categories: Primary: cs.CL\n   Published: 2025-11-26T00:18:55Z\n   Preview: Conversational Question Answering over Knowledge Graphs (KGs) combines the factual grounding of KG-based QA with the interactive nature of dialogue systems.\n\n20. LLM-Driven Transient Stability Assessment: From Automated Simulation to Neural Architecture Design\n   Authors: Lianzhe Hu, Yu Wang, Bikash Pal\n   ID: 2511.20276v1\n   Categories: Primary: eess.SY\n   Published: 2025-11-25T13:05:02Z\n   Preview: This paper presents an LLM-driven, end-to-end workflow that addresses the lack of automation and intelligence in power system transient stability assessment (TSA).\n\n21. HKRAG: Holistic Knowledge Retrieval-Augmented Generation Over Visually-Rich Documents\n   Authors: Anyang Tong, Xiang Niu, ZhiPing Liu, Chang Tian, Yanyan Wei, Zenglin Shi, Meng Wang\n   ID: 2511.20227v1\n   Categories: Primary: cs.IR\n   Published: 2025-11-25T11:59:52Z\n   Preview: Existing multimodal Retrieval-Augmented Generation (RAG) methods for visually rich documents (VRD) are often biased towards retrieving salient knowledge(e.g., prominent text and visual elements), whil...\n\n22. M$^3$Prune: Hierarchical Communication Graph Pruning for Efficient Multi-Modal Multi-Agent Retrieval-Augmented Generation\n   Authors: Weizi Shao, Taolin Zhang, Zijie Zhou, Chen Chen, Chengyu Wang, Xiaofeng He\n   ID: 2511.19969v1\n   Categories: Primary: cs.AI\n   Published: 2025-11-25T06:29:13Z\n   Preview: Recent advancements in multi-modal retrieval-augmented generation (mRAG), which enhance multi-modal large language models (MLLMs) with external knowledge, have demonstrated that the collective intelli...\n\n23. AppSelectBench: Application-Level Tool Selection Benchmark\n   Authors: Tianyi Chen, Michael Solodko, Sen Wang, Jongwoo Ko, Junheng Hao, Colby Banbury, Sara Abdali, Saeed Amizadeh, Qing Xiao, Yinheng Li, Tianyu Ding, Kamran Ghasedi Dizaji, Suzhen Zheng, Hao Fan, Justin Wagle, Pashmina Cameron, Kazuhito Koishida\n   ID: 2511.19957v2\n   Categories: Primary: cs.CL\n   Published: 2025-11-25T06:06:17Z\n   Preview: Computer Using Agents (CUAs) are increasingly equipped with external tools, enabling them to perform complex and realistic tasks.\n\n24. Large Language Model Aided Birt-Hogg-Dube Syndrome Diagnosis with Multimodal Retrieval-Augmented Generation\n   Authors: Haoqing Li, Jun Shi, Xianmeng Chen, Qiwei Jia, Rui Wang, Wei Wei, Hong An, Xiaowen Hu\n   ID: 2511.19834v1\n   Categories: Primary: cs.CV\n   Published: 2025-11-25T01:55:23Z\n   Preview: Deep learning methods face dual challenges of limited clinical samples and low inter-class differentiation among Diffuse Cystic Lung Diseases (DCLDs) in advancing Birt-Hogg-Dube syndrome (BHD) diagnos...\n\n25. NOEM$^{3}$A: A Neuro-Symbolic Ontology-Enhanced Method for Multi-Intent Understanding in Mobile Agents\n   Authors: Ioannis Tzachristas, Aifen Sui\n   ID: 2511.19780v1\n   Categories: Primary: cs.AI\n   Published: 2025-11-24T23:14:45Z\n   Preview: We introduce a neuro-symbolic framework for multi-intent understanding in mobile AI agents by integrating a structured intent ontology with compact language models.\n\n"]}
{"timestamp": "2025-12-06T08:28:06.663738Z", "type": "tool_call", "tool": "search_papers", "args": {"query": "ti:\"agentic RAG\" OR abs:\"agentic RAG\" OR ti:\"agentic retrieval\" OR abs:\"agentic retrieval\"", "max_results": 25}, "result": ["Search Results:\n\n1. ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning\n   Authors: Shengyuan Ding, Xinyu Fang, Ziyu Liu, Yuhang Zang, Yuhang Cao, Xiangyu Zhao, Haodong Duan, Xiaoyi Dong, Jianze Liang, Bin Wang, Conghui He, Dahua Lin, Jiaqi Wang\n   ID: 2512.05111v1\n   Categories: Primary: cs.CV\n   Published: 2025-12-04T18:59:52Z\n   Preview: Reward models are critical for aligning vision-language systems with human preferences, yet current approaches suffer from hallucination, weak visual grounding, and an inability to use tools for verif...\n\n2. The Personalization Paradox: Semantic Loss vs. Reasoning Gains in Agentic AI Q&A\n   Authors: Satyajit Movidi, Stephen Russell\n   ID: 2512.04343v1\n   Categories: Primary: cs.IR\n   Published: 2025-12-04T00:12:41Z\n   Preview: AIVisor, an agentic retrieval-augmented LLM for student advising, was used to examine how personalization affects system performance across multiple evaluation dimensions.\n\n3. Evaluating Long-Context Reasoning in LLM-Based WebAgents\n   Authors: Andy Chung, Yichi Zhang, Kaixiang Lin, Aditya Rawal, Qiaozi Gao, Joyce Chai\n   ID: 2512.04307v1\n   Categories: Primary: cs.LG, Additional: cs.AI\n   Published: 2025-12-03T22:53:10Z\n   Preview: As large language model (LLM)-based agents become increasingly integrated into daily digital interactions, their ability to reason across long interaction histories becomes crucial for providing perso...\n\n4. SQuARE: Structured Query & Adaptive Retrieval Engine For Tabular Formats\n   Authors: Chinmay Gondhalekar, Urjitkumar Patel, Fang-Chun Yeh\n   ID: 2512.04292v1\n   Categories: Primary: cs.CL\n   Published: 2025-12-03T22:11:45Z\n   Preview: Accurate question answering over real spreadsheets remains difficult due to multirow headers, merged cells, and unit annotations that disrupt naive chunking, while rigid SQL views fail on files lackin...\n\n5. A Hierarchical Tree-based approach for creating Configurable and Static Deep Research Agent (Static-DRA)\n   Authors: Saurav Prateek\n   ID: 2512.03887v2\n   Categories: Primary: cs.AI\n   Published: 2025-12-03T15:37:13Z\n   Preview: The advancement in Large Language Models has driven the creation of complex agentic systems, such as Deep Research Agents (DRAs), to overcome the limitations of static Retrieval Augmented Generation (...\n\n6. OmniDexVLG: Learning Dexterous Grasp Generation from Vision Language Model-Guided Grasp Semantics, Taxonomy and Functional Affordance\n   Authors: Lei Zhang, Diwen Zheng, Kaixin Bai, Zhenshan Bing, Zoltan-Csaba Marton, Zhaopeng Chen, Alois Christian Knoll, Jianwei Zhang\n   ID: 2512.03874v1\n   Categories: Primary: cs.RO, Additional: cs.LG\n   Published: 2025-12-03T15:28:23Z\n   Preview: Dexterous grasp generation aims to produce grasp poses that align with task requirements and human interpretable grasp semantics.\n\n7. MemVerse: Multimodal Memory for Lifelong Learning Agents\n   Authors: Junming Liu, Yifei Sun, Weihua Cheng, Haodong Lei, Yirong Chen, Licheng Wen, Xuemeng Yang, Daocheng Fu, Pinlong Cai, Nianchen Deng, Yi Yu, Shuyue Hu, Botian Shi, Ding Wang\n   ID: 2512.03627v1\n   Categories: Primary: cs.AI\n   Published: 2025-12-03T10:06:14Z\n   Preview: Despite rapid progress in large-scale language and vision models, AI agents still suffer from a fundamental limitation: they cannot remember.\n\n8. Towards Object-centric Understanding for Instructional Videos\n   Authors: Wenliang Guo, Yu Kong\n   ID: 2512.03479v1\n   Categories: Primary: cs.CV\n   Published: 2025-12-03T06:14:26Z\n   Preview: Understanding procedural activities is crucial for developing future assistive AI that can reason about complex real-world tasks.\n\n9. Think Before You Drive: World Model-Inspired Multimodal Grounding for Autonomous Vehicles\n   Authors: Haicheng Liao, Huanming Shen, Bonan Wang, Yongkang Li, Yihong Tang, Chengyue Wang, Dingyi Zhuang, Kehua Chen, Hai Yang, Chengzhong Xu, Zhenning Li\n   ID: 2512.03454v1\n   Categories: Primary: cs.CV, Additional: cs.AI\n   Published: 2025-12-03T05:14:16Z\n   Preview: Interpreting natural-language commands to localize target objects is critical for autonomous driving (AD).\n\n10. Multi-Aspect Knowledge-Enhanced Medical Vision-Language Pretraining with Multi-Agent Data Generation\n   Authors: Xieji Li, Siyuan Yan, Yingsheng Liu, H. Peter Soyer, Monika Janda, Victoria Mar, Zongyuan Ge\n   ID: 2512.03445v1\n   Categories: Primary: cs.CV, Additional: cs.AI\n   Published: 2025-12-03T04:55:54Z\n   Preview: Vision-language pretraining (VLP) has emerged as a powerful paradigm in medical image analysis, enabling representation learning from large-scale image-text pairs without relying on expensive manual a...\n\n11. HarnessAgent: Scaling Automatic Fuzzing Harness Construction with Tool-Augmented LLM Pipelines\n   Authors: Kang Yang, Yunhang Zhang, Zichuan Li, GuanHong Tao, Jun Xu, XiaoJing Liao\n   ID: 2512.03420v1\n   Categories: Primary: cs.CR, Additional: cs.SE\n   Published: 2025-12-03T03:55:09Z\n   Preview: Large language model (LLM)-based techniques have achieved notable progress in generating harnesses for program fuzzing.\n\n12. BookRAG: A Hierarchical Structure-aware Index-based Approach for Retrieval-Augmented Generation on Complex Documents\n   Authors: Shu Wang, Yingli Zhou, Yixiang Fang\n   ID: 2512.03413v1\n   Categories: Primary: cs.IR, Additional: cs.AI\n   Published: 2025-12-03T03:40:49Z\n   Preview: As an effective method to boost the performance of Large Language Models (LLMs) on the question answering (QA) task, Retrieval-Augmented Generation (RAG), which queries highly relevant information fro...\n\n13. Spoken Conversational Agents with Large Language Models\n   Authors: Chao-Han Huck Yang, Andreas Stolcke, Larry Heck\n   ID: 2512.02593v1\n   Categories: Primary: cs.CL, Additional: cs.MA, cs.NE, cs.SD, eess.AS\n   Published: 2025-12-02T10:02:10Z\n   Preview: Spoken conversational agents are converging toward voice-native LLMs.\n\n14. EZYer: A simulacrum of high school with generative agent\n   Authors: Jinming Yang, Zimu Ji, Weiqi Luo, Gaoxi Wang, Bin Ma, Yueling Deng\n   ID: 2512.02561v1\n   Categories: Primary: cs.MA, Additional: cs.AI\n   Published: 2025-12-02T09:28:59Z\n   Preview: With the rapid development of the online education and large language model, the existing educational tools still suffer from incomplete service, insufficient performance and weak interactivity in ter...\n\n15. In-Context Distillation with Self-Consistency Cascades: A Simple, Training-Free Way to Reduce LLM Agent Costs\n   Authors: Vishnu Sarukkai, Asanshay Gupta, James Hong, Michaël Gharbi, Kayvon Fatahalian\n   ID: 2512.02543v1\n   Categories: Primary: cs.LG\n   Published: 2025-12-02T09:11:05Z\n   Preview: The world currently has an abundance of ideas for how to use new LLM agents, and developers seek to rapidly prototype and test new agentic designs.\n\n16. Aetheria: A multimodal interpretable content safety framework based on multi-agent debate and collaboration\n   Authors: Yuxiang He, Jian Zhao, Yuchen Yuan, Tianle Zhang, Wei Cai, Haojie Cheng, Ziyan Shi, Ming Zhu, Haichuan Tang, Chi Zhang, Xuelong Li\n   ID: 2512.02530v1\n   Categories: Primary: cs.AI\n   Published: 2025-12-02T08:49:54Z\n   Preview: The exponential growth of digital content presents significant challenges for content safety.\n\n17. WorldMM: Dynamic Multimodal Memory Agent for Long Video Reasoning\n   Authors: Woongyeong Yeo, Kangsan Kim, Jaehong Yoon, Sung Ju Hwang\n   ID: 2512.02425v1\n   Categories: Primary: cs.CV, Additional: cs.AI, cs.CL, cs.IR, cs.LG\n   Published: 2025-12-02T05:14:52Z\n   Preview: Recent advances in video large language models have demonstrated strong capabilities in understanding short clips.\n\n18. Skywork-R1V4: Toward Agentic Multimodal Intelligence through Interleaved Thinking with Images and DeepResearch\n   Authors: Yifan Zhang, Liang Hu, Haofeng Sun, Peiyu Wang, Yichen Wei, Shukang Yin, Jiangbo Pei, Wei Shen, Peng Xia, Yi Peng, Tianyidan Xie, Eric Li, Yang Liu, Xuchen Song, Yahui Zhou\n   ID: 2512.02395v1\n   Categories: Primary: cs.CV\n   Published: 2025-12-02T04:12:57Z\n   Preview: Despite recent progress in multimodal agentic systems, existing approaches often treat image manipulation and web search as disjoint capabilities, rely heavily on costly reinforcement learning, and la...\n\n19. Benchmarking LLM Agents for Wealth-Management Workflows\n   Authors: Rory Milsom\n   ID: 2512.02230v1\n   Categories: Primary: cs.AI\n   Published: 2025-12-01T21:56:21Z\n   Preview: Modern work relies on an assortment of digital collaboration tools, yet routine processes continue to suffer from human error and delay.\n\n20. How Far Are We from Genuinely Useful Deep Research Agents?\n   Authors: Dingling Zhang, He Zhu, Jincheng Ren, Kangqi Song, Xinran Zhou, Boyu Feng, Shudong Liu, Jiabin Luo, Weihao Xie, Zhaohui Wang, Tianrui Qin, King Zhu, Yuqing Wang, Qianben Chen, Yuchen Eleanor Jiang, Wei Wang, Jiaheng Liu, Wangchunshu Zhou\n   ID: 2512.01948v1\n   Categories: Primary: cs.CL\n   Published: 2025-12-01T17:58:59Z\n   Preview: Deep Research Agents (DRAs) aim to automatically produce analyst-level reports through iterative information retrieval and synthesis.\n\n21. Agentic Policy Optimization via Instruction-Policy Co-Evolution\n   Authors: Han Zhou, Xingchen Wan, Ivan Vulić, Anna Korhonen\n   ID: 2512.01945v1\n   Categories: Primary: cs.LG, Additional: cs.AI, cs.CL\n   Published: 2025-12-01T17:56:29Z\n   Preview: Reinforcement Learning with Verifiable Rewards (RLVR) has advanced the reasoning capability of large language models (LLMs), enabling autonomous agents that can conduct effective multi-turn and tool-i...\n\n22. MMAG: Mixed Memory-Augmented Generation for Large Language Models Applications\n   Authors: Stefano Zeppieri\n   ID: 2512.01710v2\n   Categories: Primary: cs.CL, Additional: cs.IR\n   Published: 2025-12-01T14:16:57Z\n   Preview: Large Language Models (LLMs) excel at generating coherent text within a single prompt but fall short in sustaining relevance, personalization, and continuity across extended interactions.\n\n23. Dr.Mi-Bench: A Modular-integrated Benchmark for Scientific Deep Research Agent\n   Authors: Zhihan Guo, Feiyang Xu, Yifan Li, Muzhi Li, Shuai Zou, Jiele Wu, Han Shi, Haoli Bai, Ho-fung Leung, Irwin King\n   ID: 2512.00986v1\n   Categories: Primary: cs.CL\n   Published: 2025-11-30T17:16:47Z\n   Preview: The explosive growth in academic literature necessitates automated deep research (DR) agents, yet their evaluation remains a significant challenge.\n\n24. Memory-Integrated Reconfigurable Adapters: A Unified Framework for Settings with Multiple Tasks\n   Authors: Susmit Agrawal, Krishn Vishwas Kher, Saksham Mittal, Swarnim Maheshwari, Vineeth N. Balasubramanian\n   ID: 2512.00940v1\n   Categories: Primary: cs.LG\n   Published: 2025-11-30T15:45:55Z\n   Preview: Organisms constantly pivot between tasks such as evading predators, foraging, traversing rugged terrain, and socializing, often within milliseconds.\n\n25. ML-Tool-Bench: Tool-Augmented Planning for ML Tasks\n   Authors: Yaswanth Chittepu, Raghavendra Addanki, Tung Mai, Anup Rao, Branislav Kveton\n   ID: 2512.00672v1\n   Categories: Primary: cs.LG, Additional: cs.AI\n   Published: 2025-11-29T23:59:40Z\n   Preview: The development of autonomous machine learning (ML) agents capable of end-to-end data science workflows represents a significant frontier in artificial intelligence.\n\n"]}
{"timestamp": "2025-12-06T08:28:25.222975Z", "type": "llm_call", "model": "gpt-5", "messages": [{"role": "system", "content": "You are an intelligent assistant equipped with specific tools.\n\n### WORKFLOW\n1. **Analyze Request**: Determine if the user's request requires performing actions (e.g., file I/O, web fetching) that your tools can handle.\n2. **Select Tool**: If an action is needed, select the appropriate tool from your available list.\n3. **Execute via Function Call**:\n   - You MUST use the native **function calling** mechanism to invoke the tool.\n   - Do NOT output the JSON arguments inside the conversation text (content).\n   - The system will only execute tools passed through the dedicated `tool_calls` field, NOT those written in the message body.\n\n### STRICT RULES\n- **NO JSON IN TEXT**: Never output raw JSON or code blocks containing tool arguments in your response text.\n- **Context Awareness**: Rely on the provided context. Do not invent file paths or data that do not exist.\n- **Fabrication Prohibited**: Do not make up file paths or information not present in the context.\n\nYou are a stateful assistant with access to prior conversation history. Use it to stay consistent, avoid repetition, and reference past context when helpful."}, {"role": "user", "content": "帮我查一下最近关于 RAG Agent 的 arXiv 论文，总结一下他们的核心观点。然后把总结写到 Notion 的页面里去。"}, {"role": "assistant", "content": "", "tool_calls": [{"id": "call_4aVFbtd5SHoo6jnwSWviCHj0", "type": "function", "function": {"name": "search_papers", "arguments": "{\"query\": \"ti:\\\"RAG agent\\\" OR abs:\\\"RAG agent\\\"\", \"max_results\": 25}"}}, {"id": "call_BiDHkzDWmSntNwWBNIlWKkQ5", "type": "function", "function": {"name": "search_papers", "arguments": "{\"query\": \"(ti:\\\"retrieval-augmented\\\" AND (ti:agent OR abs:agent)) OR abs:\\\"retrieval-augmented agent\\\"\", \"max_results\": 25}"}}, {"id": "call_wxFZHag4GpUWkgJCVqzkRRbO", "type": "function", "function": {"name": "search_papers", "arguments": "{\"query\": \"ti:\\\"agentic RAG\\\" OR abs:\\\"agentic RAG\\\" OR ti:\\\"agentic retrieval\\\" OR abs:\\\"agentic retrieval\\\"\", \"max_results\": 25}"}}]}, {"role": "tool", "content": "[\"Search Results:\\n\\n1. Evaluating Long-Context Reasoning in LLM-Based WebAgents\\n   Authors: Andy Chung, Yichi Zhang, Kaixiang Lin, Aditya Rawal, Qiaozi Gao, Joyce Chai\\n   ID: 2512.04307v1\\n   Categories: Primary: cs.LG, Additional: cs.AI\\n   Published: 2025-12-03T22:53:10Z\\n   Preview: As large language model (LLM)-based agents become increasingly integrated into daily digital interactions, their ability to reason across long interaction histories becomes crucial for providing perso...\\n\\n2. A Hierarchical Tree-based approach for creating Configurable and Static Deep Research Agent (Static-DRA)\\n   Authors: Saurav Prateek\\n   ID: 2512.03887v2\\n   Categories: Primary: cs.AI\\n   Published: 2025-12-03T15:37:13Z\\n   Preview: The advancement in Large Language Models has driven the creation of complex agentic systems, such as Deep Research Agents (DRAs), to overcome the limitations of static Retrieval Augmented Generation (...\\n\\n3. Think Before You Drive: World Model-Inspired Multimodal Grounding for Autonomous Vehicles\\n   Authors: Haicheng Liao, Huanming Shen, Bonan Wang, Yongkang Li, Yihong Tang, Chengyue Wang, Dingyi Zhuang, Kehua Chen, Hai Yang, Chengzhong Xu, Zhenning Li\\n   ID: 2512.03454v1\\n   Categories: Primary: cs.CV, Additional: cs.AI\\n   Published: 2025-12-03T05:14:16Z\\n   Preview: Interpreting natural-language commands to localize target objects is critical for autonomous driving (AD).\\n\\n4. BookRAG: A Hierarchical Structure-aware Index-based Approach for Retrieval-Augmented Generation on Complex Documents\\n   Authors: Shu Wang, Yingli Zhou, Yixiang Fang\\n   ID: 2512.03413v1\\n   Categories: Primary: cs.IR, Additional: cs.AI\\n   Published: 2025-12-03T03:40:49Z\\n   Preview: As an effective method to boost the performance of Large Language Models (LLMs) on the question answering (QA) task, Retrieval-Augmented Generation (RAG), which queries highly relevant information fro...\\n\\n5. Aetheria: A multimodal interpretable content safety framework based on multi-agent debate and collaboration\\n   Authors: Yuxiang He, Jian Zhao, Yuchen Yuan, Tianle Zhang, Wei Cai, Haojie Cheng, Ziyan Shi, Ming Zhu, Haichuan Tang, Chi Zhang, Xuelong Li\\n   ID: 2512.02530v1\\n   Categories: Primary: cs.AI\\n   Published: 2025-12-02T08:49:54Z\\n   Preview: The exponential growth of digital content presents significant challenges for content safety.\\n\\n6. CogEvo-Edu: Cognitive Evolution Educational Multi-Agent Collaborative System\\n   Authors: Yefeng Wu, Yuchen Song, Yecheng Zhao, Ling Wu, Shan Wan\\n   ID: 2512.00331v1\\n   Categories: Primary: cs.AI, Additional: cs.MA\\n   Published: 2025-11-29T05:41:57Z\\n   Preview: Large language models (LLMs) are increasingly deployed as conversational tutors in STEM education, yet most systems still rely on a single LLM with a static retrieval-augmented generation (RAG) pipeli...\\n\\n7. MegaChat: A Synthetic Persian Q&A Dataset for High-Quality Sales Chatbot Evaluation\\n   Authors: Mahdi Rahmani, AmirHossein Saffari, Reyhane Rahmani\\n   ID: 2511.23397v1\\n   Categories: Primary: cs.CL, Additional: cs.AI, cs.MA\\n   Published: 2025-11-28T17:44:20Z\\n   Preview: Small and medium-sized enterprises (SMEs) in Iran increasingly leverage Telegram for sales, where real-time engagement is essential for conversion.\\n\\n8. MCP vs RAG vs NLWeb vs HTML: A Comparison of the Effectiveness and Efficiency of Different Agent Interfaces to the Web (Technical Report)\\n   Authors: Aaron Steiner, Ralph Peeters, Christian Bizer\\n   ID: 2511.23281v1\\n   Categories: Primary: cs.CL\\n   Published: 2025-11-28T15:32:15Z\\n   Preview: Large language model agents are increasingly used to automate web tasks such as product search, offer comparison, and checkout.\\n\\n9. RAG-Empowered LLM-Driven Dynamic Radio Resource Management in Open 6G RAN\\n   Authors: Onur Salan, Burak \\u00c7\\u0131ra\\u011f, Onur Sever, \\u0130brahim H\\u00f6kelek, Ali G\\u00f6r\\u00e7in, Hakan Ali \\u00c7\\u0131rpan\\n   ID: 2511.22933v1\\n   Categories: Primary: eess.SP\\n   Published: 2025-11-28T07:20:25Z\\n   Preview: Implications of the advancements in the area of artificial intelligence to the wireless communications is extremely significant, especially in terms of resource management.\\n\\n10. Advancing Automated In-Isolation Validation in Repository-Level Code Translation\\n   Authors: Kaiyao Ke, Ali Reza Ibrahimzada, Rangeet Pan, Saurabh Sinha, Reyhaneh Jabbarvand\\n   ID: 2511.21878v1\\n   Categories: Primary: cs.SE, Additional: cs.PL\\n   Published: 2025-11-26T19:53:46Z\\n   Preview: Repository-level code translation aims to migrate entire repositories across programming languages while preserving functionality automatically.\\n\\n11. Chatty-KG: A Multi-Agent AI System for On-Demand Conversational Question Answering over Knowledge Graphs\\n   Authors: Reham Omar, Abdelghny Orogat, Ibrahim Abdelaziz, Omij Mangukiya, Panos Kalnis, Essam Mansour\\n   ID: 2511.20940v1\\n   Categories: Primary: cs.CL\\n   Published: 2025-11-26T00:18:55Z\\n   Preview: Conversational Question Answering over Knowledge Graphs (KGs) combines the factual grounding of KG-based QA with the interactive nature of dialogue systems.\\n\\n12. HKRAG: Holistic Knowledge Retrieval-Augmented Generation Over Visually-Rich Documents\\n   Authors: Anyang Tong, Xiang Niu, ZhiPing Liu, Chang Tian, Yanyan Wei, Zenglin Shi, Meng Wang\\n   ID: 2511.20227v1\\n   Categories: Primary: cs.IR\\n   Published: 2025-11-25T11:59:52Z\\n   Preview: Existing multimodal Retrieval-Augmented Generation (RAG) methods for visually rich documents (VRD) are often biased towards retrieving salient knowledge(e.g., prominent text and visual elements), whil...\\n\\n13. Large Language Model Aided Birt-Hogg-Dube Syndrome Diagnosis with Multimodal Retrieval-Augmented Generation\\n   Authors: Haoqing Li, Jun Shi, Xianmeng Chen, Qiwei Jia, Rui Wang, Wei Wei, Hong An, Xiaowen Hu\\n   ID: 2511.19834v1\\n   Categories: Primary: cs.CV\\n   Published: 2025-11-25T01:55:23Z\\n   Preview: Deep learning methods face dual challenges of limited clinical samples and low inter-class differentiation among Diffuse Cystic Lung Diseases (DCLDs) in advancing Birt-Hogg-Dube syndrome (BHD) diagnos...\\n\\n14. Beyond Protein Language Models: An Agentic LLM Framework for Mechanistic Enzyme Design\\n   Authors: Bruno Jacob, Khushbu Agarwal, Marcel Baer, Peter Rice, Simone Raugei\\n   ID: 2511.19423v1\\n   Categories: Primary: q-bio.QM, Additional: cs.AI\\n   Published: 2025-11-24T18:57:07Z\\n   Preview: We present Genie-CAT, a tool-augmented large-language-model (LLM) system designed to accelerate scientific hypothesis generation in protein design.\\n\\n15. Cross-Disciplinary Knowledge Retrieval and Synthesis: A Compound AI Architecture for Scientific Discovery\\n   Authors: Svitlana Volkova, Peter Bautista, Avinash Hiriyanna, Gabriel Ganberg, Isabel Erickson, Zachary Klinefelter, Nick Abele, Hsien-Te Kao, Grant Engberson\\n   ID: 2511.18298v1\\n   Categories: Primary: cs.AI\\n   Published: 2025-11-23T05:33:11Z\\n   Preview: The exponential growth of scientific knowledge has created significant barriers to cross-disciplinary knowledge discovery, synthesis and research collaboration.\\n\\n16. Rethinking Retrieval: From Traditional Retrieval Augmented Generation to Agentic and Non-Vector Reasoning Systems in the Financial Domain for Large Language Models\\n   Authors: Elias Lumer, Matt Melich, Olivia Zino, Elena Kim, Sara Dieter, Pradeep Honaganahalli Basavaraju, Vamse Kumar Subbiah, James A. Burke, Roberto Hernandez\\n   ID: 2511.18177v1\\n   Categories: Primary: cs.CL\\n   Published: 2025-11-22T20:06:25Z\\n   Preview: Recent advancements in Retrieval-Augmented Generation (RAG) have enabled Large Language Models to answer financial questions using external knowledge bases of U.S.\\n\\n17. AI-Augmented Bibliometric Framework: A Paradigm Shift with Agentic AI for Dynamic, Snippet-Based Research Analysis\\n   Authors: Adela Bara, Simona-Vasilica Oprea\\n   ID: 2511.21745v1\\n   Categories: Primary: cs.DL\\n   Published: 2025-11-22T08:41:38Z\\n   Preview: Our paper introduces a generative, multiagent AI framework designed to overcome the rigidity, limited flexibility and technical barriers of current bibliometric tools.\\n\\n18. PersonaAgent with GraphRAG: Community-Aware Knowledge Graphs for Personalized LLM\\n   Authors: Siqi Liang, Yudi Zhang, Yue Guo\\n   ID: 2511.17467v2\\n   Categories: Primary: cs.LG, Additional: cs.AI\\n   Published: 2025-11-21T18:15:47Z\\n   Preview: We propose a novel framework for persona-based language model system, motivated by the need for personalized AI agents that adapt to individual user preferences.\\n\\n19. REMSA: An LLM Agent for Foundation Model Selection in Remote Sensing\\n   Authors: Binger Chen, Tacettin Emre B\\u00f6k, Behnood Rasti, Volker Markl, Beg\\u00fcm Demir\\n   ID: 2511.17442v1\\n   Categories: Primary: cs.CV, Additional: cs.AI\\n   Published: 2025-11-21T17:41:26Z\\n   Preview: Foundation Models (FMs) are increasingly used in remote sensing (RS) for tasks such as environmental monitoring, disaster assessment, and land-use mapping.\\n\\n20. Beyond Component Strength: Synergistic Integration and Adaptive Calibration in Multi-Agent RAG Systems\\n   Authors: Jithin Krishnan\\n   ID: 2511.21729v1\\n   Categories: Primary: cs.CL, Additional: cs.AI\\n   Published: 2025-11-21T07:53:56Z\\n   Preview: Building reliable retrieval-augmented generation (RAG) systems requires more than adding powerful components; it requires understanding how they interact.\\n\\n21. LLM and Agent-Driven Data Analysis: A Systematic Approach for Enterprise Applications and System-level Deployment\\n   Authors: Xi Wang, Xianyao Ling, Kun Li, Gang Yin, Liang Zhang, Jiang Wu, Annie Wang, Weizhe Wang\\n   ID: 2511.17676v1\\n   Categories: Primary: cs.DB, Additional: cs.AI, cs.CL\\n   Published: 2025-11-21T07:16:31Z\\n   Preview: The rapid progress in Generative AI and Agent technologies is profoundly transforming enterprise data management and analytics.\\n\\n22. SurvAgent: Hierarchical CoT-Enhanced Case Banking and Dichotomy-Based Multi-Agent System for Multimodal Survival Prediction\\n   Authors: Guolin Huang, Wenting Chen, Jiaqi Yang, Xinheng Lyu, Xiaoling Luo, Sen Yang, Xiaohan Xing, Linlin Shen\\n   ID: 2511.16635v1\\n   Categories: Primary: cs.CV, Additional: cs.CL\\n   Published: 2025-11-20T18:41:44Z\\n   Preview: Survival analysis is critical for cancer prognosis and treatment planning, yet existing methods lack the transparency essential for clinical adoption.\\n\\n23. CorrectHDL: Agentic HDL Design with LLMs Leveraging High-Level Synthesis as Reference\\n   Authors: Kangwei Xu, Grace Li Zhang, Ulf Schlichtmann, Bing Li\\n   ID: 2511.16395v1\\n   Categories: Primary: cs.AI, Additional: cs.PL, cs.SE, eess.SY\\n   Published: 2025-11-20T14:13:38Z\\n   Preview: Large Language Models (LLMs) have demonstrated remarkable potential in hardware front-end design using hardware description languages (HDLs).\\n\\n24. Operon: Incremental Construction of Ragged Data via Named Dimensions\\n   Authors: Sungbin Moon, Jiho Park, Suyoung Hwang, Donghyun Koh, Seunghyun Moon, Minhyeong Lee\\n   ID: 2511.16080v1\\n   Categories: Primary: cs.PL, Additional: cs.AI, cs.LG\\n   Published: 2025-11-20T06:16:31Z\\n   Preview: Modern data processing workflows frequently encounter ragged data: collections with variable-length elements that arise naturally in domains like natural language processing, scientific measurements,...\\n\\n25. KRAL: Knowledge and Reasoning Augmented Learning for LLM-assisted Clinical Antimicrobial Therapy\\n   Authors: Zhe Li, Yehan Qiu, Yujie Chen, Xiang Zhou\\n   ID: 2511.15974v4\\n   Categories: Primary: cs.AI\\n   Published: 2025-11-20T02:04:46Z\\n   Preview: Clinical antimicrobial therapy requires the dynamic integration of pathogen profiles,host factors, pharmacological properties of antimicrobials,and the severity of infection.\\n\\n\"]", "tool_call_id": "call_4aVFbtd5SHoo6jnwSWviCHj0"}, {"role": "tool", "content": "[\"Search Results:\\n\\n1. The Personalization Paradox: Semantic Loss vs. Reasoning Gains in Agentic AI Q&A\\n   Authors: Satyajit Movidi, Stephen Russell\\n   ID: 2512.04343v1\\n   Categories: Primary: cs.IR\\n   Published: 2025-12-04T00:12:41Z\\n   Preview: AIVisor, an agentic retrieval-augmented LLM for student advising, was used to examine how personalization affects system performance across multiple evaluation dimensions.\\n\\n2. A Hierarchical Tree-based approach for creating Configurable and Static Deep Research Agent (Static-DRA)\\n   Authors: Saurav Prateek\\n   ID: 2512.03887v2\\n   Categories: Primary: cs.AI\\n   Published: 2025-12-03T15:37:13Z\\n   Preview: The advancement in Large Language Models has driven the creation of complex agentic systems, such as Deep Research Agents (DRAs), to overcome the limitations of static Retrieval Augmented Generation (...\\n\\n3. OmniDexVLG: Learning Dexterous Grasp Generation from Vision Language Model-Guided Grasp Semantics, Taxonomy and Functional Affordance\\n   Authors: Lei Zhang, Diwen Zheng, Kaixin Bai, Zhenshan Bing, Zoltan-Csaba Marton, Zhaopeng Chen, Alois Christian Knoll, Jianwei Zhang\\n   ID: 2512.03874v1\\n   Categories: Primary: cs.RO, Additional: cs.LG\\n   Published: 2025-12-03T15:28:23Z\\n   Preview: Dexterous grasp generation aims to produce grasp poses that align with task requirements and human interpretable grasp semantics.\\n\\n4. Think Before You Drive: World Model-Inspired Multimodal Grounding for Autonomous Vehicles\\n   Authors: Haicheng Liao, Huanming Shen, Bonan Wang, Yongkang Li, Yihong Tang, Chengyue Wang, Dingyi Zhuang, Kehua Chen, Hai Yang, Chengzhong Xu, Zhenning Li\\n   ID: 2512.03454v1\\n   Categories: Primary: cs.CV, Additional: cs.AI\\n   Published: 2025-12-03T05:14:16Z\\n   Preview: Interpreting natural-language commands to localize target objects is critical for autonomous driving (AD).\\n\\n5. Multi-Aspect Knowledge-Enhanced Medical Vision-Language Pretraining with Multi-Agent Data Generation\\n   Authors: Xieji Li, Siyuan Yan, Yingsheng Liu, H. Peter Soyer, Monika Janda, Victoria Mar, Zongyuan Ge\\n   ID: 2512.03445v1\\n   Categories: Primary: cs.CV, Additional: cs.AI\\n   Published: 2025-12-03T04:55:54Z\\n   Preview: Vision-language pretraining (VLP) has emerged as a powerful paradigm in medical image analysis, enabling representation learning from large-scale image-text pairs without relying on expensive manual a...\\n\\n6. HarnessAgent: Scaling Automatic Fuzzing Harness Construction with Tool-Augmented LLM Pipelines\\n   Authors: Kang Yang, Yunhang Zhang, Zichuan Li, GuanHong Tao, Jun Xu, XiaoJing Liao\\n   ID: 2512.03420v1\\n   Categories: Primary: cs.CR, Additional: cs.SE\\n   Published: 2025-12-03T03:55:09Z\\n   Preview: Large language model (LLM)-based techniques have achieved notable progress in generating harnesses for program fuzzing.\\n\\n7. BookRAG: A Hierarchical Structure-aware Index-based Approach for Retrieval-Augmented Generation on Complex Documents\\n   Authors: Shu Wang, Yingli Zhou, Yixiang Fang\\n   ID: 2512.03413v1\\n   Categories: Primary: cs.IR, Additional: cs.AI\\n   Published: 2025-12-03T03:40:49Z\\n   Preview: As an effective method to boost the performance of Large Language Models (LLMs) on the question answering (QA) task, Retrieval-Augmented Generation (RAG), which queries highly relevant information fro...\\n\\n8. WorldMM: Dynamic Multimodal Memory Agent for Long Video Reasoning\\n   Authors: Woongyeong Yeo, Kangsan Kim, Jaehong Yoon, Sung Ju Hwang\\n   ID: 2512.02425v1\\n   Categories: Primary: cs.CV, Additional: cs.AI, cs.CL, cs.IR, cs.LG\\n   Published: 2025-12-02T05:14:52Z\\n   Preview: Recent advances in video large language models have demonstrated strong capabilities in understanding short clips.\\n\\n9. MMAG: Mixed Memory-Augmented Generation for Large Language Models Applications\\n   Authors: Stefano Zeppieri\\n   ID: 2512.01710v2\\n   Categories: Primary: cs.CL, Additional: cs.IR\\n   Published: 2025-12-01T14:16:57Z\\n   Preview: Large Language Models (LLMs) excel at generating coherent text within a single prompt but fall short in sustaining relevance, personalization, and continuity across extended interactions.\\n\\n10. Memory-Integrated Reconfigurable Adapters: A Unified Framework for Settings with Multiple Tasks\\n   Authors: Susmit Agrawal, Krishn Vishwas Kher, Saksham Mittal, Swarnim Maheshwari, Vineeth N. Balasubramanian\\n   ID: 2512.00940v1\\n   Categories: Primary: cs.LG\\n   Published: 2025-11-30T15:45:55Z\\n   Preview: Organisms constantly pivot between tasks such as evading predators, foraging, traversing rugged terrain, and socializing, often within milliseconds.\\n\\n11. ML-Tool-Bench: Tool-Augmented Planning for ML Tasks\\n   Authors: Yaswanth Chittepu, Raghavendra Addanki, Tung Mai, Anup Rao, Branislav Kveton\\n   ID: 2512.00672v1\\n   Categories: Primary: cs.LG, Additional: cs.AI\\n   Published: 2025-11-29T23:59:40Z\\n   Preview: The development of autonomous machine learning (ML) agents capable of end-to-end data science workflows represents a significant frontier in artificial intelligence.\\n\\n12. CogEvo-Edu: Cognitive Evolution Educational Multi-Agent Collaborative System\\n   Authors: Yefeng Wu, Yuchen Song, Yecheng Zhao, Ling Wu, Shan Wan\\n   ID: 2512.00331v1\\n   Categories: Primary: cs.AI, Additional: cs.MA\\n   Published: 2025-11-29T05:41:57Z\\n   Preview: Large language models (LLMs) are increasingly deployed as conversational tutors in STEM education, yet most systems still rely on a single LLM with a static retrieval-augmented generation (RAG) pipeli...\\n\\n13. Words into World: A Task-Adaptive Agent for Language-Guided Spatial Retrieval in AR\\n   Authors: Lixing Guo, Tobias H\\u00f6llerer\\n   ID: 2512.00294v1\\n   Categories: Primary: cs.CV, Additional: cs.AI, cs.HC\\n   Published: 2025-11-29T03:29:15Z\\n   Preview: Traditional augmented reality (AR) systems predominantly rely on fixed class detectors or fiducial markers, limiting their ability to interpret complex, open-vocabulary natural language queries.\\n\\n14. MegaChat: A Synthetic Persian Q&A Dataset for High-Quality Sales Chatbot Evaluation\\n   Authors: Mahdi Rahmani, AmirHossein Saffari, Reyhane Rahmani\\n   ID: 2511.23397v1\\n   Categories: Primary: cs.CL, Additional: cs.AI, cs.MA\\n   Published: 2025-11-28T17:44:20Z\\n   Preview: Small and medium-sized enterprises (SMEs) in Iran increasingly leverage Telegram for sales, where real-time engagement is essential for conversion.\\n\\n15. MCP vs RAG vs NLWeb vs HTML: A Comparison of the Effectiveness and Efficiency of Different Agent Interfaces to the Web (Technical Report)\\n   Authors: Aaron Steiner, Ralph Peeters, Christian Bizer\\n   ID: 2511.23281v1\\n   Categories: Primary: cs.CL\\n   Published: 2025-11-28T15:32:15Z\\n   Preview: Large language model agents are increasingly used to automate web tasks such as product search, offer comparison, and checkout.\\n\\n16. RAG-Empowered LLM-Driven Dynamic Radio Resource Management in Open 6G RAN\\n   Authors: Onur Salan, Burak \\u00c7\\u0131ra\\u011f, Onur Sever, \\u0130brahim H\\u00f6kelek, Ali G\\u00f6r\\u00e7in, Hakan Ali \\u00c7\\u0131rpan\\n   ID: 2511.22933v1\\n   Categories: Primary: eess.SP\\n   Published: 2025-11-28T07:20:25Z\\n   Preview: Implications of the advancements in the area of artificial intelligence to the wireless communications is extremely significant, especially in terms of resource management.\\n\\n17. Resolving Evidence Sparsity: Agentic Context Engineering for Long-Document Understanding\\n   Authors: Keliang Liu, Zizhi Chen, Mingcheng Li, Jingqun Tang, Dingkang Yang, Lihua Zhang\\n   ID: 2511.22850v1\\n   Categories: Primary: cs.CV\\n   Published: 2025-11-28T03:09:40Z\\n   Preview: Document understanding is a long standing practical task.\\n\\n18. Real-Time Procedural Learning From Experience for AI Agents\\n   Authors: Dasheng Bi, Yubin Hu, Mohammed N. Nasir\\n   ID: 2511.22074v2\\n   Categories: Primary: cs.AI, Additional: cs.IR\\n   Published: 2025-11-27T03:51:49Z\\n   Preview: Learning how to do things from trial and error in real time is a hallmark of biological intelligence, yet most LLM-based agents lack mechanisms to acquire procedural knowledge after deployment.\\n\\n19. Chatty-KG: A Multi-Agent AI System for On-Demand Conversational Question Answering over Knowledge Graphs\\n   Authors: Reham Omar, Abdelghny Orogat, Ibrahim Abdelaziz, Omij Mangukiya, Panos Kalnis, Essam Mansour\\n   ID: 2511.20940v1\\n   Categories: Primary: cs.CL\\n   Published: 2025-11-26T00:18:55Z\\n   Preview: Conversational Question Answering over Knowledge Graphs (KGs) combines the factual grounding of KG-based QA with the interactive nature of dialogue systems.\\n\\n20. LLM-Driven Transient Stability Assessment: From Automated Simulation to Neural Architecture Design\\n   Authors: Lianzhe Hu, Yu Wang, Bikash Pal\\n   ID: 2511.20276v1\\n   Categories: Primary: eess.SY\\n   Published: 2025-11-25T13:05:02Z\\n   Preview: This paper presents an LLM-driven, end-to-end workflow that addresses the lack of automation and intelligence in power system transient stability assessment (TSA).\\n\\n21. HKRAG: Holistic Knowledge Retrieval-Augmented Generation Over Visually-Rich Documents\\n   Authors: Anyang Tong, Xiang Niu, ZhiPing Liu, Chang Tian, Yanyan Wei, Zenglin Shi, Meng Wang\\n   ID: 2511.20227v1\\n   Categories: Primary: cs.IR\\n   Published: 2025-11-25T11:59:52Z\\n   Preview: Existing multimodal Retrieval-Augmented Generation (RAG) methods for visually rich documents (VRD) are often biased towards retrieving salient knowledge(e.g., prominent text and visual elements), whil...\\n\\n22. M$^3$Prune: Hierarchical Communication Graph Pruning for Efficient Multi-Modal Multi-Agent Retrieval-Augmented Generation\\n   Authors: Weizi Shao, Taolin Zhang, Zijie Zhou, Chen Chen, Chengyu Wang, Xiaofeng He\\n   ID: 2511.19969v1\\n   Categories: Primary: cs.AI\\n   Published: 2025-11-25T06:29:13Z\\n   Preview: Recent advancements in multi-modal retrieval-augmented generation (mRAG), which enhance multi-modal large language models (MLLMs) with external knowledge, have demonstrated that the collective intelli...\\n\\n23. AppSelectBench: Application-Level Tool Selection Benchmark\\n   Authors: Tianyi Chen, Michael Solodko, Sen Wang, Jongwoo Ko, Junheng Hao, Colby Banbury, Sara Abdali, Saeed Amizadeh, Qing Xiao, Yinheng Li, Tianyu Ding, Kamran Ghasedi Dizaji, Suzhen Zheng, Hao Fan, Justin Wagle, Pashmina Cameron, Kazuhito Koishida\\n   ID: 2511.19957v2\\n   Categories: Primary: cs.CL\\n   Published: 2025-11-25T06:06:17Z\\n   Preview: Computer Using Agents (CUAs) are increasingly equipped with external tools, enabling them to perform complex and realistic tasks.\\n\\n24. Large Language Model Aided Birt-Hogg-Dube Syndrome Diagnosis with Multimodal Retrieval-Augmented Generation\\n   Authors: Haoqing Li, Jun Shi, Xianmeng Chen, Qiwei Jia, Rui Wang, Wei Wei, Hong An, Xiaowen Hu\\n   ID: 2511.19834v1\\n   Categories: Primary: cs.CV\\n   Published: 2025-11-25T01:55:23Z\\n   Preview: Deep learning methods face dual challenges of limited clinical samples and low inter-class differentiation among Diffuse Cystic Lung Diseases (DCLDs) in advancing Birt-Hogg-Dube syndrome (BHD) diagnos...\\n\\n25. NOEM$^{3}$A: A Neuro-Symbolic Ontology-Enhanced Method for Multi-Intent Understanding in Mobile Agents\\n   Authors: Ioannis Tzachristas, Aifen Sui\\n   ID: 2511.19780v1\\n   Categories: Primary: cs.AI\\n   Published: 2025-11-24T23:14:45Z\\n   Preview: We introduce a neuro-symbolic framework for multi-intent understanding in mobile AI agents by integrating a structured intent ontology with compact language models.\\n\\n\"]", "tool_call_id": "call_BiDHkzDWmSntNwWBNIlWKkQ5"}, {"role": "tool", "content": "[\"Search Results:\\n\\n1. ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning\\n   Authors: Shengyuan Ding, Xinyu Fang, Ziyu Liu, Yuhang Zang, Yuhang Cao, Xiangyu Zhao, Haodong Duan, Xiaoyi Dong, Jianze Liang, Bin Wang, Conghui He, Dahua Lin, Jiaqi Wang\\n   ID: 2512.05111v1\\n   Categories: Primary: cs.CV\\n   Published: 2025-12-04T18:59:52Z\\n   Preview: Reward models are critical for aligning vision-language systems with human preferences, yet current approaches suffer from hallucination, weak visual grounding, and an inability to use tools for verif...\\n\\n2. The Personalization Paradox: Semantic Loss vs. Reasoning Gains in Agentic AI Q&A\\n   Authors: Satyajit Movidi, Stephen Russell\\n   ID: 2512.04343v1\\n   Categories: Primary: cs.IR\\n   Published: 2025-12-04T00:12:41Z\\n   Preview: AIVisor, an agentic retrieval-augmented LLM for student advising, was used to examine how personalization affects system performance across multiple evaluation dimensions.\\n\\n3. Evaluating Long-Context Reasoning in LLM-Based WebAgents\\n   Authors: Andy Chung, Yichi Zhang, Kaixiang Lin, Aditya Rawal, Qiaozi Gao, Joyce Chai\\n   ID: 2512.04307v1\\n   Categories: Primary: cs.LG, Additional: cs.AI\\n   Published: 2025-12-03T22:53:10Z\\n   Preview: As large language model (LLM)-based agents become increasingly integrated into daily digital interactions, their ability to reason across long interaction histories becomes crucial for providing perso...\\n\\n4. SQuARE: Structured Query & Adaptive Retrieval Engine For Tabular Formats\\n   Authors: Chinmay Gondhalekar, Urjitkumar Patel, Fang-Chun Yeh\\n   ID: 2512.04292v1\\n   Categories: Primary: cs.CL\\n   Published: 2025-12-03T22:11:45Z\\n   Preview: Accurate question answering over real spreadsheets remains difficult due to multirow headers, merged cells, and unit annotations that disrupt naive chunking, while rigid SQL views fail on files lackin...\\n\\n5. A Hierarchical Tree-based approach for creating Configurable and Static Deep Research Agent (Static-DRA)\\n   Authors: Saurav Prateek\\n   ID: 2512.03887v2\\n   Categories: Primary: cs.AI\\n   Published: 2025-12-03T15:37:13Z\\n   Preview: The advancement in Large Language Models has driven the creation of complex agentic systems, such as Deep Research Agents (DRAs), to overcome the limitations of static Retrieval Augmented Generation (...\\n\\n6. OmniDexVLG: Learning Dexterous Grasp Generation from Vision Language Model-Guided Grasp Semantics, Taxonomy and Functional Affordance\\n   Authors: Lei Zhang, Diwen Zheng, Kaixin Bai, Zhenshan Bing, Zoltan-Csaba Marton, Zhaopeng Chen, Alois Christian Knoll, Jianwei Zhang\\n   ID: 2512.03874v1\\n   Categories: Primary: cs.RO, Additional: cs.LG\\n   Published: 2025-12-03T15:28:23Z\\n   Preview: Dexterous grasp generation aims to produce grasp poses that align with task requirements and human interpretable grasp semantics.\\n\\n7. MemVerse: Multimodal Memory for Lifelong Learning Agents\\n   Authors: Junming Liu, Yifei Sun, Weihua Cheng, Haodong Lei, Yirong Chen, Licheng Wen, Xuemeng Yang, Daocheng Fu, Pinlong Cai, Nianchen Deng, Yi Yu, Shuyue Hu, Botian Shi, Ding Wang\\n   ID: 2512.03627v1\\n   Categories: Primary: cs.AI\\n   Published: 2025-12-03T10:06:14Z\\n   Preview: Despite rapid progress in large-scale language and vision models, AI agents still suffer from a fundamental limitation: they cannot remember.\\n\\n8. Towards Object-centric Understanding for Instructional Videos\\n   Authors: Wenliang Guo, Yu Kong\\n   ID: 2512.03479v1\\n   Categories: Primary: cs.CV\\n   Published: 2025-12-03T06:14:26Z\\n   Preview: Understanding procedural activities is crucial for developing future assistive AI that can reason about complex real-world tasks.\\n\\n9. Think Before You Drive: World Model-Inspired Multimodal Grounding for Autonomous Vehicles\\n   Authors: Haicheng Liao, Huanming Shen, Bonan Wang, Yongkang Li, Yihong Tang, Chengyue Wang, Dingyi Zhuang, Kehua Chen, Hai Yang, Chengzhong Xu, Zhenning Li\\n   ID: 2512.03454v1\\n   Categories: Primary: cs.CV, Additional: cs.AI\\n   Published: 2025-12-03T05:14:16Z\\n   Preview: Interpreting natural-language commands to localize target objects is critical for autonomous driving (AD).\\n\\n10. Multi-Aspect Knowledge-Enhanced Medical Vision-Language Pretraining with Multi-Agent Data Generation\\n   Authors: Xieji Li, Siyuan Yan, Yingsheng Liu, H. Peter Soyer, Monika Janda, Victoria Mar, Zongyuan Ge\\n   ID: 2512.03445v1\\n   Categories: Primary: cs.CV, Additional: cs.AI\\n   Published: 2025-12-03T04:55:54Z\\n   Preview: Vision-language pretraining (VLP) has emerged as a powerful paradigm in medical image analysis, enabling representation learning from large-scale image-text pairs without relying on expensive manual a...\\n\\n11. HarnessAgent: Scaling Automatic Fuzzing Harness Construction with Tool-Augmented LLM Pipelines\\n   Authors: Kang Yang, Yunhang Zhang, Zichuan Li, GuanHong Tao, Jun Xu, XiaoJing Liao\\n   ID: 2512.03420v1\\n   Categories: Primary: cs.CR, Additional: cs.SE\\n   Published: 2025-12-03T03:55:09Z\\n   Preview: Large language model (LLM)-based techniques have achieved notable progress in generating harnesses for program fuzzing.\\n\\n12. BookRAG: A Hierarchical Structure-aware Index-based Approach for Retrieval-Augmented Generation on Complex Documents\\n   Authors: Shu Wang, Yingli Zhou, Yixiang Fang\\n   ID: 2512.03413v1\\n   Categories: Primary: cs.IR, Additional: cs.AI\\n   Published: 2025-12-03T03:40:49Z\\n   Preview: As an effective method to boost the performance of Large Language Models (LLMs) on the question answering (QA) task, Retrieval-Augmented Generation (RAG), which queries highly relevant information fro...\\n\\n13. Spoken Conversational Agents with Large Language Models\\n   Authors: Chao-Han Huck Yang, Andreas Stolcke, Larry Heck\\n   ID: 2512.02593v1\\n   Categories: Primary: cs.CL, Additional: cs.MA, cs.NE, cs.SD, eess.AS\\n   Published: 2025-12-02T10:02:10Z\\n   Preview: Spoken conversational agents are converging toward voice-native LLMs.\\n\\n14. EZYer: A simulacrum of high school with generative agent\\n   Authors: Jinming Yang, Zimu Ji, Weiqi Luo, Gaoxi Wang, Bin Ma, Yueling Deng\\n   ID: 2512.02561v1\\n   Categories: Primary: cs.MA, Additional: cs.AI\\n   Published: 2025-12-02T09:28:59Z\\n   Preview: With the rapid development of the online education and large language model, the existing educational tools still suffer from incomplete service, insufficient performance and weak interactivity in ter...\\n\\n15. In-Context Distillation with Self-Consistency Cascades: A Simple, Training-Free Way to Reduce LLM Agent Costs\\n   Authors: Vishnu Sarukkai, Asanshay Gupta, James Hong, Micha\\u00ebl Gharbi, Kayvon Fatahalian\\n   ID: 2512.02543v1\\n   Categories: Primary: cs.LG\\n   Published: 2025-12-02T09:11:05Z\\n   Preview: The world currently has an abundance of ideas for how to use new LLM agents, and developers seek to rapidly prototype and test new agentic designs.\\n\\n16. Aetheria: A multimodal interpretable content safety framework based on multi-agent debate and collaboration\\n   Authors: Yuxiang He, Jian Zhao, Yuchen Yuan, Tianle Zhang, Wei Cai, Haojie Cheng, Ziyan Shi, Ming Zhu, Haichuan Tang, Chi Zhang, Xuelong Li\\n   ID: 2512.02530v1\\n   Categories: Primary: cs.AI\\n   Published: 2025-12-02T08:49:54Z\\n   Preview: The exponential growth of digital content presents significant challenges for content safety.\\n\\n17. WorldMM: Dynamic Multimodal Memory Agent for Long Video Reasoning\\n   Authors: Woongyeong Yeo, Kangsan Kim, Jaehong Yoon, Sung Ju Hwang\\n   ID: 2512.02425v1\\n   Categories: Primary: cs.CV, Additional: cs.AI, cs.CL, cs.IR, cs.LG\\n   Published: 2025-12-02T05:14:52Z\\n   Preview: Recent advances in video large language models have demonstrated strong capabilities in understanding short clips.\\n\\n18. Skywork-R1V4: Toward Agentic Multimodal Intelligence through Interleaved Thinking with Images and DeepResearch\\n   Authors: Yifan Zhang, Liang Hu, Haofeng Sun, Peiyu Wang, Yichen Wei, Shukang Yin, Jiangbo Pei, Wei Shen, Peng Xia, Yi Peng, Tianyidan Xie, Eric Li, Yang Liu, Xuchen Song, Yahui Zhou\\n   ID: 2512.02395v1\\n   Categories: Primary: cs.CV\\n   Published: 2025-12-02T04:12:57Z\\n   Preview: Despite recent progress in multimodal agentic systems, existing approaches often treat image manipulation and web search as disjoint capabilities, rely heavily on costly reinforcement learning, and la...\\n\\n19. Benchmarking LLM Agents for Wealth-Management Workflows\\n   Authors: Rory Milsom\\n   ID: 2512.02230v1\\n   Categories: Primary: cs.AI\\n   Published: 2025-12-01T21:56:21Z\\n   Preview: Modern work relies on an assortment of digital collaboration tools, yet routine processes continue to suffer from human error and delay.\\n\\n20. How Far Are We from Genuinely Useful Deep Research Agents?\\n   Authors: Dingling Zhang, He Zhu, Jincheng Ren, Kangqi Song, Xinran Zhou, Boyu Feng, Shudong Liu, Jiabin Luo, Weihao Xie, Zhaohui Wang, Tianrui Qin, King Zhu, Yuqing Wang, Qianben Chen, Yuchen Eleanor Jiang, Wei Wang, Jiaheng Liu, Wangchunshu Zhou\\n   ID: 2512.01948v1\\n   Categories: Primary: cs.CL\\n   Published: 2025-12-01T17:58:59Z\\n   Preview: Deep Research Agents (DRAs) aim to automatically produce analyst-level reports through iterative information retrieval and synthesis.\\n\\n21. Agentic Policy Optimization via Instruction-Policy Co-Evolution\\n   Authors: Han Zhou, Xingchen Wan, Ivan Vuli\\u0107, Anna Korhonen\\n   ID: 2512.01945v1\\n   Categories: Primary: cs.LG, Additional: cs.AI, cs.CL\\n   Published: 2025-12-01T17:56:29Z\\n   Preview: Reinforcement Learning with Verifiable Rewards (RLVR) has advanced the reasoning capability of large language models (LLMs), enabling autonomous agents that can conduct effective multi-turn and tool-i...\\n\\n22. MMAG: Mixed Memory-Augmented Generation for Large Language Models Applications\\n   Authors: Stefano Zeppieri\\n   ID: 2512.01710v2\\n   Categories: Primary: cs.CL, Additional: cs.IR\\n   Published: 2025-12-01T14:16:57Z\\n   Preview: Large Language Models (LLMs) excel at generating coherent text within a single prompt but fall short in sustaining relevance, personalization, and continuity across extended interactions.\\n\\n23. Dr.Mi-Bench: A Modular-integrated Benchmark for Scientific Deep Research Agent\\n   Authors: Zhihan Guo, Feiyang Xu, Yifan Li, Muzhi Li, Shuai Zou, Jiele Wu, Han Shi, Haoli Bai, Ho-fung Leung, Irwin King\\n   ID: 2512.00986v1\\n   Categories: Primary: cs.CL\\n   Published: 2025-11-30T17:16:47Z\\n   Preview: The explosive growth in academic literature necessitates automated deep research (DR) agents, yet their evaluation remains a significant challenge.\\n\\n24. Memory-Integrated Reconfigurable Adapters: A Unified Framework for Settings with Multiple Tasks\\n   Authors: Susmit Agrawal, Krishn Vishwas Kher, Saksham Mittal, Swarnim Maheshwari, Vineeth N. Balasubramanian\\n   ID: 2512.00940v1\\n   Categories: Primary: cs.LG\\n   Published: 2025-11-30T15:45:55Z\\n   Preview: Organisms constantly pivot between tasks such as evading predators, foraging, traversing rugged terrain, and socializing, often within milliseconds.\\n\\n25. ML-Tool-Bench: Tool-Augmented Planning for ML Tasks\\n   Authors: Yaswanth Chittepu, Raghavendra Addanki, Tung Mai, Anup Rao, Branislav Kveton\\n   ID: 2512.00672v1\\n   Categories: Primary: cs.LG, Additional: cs.AI\\n   Published: 2025-11-29T23:59:40Z\\n   Preview: The development of autonomous machine learning (ML) agents capable of end-to-end data science workflows represents a significant frontier in artificial intelligence.\\n\\n\"]", "tool_call_id": "call_wxFZHag4GpUWkgJCVqzkRRbO"}], "tool_calls": [{"id": "call_Zzixlq2O2pJMx8RRYSbtj7u4", "name": "get_paper_data", "arguments": "{\"paper_id\": \"2512.01948v1\"}"}, {"id": "call_RKnT5QfH3UzBlWl7LU8URriM", "name": "get_paper_data", "arguments": "{\"paper_id\": \"2512.03887v2\"}"}, {"id": "call_ywekqaFuZapHMGrU0LKPidzx", "name": "get_paper_data", "arguments": "{\"paper_id\": \"2511.21729v1\"}"}, {"id": "call_FTkWipLI05JXDaTgYS0xtgbw", "name": "get_paper_data", "arguments": "{\"paper_id\": \"2511.19969v1\"}"}, {"id": "call_m5Y8yOLnylwIyOsWTFAZgkoA", "name": "get_paper_data", "arguments": "{\"paper_id\": \"2512.04343v1\"}"}, {"id": "call_ybHuNDTnP3LzgUeZNHiX6l8d", "name": "get_paper_data", "arguments": "{\"paper_id\": \"2511.18177v1\"}"}, {"id": "call_jSyZSv8qD7K1nh0OfnmB3JiS", "name": "get_paper_data", "arguments": "{\"paper_id\": \"2511.22850v1\"}"}, {"id": "call_6Rpnt58eeZnN2OAFlAG3L2Ai", "name": "get_paper_data", "arguments": "{\"paper_id\": \"2511.17467v2\"}"}, {"id": "call_bcLbKIRxHOLwSkC7yEVM7BoG", "name": "get_paper_data", "arguments": "{\"paper_id\": \"2511.20227v1\"}"}, {"id": "call_pdVijcKxLM4X9KXaYlGq50Pw", "name": "get_paper_data", "arguments": "{\"paper_id\": \"2512.00986v1\"}"}], "response_content": ""}
{"timestamp": "2025-12-06T08:28:27.082401Z", "type": "tool_call", "tool": "get_paper_data", "args": {"paper_id": "2512.01948v1"}, "result": ["Title: How Far Are We from Genuinely Useful Deep Research Agents?\n\nMetadata:\n- Authors: Dingling Zhang, He Zhu, Jincheng Ren, Kangqi Song, Xinran Zhou, Boyu Feng, Shudong Liu, Jiabin Luo, Weihao Xie, Zhaohui Wang, Tianrui Qin, King Zhu, Yuqing Wang, Qianben Chen, Yuchen Eleanor Jiang, Wei Wang, Jiaheng Liu, Wangchunshu Zhou\n- Published: 2025-12-01T17:58:59Z\n- Last Updated: 2025-12-01T17:58:59Z\n- Categories: Primary: cs.CL\n\nAbstract:\nDeep Research Agents (DRAs) aim to automatically produce analyst-level reports through iterative information retrieval and synthesis. However, most existing DRAs were validated on question-answering benchmarks, while research on generating comprehensive reports remains overlooked. Worse, current benchmarks for report synthesis suffer from task complexity and subjective metrics -- this fails to reflect user demands and limits the practical utility of generated reports. To address these gaps, we present Fine-grained DEepResearch bench (FINDER), an enhanced benchmark consisting of 100 human-curated research tasks with 419 structured checklist items that standardize report structure, analytical depth, and factual grounding. Based on approximately 1,000 reports produced by mainstream DRAs, we further propose Deep rEsearch Failure Taxonomy (DEFT), the first failure taxonomy for deep research agents. DEFT contains 14 fine-grained failure modes across reasoning, retrieval, and generation, and is built upon grounded theory with human-LLM co-annotating and inter-annotator reliability validation. Our experimental findings reveal that current DRAs struggle not with task comprehension but with evidence integration, verification, and reasoning-resilient planning.\n\nAccess Options:\n- Abstract page: https://arxiv.org/abs/2512.01948v1\n- Full text HTML version: https://arxiv.org/html/2512.01948v1\n- PDF version: https://arxiv.org/pdf/2512.01948v1\n\nAdditional Information:\n- Comment: 34 pages\n"]}
{"timestamp": "2025-12-06T08:28:29.831477Z", "type": "tool_call", "tool": "get_paper_data", "args": {"paper_id": "2512.03887v2"}, "result": ["Title: A Hierarchical Tree-based approach for creating Configurable and Static Deep Research Agent (Static-DRA)\n\nMetadata:\n- Authors: Saurav Prateek\n- Published: 2025-12-03T15:37:13Z\n- Last Updated: 2025-12-04T05:41:20Z\n- Categories: Primary: cs.AI\n\nAbstract:\nThe advancement in Large Language Models has driven the creation of complex agentic systems, such as Deep Research Agents (DRAs), to overcome the limitations of static Retrieval Augmented Generation (RAG) pipelines in handling complex, multi-turn research tasks. This paper introduces the Static Deep Research Agent (Static-DRA), a novel solution built upon a configurable and hierarchical Tree-based static workflow. The core contribution is the integration of two user-tunable parameters, Depth and Breadth, which provide granular control over the research intensity. This design allows end-users to consciously balance the desired quality and comprehensiveness of the research report against the associated computational cost of Large Language Model (LLM) interactions. The agent's architecture, comprising Supervisor, Independent, and Worker agents, facilitates effective multi-hop information retrieval and parallel sub-topic investigation. We evaluate the Static-DRA against the established DeepResearch Bench using the RACE (Reference-based Adaptive Criteria-driven Evaluation) framework. Configured with a depth of 2 and a breadth of 5, and powered by the gemini-2.5-pro model, the agent achieved an overall score of 34.72. Our experiments validate that increasing the configured Depth and Breadth parameters results in a more in-depth research process and a correspondingly higher evaluation score. The Static-DRA offers a pragmatic and resource-aware solution, empowering users with transparent control over the deep research process. The entire source code, outputs and benchmark results are open-sourced at https://github.com/SauravP97/Static-Deep-Research/\n\nAccess Options:\n- Abstract page: https://arxiv.org/abs/2512.03887v2\n- Full text HTML version: https://arxiv.org/html/2512.03887v2\n- PDF version: https://arxiv.org/pdf/2512.03887v2\n\nAdditional Information:\n- Comment: 16 pages, 6 figures, 4 tables. Code available at: https://github.com/SauravP97/Static-Deep-Research\n"]}
{"timestamp": "2025-12-06T08:28:32.898541Z", "type": "tool_call", "tool": "get_paper_data", "args": {"paper_id": "2511.21729v1"}, "result": ["Title: Beyond Component Strength: Synergistic Integration and Adaptive Calibration in Multi-Agent RAG Systems\n\nMetadata:\n- Authors: Jithin Krishnan\n- Published: 2025-11-21T07:53:56Z\n- Last Updated: 2025-11-21T07:53:56Z\n- Categories: Primary: cs.CL, Additional: cs.AI\n\nAbstract:\nBuilding reliable retrieval-augmented generation (RAG) systems requires more than adding powerful components; it requires understanding how they interact. Using ablation studies on 50 queries (15 answerable, 10 edge cases, and 25 adversarial), we show that enhancements such as hybrid retrieval, ensemble verification, and adaptive thresholding provide almost no benefit when used in isolation, yet together achieve a 95% reduction in abstention (from 40% to 2%) without increasing hallucinations. We also identify a measurement challenge: different verification strategies can behave safely but assign inconsistent labels (for example, \"abstained\" versus \"unsupported\"), creating apparent hallucination rates that are actually artifacts of labeling. Our results show that synergistic integration matters more than the strength of any single component, that standardized metrics and labels are essential for correctly interpreting performance, and that adaptive calibration is needed to prevent overconfident over-answering even when retrieval quality is high.\n\nAccess Options:\n- Abstract page: https://arxiv.org/abs/2511.21729v1\n- Full text HTML version: https://arxiv.org/html/2511.21729v1\n- PDF version: https://arxiv.org/pdf/2511.21729v1\n\nAdditional Information:\n- Comment: 10 pages, 4 figures\n"]}
{"timestamp": "2025-12-06T08:28:35.866361Z", "type": "tool_call", "tool": "get_paper_data", "args": {"paper_id": "2511.19969v1"}, "result": ["Title: M$^3$Prune: Hierarchical Communication Graph Pruning for Efficient Multi-Modal Multi-Agent Retrieval-Augmented Generation\n\nMetadata:\n- Authors: Weizi Shao, Taolin Zhang, Zijie Zhou, Chen Chen, Chengyu Wang, Xiaofeng He\n- Published: 2025-11-25T06:29:13Z\n- Last Updated: 2025-11-25T06:29:13Z\n- Categories: Primary: cs.AI\n\nAbstract:\nRecent advancements in multi-modal retrieval-augmented generation (mRAG), which enhance multi-modal large language models (MLLMs) with external knowledge, have demonstrated that the collective intelligence of multiple agents can significantly outperform a single model through effective communication. Despite impressive performance, existing multi-agent systems inherently incur substantial token overhead and increased computational costs, posing challenges for large-scale deployment. To address these issues, we propose a novel Multi-Modal Multi-agent hierarchical communication graph PRUNING framework, termed M$^3$Prune. Our framework eliminates redundant edges across different modalities, achieving an optimal balance between task performance and token overhead. Specifically, M$^3$Prune first applies intra-modal graph sparsification to textual and visual modalities, identifying the edges most critical for solving the task. Subsequently, we construct a dynamic communication topology using these key edges for inter-modal graph sparsification. Finally, we progressively prune redundant edges to obtain a more efficient and hierarchical topology. Extensive experiments on both general and domain-specific mRAG benchmarks demonstrate that our method consistently outperforms both single-agent and robust multi-agent mRAG systems while significantly reducing token consumption.\n\nAccess Options:\n- Abstract page: https://arxiv.org/abs/2511.19969v1\n- Full text HTML version: https://arxiv.org/html/2511.19969v1\n- PDF version: https://arxiv.org/pdf/2511.19969v1\n"]}
{"timestamp": "2025-12-06T08:28:38.940068Z", "type": "tool_call", "tool": "get_paper_data", "args": {"paper_id": "2512.04343v1"}, "result": ["Title: The Personalization Paradox: Semantic Loss vs. Reasoning Gains in Agentic AI Q&A\n\nMetadata:\n- Authors: Satyajit Movidi, Stephen Russell\n- Published: 2025-12-04T00:12:41Z\n- Last Updated: 2025-12-04T00:12:41Z\n- Categories: Primary: cs.IR\n\nAbstract:\nAIVisor, an agentic retrieval-augmented LLM for student advising, was used to examine how personalization affects system performance across multiple evaluation dimensions. Using twelve authentic advising questions intentionally designed to stress lexical precision, we compared ten personalized and non-personalized system configurations and analyzed outcomes with a Linear Mixed-Effects Model across lexical (BLEU, ROUGE-L), semantic (METEOR, BERTScore), and grounding (RAGAS) metrics. Results showed a consistent trade-off: personalization reliably improved reasoning quality and grounding, yet introduced a significant negative interaction on semantic similarity, driven not by poorer answers but by the limits of current metrics, which penalize meaningful personalized deviations from generic reference texts. This reveals a structural flaw in prevailing LLM evaluation methods, which are ill-suited for assessing user-specific responses. The fully integrated personalized configuration produced the highest overall gains, suggesting that personalization can enhance system effectiveness when evaluated with appropriate multidimensional metrics. Overall, the study demonstrates that personalization produces metric-dependent shifts rather than uniform improvements and provides a methodological foundation for more transparent and robust personalization in agentic AI.\n\nAccess Options:\n- Abstract page: https://arxiv.org/abs/2512.04343v1\n- Full text HTML version: https://arxiv.org/html/2512.04343v1\n- PDF version: https://arxiv.org/pdf/2512.04343v1\n"]}
{"timestamp": "2025-12-06T08:28:41.877547Z", "type": "tool_call", "tool": "get_paper_data", "args": {"paper_id": "2511.18177v1"}, "result": ["Title: Rethinking Retrieval: From Traditional Retrieval Augmented Generation to Agentic and Non-Vector Reasoning Systems in the Financial Domain for Large Language Models\n\nMetadata:\n- Authors: Elias Lumer, Matt Melich, Olivia Zino, Elena Kim, Sara Dieter, Pradeep Honaganahalli Basavaraju, Vamse Kumar Subbiah, James A. Burke, Roberto Hernandez\n- Published: 2025-11-22T20:06:25Z\n- Last Updated: 2025-11-22T20:06:25Z\n- Categories: Primary: cs.CL\n\nAbstract:\nRecent advancements in Retrieval-Augmented Generation (RAG) have enabled Large Language Models to answer financial questions using external knowledge bases of U.S. SEC filings, earnings reports, and regulatory documents. However, existing work lacks systematic comparison of vector-based and non-vector RAG architectures for financial documents, and the empirical impact of advanced RAG techniques on retrieval accuracy, answer quality, latency, and cost remain unclear. We present the first systematic evaluation comparing vector-based agentic RAG using hybrid search and metadata filtering against hierarchical node-based systems that traverse document structure without embeddings. We evaluate two enhancement techniques applied to the vector-based architecture, i) cross-encoder reranking for retrieval precision, and ii) small-to-big chunk retrieval for context completeness. Across 1,200 SEC 10-K, 10-Q, and 8-K filings on a 150-question benchmark, we measure retrieval metrics (MRR, Recall@5), answer quality through LLM-as-a-judge pairwise comparisons, latency, and preprocessing costs. Vector-based agentic RAG achieves a 68% win rate over hierarchical node-based systems with comparable latency (5.2 compared to 5.98 seconds). Cross-encoder reranking achieves a 59% absolute improvement at optimal parameters (10, 5) for MRR@5. Small-to-big retrieval achieves a 65% win rate over baseline chunking with only 0.2 seconds additional latency. Our findings reveal that applying advanced RAG techniques to financial Q&A systems improves retrieval accuracy, answer quality, and has cost-performance tradeoffs to be considered in production.\n\nAccess Options:\n- Abstract page: https://arxiv.org/abs/2511.18177v1\n- Full text HTML version: https://arxiv.org/html/2511.18177v1\n- PDF version: https://arxiv.org/pdf/2511.18177v1\n\nAdditional Information:\n- Comment: 8 pages, 2 figures\n"]}
{"timestamp": "2025-12-06T08:28:44.878298Z", "type": "tool_call", "tool": "get_paper_data", "args": {"paper_id": "2511.22850v1"}, "result": ["Title: Resolving Evidence Sparsity: Agentic Context Engineering for Long-Document Understanding\n\nMetadata:\n- Authors: Keliang Liu, Zizhi Chen, Mingcheng Li, Jingqun Tang, Dingkang Yang, Lihua Zhang\n- Published: 2025-11-28T03:09:40Z\n- Last Updated: 2025-11-28T03:09:40Z\n- Categories: Primary: cs.CV\n\nAbstract:\nDocument understanding is a long standing practical task. Vision Language Models (VLMs) have gradually become a primary approach in this domain, demonstrating effective performance on single page tasks. However, their effectiveness diminishes when handling long documents. In such scenarios, clues are often scattered across multiple pages and modalities, and redundancy from lengthy inputs can impair the models judgment. While retrieval augmented generation mitigates this issue by filtering for question relevant content, the retrieved results still contain substantial redundancy. To address these limitations, we propose SLEUTH, a multi agent framework. Concretely, SLEUTH orchestrates a retriever and four collaborative agents in a coarse to fine process. The framework identifies key textual and visual clues within the retrieved pages, filters for salient visual evidence such as tables and charts, and analyzes the query to devise a reasoning strategy. It ultimately synthesizes a distilled, evidence dense multimodal context to generate the final prediction. SLEUTH is model agnostic and scalable. When paired with advanced VLM backbones, it consistently improves performance on multiple long document benchmarks, achieving state of the art results. Ablation studies verify each modules effectiveness and confirm the benefits of our hierarchical refinement paradigm.\n\nAccess Options:\n- Abstract page: https://arxiv.org/abs/2511.22850v1\n- Full text HTML version: https://arxiv.org/html/2511.22850v1\n- PDF version: https://arxiv.org/pdf/2511.22850v1\n"]}
{"timestamp": "2025-12-06T08:28:47.847438Z", "type": "tool_call", "tool": "get_paper_data", "args": {"paper_id": "2511.17467v2"}, "result": ["Title: PersonaAgent with GraphRAG: Community-Aware Knowledge Graphs for Personalized LLM\n\nMetadata:\n- Authors: Siqi Liang, Yudi Zhang, Yue Guo\n- Published: 2025-11-21T18:15:47Z\n- Last Updated: 2025-12-02T04:45:19Z\n- Categories: Primary: cs.LG, Additional: cs.AI\n\nAbstract:\nWe propose a novel framework for persona-based language model system, motivated by the need for personalized AI agents that adapt to individual user preferences. In our approach, the agent embodies the user's \"persona\" (e.g. user profile or taste) and is powered by a large language model (LLM). To enable the agent to leverage rich contextual information, we introduce a Knowledge-Graph-enhanced Retrieval-Augmented Generation (Graph RAG) mechanism that constructs an LLM-derived graph index of relevant documents and summarizes communities of related information. Our framework generates personalized prompts by combining: (1) a summary of the user's historical behaviors and preferences extracted from the knowledge graph, and (2) relevant global interaction patterns identified through graph-based community detection. This dynamic prompt engineering approach allows the agent to maintain consistent persona-aligned behaviors while benefiting from collective knowledge. On the LaMP benchmark, our method improves news categorization F1 by 11.1%, movie tagging F1 by 56.1%, and reduces product rating MAE by 10.4% over prior methods. Our code is available at https://anonymous.4open.science/r/PersonaAgentwGraphRAG-DE6F\n\nAccess Options:\n- Abstract page: https://arxiv.org/abs/2511.17467v2\n- Full text HTML version: https://arxiv.org/html/2511.17467v2\n- PDF version: https://arxiv.org/pdf/2511.17467v2\n"]}
{"timestamp": "2025-12-06T08:28:50.835095Z", "type": "tool_call", "tool": "get_paper_data", "args": {"paper_id": "2511.20227v1"}, "result": ["Title: HKRAG: Holistic Knowledge Retrieval-Augmented Generation Over Visually-Rich Documents\n\nMetadata:\n- Authors: Anyang Tong, Xiang Niu, ZhiPing Liu, Chang Tian, Yanyan Wei, Zenglin Shi, Meng Wang\n- Published: 2025-11-25T11:59:52Z\n- Last Updated: 2025-11-25T11:59:52Z\n- Categories: Primary: cs.IR\n\nAbstract:\nExisting multimodal Retrieval-Augmented Generation (RAG) methods for visually rich documents (VRD) are often biased towards retrieving salient knowledge(e.g., prominent text and visual elements), while largely neglecting the critical fine-print knowledge(e.g., small text, contextual details). This limitation leads to incomplete retrieval and compromises the generator's ability to produce accurate and comprehensive answers. To bridge this gap, we propose HKRAG, a new holistic RAG framework designed to explicitly capture and integrate both knowledge types. Our framework features two key components: (1) a Hybrid Masking-based Holistic Retriever that employs explicit masking strategies to separately model salient and fine-print knowledge, ensuring a query-relevant holistic information retrieval; and (2) an Uncertainty-guided Agentic Generator that dynamically assesses the uncertainty of initial answers and actively decides how to integrate the two distinct knowledge streams for optimal response generation. Extensive experiments on open-domain visual question answering benchmarks show that HKRAG consistently outperforms existing methods in both zero-shot and supervised settings, demonstrating the critical importance of holistic knowledge retrieval for VRD understanding.\n\nAccess Options:\n- Abstract page: https://arxiv.org/abs/2511.20227v1\n- Full text HTML version: https://arxiv.org/html/2511.20227v1\n- PDF version: https://arxiv.org/pdf/2511.20227v1\n"]}
{"timestamp": "2025-12-06T08:28:53.842381Z", "type": "tool_call", "tool": "get_paper_data", "args": {"paper_id": "2512.00986v1"}, "result": ["Title: Dr.Mi-Bench: A Modular-integrated Benchmark for Scientific Deep Research Agent\n\nMetadata:\n- Authors: Zhihan Guo, Feiyang Xu, Yifan Li, Muzhi Li, Shuai Zou, Jiele Wu, Han Shi, Haoli Bai, Ho-fung Leung, Irwin King\n- Published: 2025-11-30T17:16:47Z\n- Last Updated: 2025-11-30T17:16:47Z\n- Categories: Primary: cs.CL\n\nAbstract:\nThe explosive growth in academic literature necessitates automated deep research (DR) agents, yet their evaluation remains a significant challenge. First, existing benchmarks often focus narrowly on retrieval while neglecting high-level planning and reasoning. Second, existing benchmarks favor general domains over the scientific domains that are the core application for DR agents. To address these gaps, we introduce Dr.Mi-Bench, a Modular-integrated benchmark for scientific DR agents. Grounded in academic literature, our benchmark uses a human-annotated dataset of 200 instances across 10 scientific domains, including both research and review papers. Besides, we also propose a Modular-integrated Evaluation Paradigm for DR Agents (Dr.Mi-Eval), a novel modular-integrated evaluation paradigm, which leverages the rich structure of academic papers to assess the core competencies of planning, retrieval, and reasoning through two complementary modes: an end-to-end evaluation for DR agents and an isolated evaluation for foundational LLMs as potential backbones. Experimental results reveal a fragmented performance landscape: agents exhibit specialized strengths but share critical weaknesses, most notably in performing the multi-source retrieval required for review-style tasks and performing consistently across diverse scientific fields. Moreover, improving high-level planning capability is the crucial factor for unlocking the reasoning potential of foundational LLMs as backbones. By exposing these actionable failure modes, Dr.Mi-Bench provides a diagnostic tool to guide the development of more reliable academic research assistants.\n\nAccess Options:\n- Abstract page: https://arxiv.org/abs/2512.00986v1\n- Full text HTML version: https://arxiv.org/html/2512.00986v1\n- PDF version: https://arxiv.org/pdf/2512.00986v1\n"]}
{"timestamp": "2025-12-06T08:29:47.633795Z", "type": "llm_call", "model": "gpt-5", "messages": [{"role": "system", "content": "You are an intelligent assistant equipped with specific tools.\n\n### WORKFLOW\n1. **Analyze Request**: Determine if the user's request requires performing actions (e.g., file I/O, web fetching) that your tools can handle.\n2. **Select Tool**: If an action is needed, select the appropriate tool from your available list.\n3. **Execute via Function Call**:\n   - You MUST use the native **function calling** mechanism to invoke the tool.\n   - Do NOT output the JSON arguments inside the conversation text (content).\n   - The system will only execute tools passed through the dedicated `tool_calls` field, NOT those written in the message body.\n\n### STRICT RULES\n- **NO JSON IN TEXT**: Never output raw JSON or code blocks containing tool arguments in your response text.\n- **Context Awareness**: Rely on the provided context. Do not invent file paths or data that do not exist.\n- **Fabrication Prohibited**: Do not make up file paths or information not present in the context.\n\nYou are a stateful assistant with access to prior conversation history. Use it to stay consistent, avoid repetition, and reference past context when helpful."}, {"role": "user", "content": "帮我查一下最近关于 RAG Agent 的 arXiv 论文，总结一下他们的核心观点。然后把总结写到 Notion 的页面里去。"}, {"role": "assistant", "content": "", "tool_calls": [{"id": "call_4aVFbtd5SHoo6jnwSWviCHj0", "type": "function", "function": {"name": "search_papers", "arguments": "{\"query\": \"ti:\\\"RAG agent\\\" OR abs:\\\"RAG agent\\\"\", \"max_results\": 25}"}}, {"id": "call_BiDHkzDWmSntNwWBNIlWKkQ5", "type": "function", "function": {"name": "search_papers", "arguments": "{\"query\": \"(ti:\\\"retrieval-augmented\\\" AND (ti:agent OR abs:agent)) OR abs:\\\"retrieval-augmented agent\\\"\", \"max_results\": 25}"}}, {"id": "call_wxFZHag4GpUWkgJCVqzkRRbO", "type": "function", "function": {"name": "search_papers", "arguments": "{\"query\": \"ti:\\\"agentic RAG\\\" OR abs:\\\"agentic RAG\\\" OR ti:\\\"agentic retrieval\\\" OR abs:\\\"agentic retrieval\\\"\", \"max_results\": 25}"}}]}, {"role": "tool", "content": "[\"Search Results:\\n\\n1. Evaluating Long-Context Reasoning in LLM-Based WebAgents\\n   Authors: Andy Chung, Yichi Zhang, Kaixiang Lin, Aditya Rawal, Qiaozi Gao, Joyce Chai\\n   ID: 2512.04307v1\\n   Categories: Primary: cs.LG, Additional: cs.AI\\n   Published: 2025-12-03T22:53:10Z\\n   Preview: As large language model (LLM)-based agents become increasingly integrated into daily digital interactions, their ability to reason across long interaction histories becomes crucial for providing perso...\\n\\n2. A Hierarchical Tree-based approach for creating Configurable and Static Deep Research Agent (Static-DRA)\\n   Authors: Saurav Prateek\\n   ID: 2512.03887v2\\n   Categories: Primary: cs.AI\\n   Published: 2025-12-03T15:37:13Z\\n   Preview: The advancement in Large Language Models has driven the creation of complex agentic systems, such as Deep Research Agents (DRAs), to overcome the limitations of static Retrieval Augmented Generation (...\\n\\n3. Think Before You Drive: World Model-Inspired Multimodal Grounding for Autonomous Vehicles\\n   Authors: Haicheng Liao, Huanming Shen, Bonan Wang, Yongkang Li, Yihong Tang, Chengyue Wang, Dingyi Zhuang, Kehua Chen, Hai Yang, Chengzhong Xu, Zhenning Li\\n   ID: 2512.03454v1\\n   Categories: Primary: cs.CV, Additional: cs.AI\\n   Published: 2025-12-03T05:14:16Z\\n   Preview: Interpreting natural-language commands to localize target objects is critical for autonomous driving (AD).\\n\\n4. BookRAG: A Hierarchical Structure-aware Index-based Approach for Retrieval-Augmented Generation on Complex Documents\\n   Authors: Shu Wang, Yingli Zhou, Yixiang Fang\\n   ID: 2512.03413v1\\n   Categories: Primary: cs.IR, Additional: cs.AI\\n   Published: 2025-12-03T03:40:49Z\\n   Preview: As an effective method to boost the performance of Large Language Models (LLMs) on the question answering (QA) task, Retrieval-Augmented Generation (RAG), which queries highly relevant information fro...\\n\\n5. Aetheria: A multimodal interpretable content safety framework based on multi-agent debate and collaboration\\n   Authors: Yuxiang He, Jian Zhao, Yuchen Yuan, Tianle Zhang, Wei Cai, Haojie Cheng, Ziyan Shi, Ming Zhu, Haichuan Tang, Chi Zhang, Xuelong Li\\n   ID: 2512.02530v1\\n   Categories: Primary: cs.AI\\n   Published: 2025-12-02T08:49:54Z\\n   Preview: The exponential growth of digital content presents significant challenges for content safety.\\n\\n6. CogEvo-Edu: Cognitive Evolution Educational Multi-Agent Collaborative System\\n   Authors: Yefeng Wu, Yuchen Song, Yecheng Zhao, Ling Wu, Shan Wan\\n   ID: 2512.00331v1\\n   Categories: Primary: cs.AI, Additional: cs.MA\\n   Published: 2025-11-29T05:41:57Z\\n   Preview: Large language models (LLMs) are increasingly deployed as conversational tutors in STEM education, yet most systems still rely on a single LLM with a static retrieval-augmented generation (RAG) pipeli...\\n\\n7. MegaChat: A Synthetic Persian Q&A Dataset for High-Quality Sales Chatbot Evaluation\\n   Authors: Mahdi Rahmani, AmirHossein Saffari, Reyhane Rahmani\\n   ID: 2511.23397v1\\n   Categories: Primary: cs.CL, Additional: cs.AI, cs.MA\\n   Published: 2025-11-28T17:44:20Z\\n   Preview: Small and medium-sized enterprises (SMEs) in Iran increasingly leverage Telegram for sales, where real-time engagement is essential for conversion.\\n\\n8. MCP vs RAG vs NLWeb vs HTML: A Comparison of the Effectiveness and Efficiency of Different Agent Interfaces to the Web (Technical Report)\\n   Authors: Aaron Steiner, Ralph Peeters, Christian Bizer\\n   ID: 2511.23281v1\\n   Categories: Primary: cs.CL\\n   Published: 2025-11-28T15:32:15Z\\n   Preview: Large language model agents are increasingly used to automate web tasks such as product search, offer comparison, and checkout.\\n\\n9. RAG-Empowered LLM-Driven Dynamic Radio Resource Management in Open 6G RAN\\n   Authors: Onur Salan, Burak \\u00c7\\u0131ra\\u011f, Onur Sever, \\u0130brahim H\\u00f6kelek, Ali G\\u00f6r\\u00e7in, Hakan Ali \\u00c7\\u0131rpan\\n   ID: 2511.22933v1\\n   Categories: Primary: eess.SP\\n   Published: 2025-11-28T07:20:25Z\\n   Preview: Implications of the advancements in the area of artificial intelligence to the wireless communications is extremely significant, especially in terms of resource management.\\n\\n10. Advancing Automated In-Isolation Validation in Repository-Level Code Translation\\n   Authors: Kaiyao Ke, Ali Reza Ibrahimzada, Rangeet Pan, Saurabh Sinha, Reyhaneh Jabbarvand\\n   ID: 2511.21878v1\\n   Categories: Primary: cs.SE, Additional: cs.PL\\n   Published: 2025-11-26T19:53:46Z\\n   Preview: Repository-level code translation aims to migrate entire repositories across programming languages while preserving functionality automatically.\\n\\n11. Chatty-KG: A Multi-Agent AI System for On-Demand Conversational Question Answering over Knowledge Graphs\\n   Authors: Reham Omar, Abdelghny Orogat, Ibrahim Abdelaziz, Omij Mangukiya, Panos Kalnis, Essam Mansour\\n   ID: 2511.20940v1\\n   Categories: Primary: cs.CL\\n   Published: 2025-11-26T00:18:55Z\\n   Preview: Conversational Question Answering over Knowledge Graphs (KGs) combines the factual grounding of KG-based QA with the interactive nature of dialogue systems.\\n\\n12. HKRAG: Holistic Knowledge Retrieval-Augmented Generation Over Visually-Rich Documents\\n   Authors: Anyang Tong, Xiang Niu, ZhiPing Liu, Chang Tian, Yanyan Wei, Zenglin Shi, Meng Wang\\n   ID: 2511.20227v1\\n   Categories: Primary: cs.IR\\n   Published: 2025-11-25T11:59:52Z\\n   Preview: Existing multimodal Retrieval-Augmented Generation (RAG) methods for visually rich documents (VRD) are often biased towards retrieving salient knowledge(e.g., prominent text and visual elements), whil...\\n\\n13. Large Language Model Aided Birt-Hogg-Dube Syndrome Diagnosis with Multimodal Retrieval-Augmented Generation\\n   Authors: Haoqing Li, Jun Shi, Xianmeng Chen, Qiwei Jia, Rui Wang, Wei Wei, Hong An, Xiaowen Hu\\n   ID: 2511.19834v1\\n   Categories: Primary: cs.CV\\n   Published: 2025-11-25T01:55:23Z\\n   Preview: Deep learning methods face dual challenges of limited clinical samples and low inter-class differentiation among Diffuse Cystic Lung Diseases (DCLDs) in advancing Birt-Hogg-Dube syndrome (BHD) diagnos...\\n\\n14. Beyond Protein Language Models: An Agentic LLM Framework for Mechanistic Enzyme Design\\n   Authors: Bruno Jacob, Khushbu Agarwal, Marcel Baer, Peter Rice, Simone Raugei\\n   ID: 2511.19423v1\\n   Categories: Primary: q-bio.QM, Additional: cs.AI\\n   Published: 2025-11-24T18:57:07Z\\n   Preview: We present Genie-CAT, a tool-augmented large-language-model (LLM) system designed to accelerate scientific hypothesis generation in protein design.\\n\\n15. Cross-Disciplinary Knowledge Retrieval and Synthesis: A Compound AI Architecture for Scientific Discovery\\n   Authors: Svitlana Volkova, Peter Bautista, Avinash Hiriyanna, Gabriel Ganberg, Isabel Erickson, Zachary Klinefelter, Nick Abele, Hsien-Te Kao, Grant Engberson\\n   ID: 2511.18298v1\\n   Categories: Primary: cs.AI\\n   Published: 2025-11-23T05:33:11Z\\n   Preview: The exponential growth of scientific knowledge has created significant barriers to cross-disciplinary knowledge discovery, synthesis and research collaboration.\\n\\n16. Rethinking Retrieval: From Traditional Retrieval Augmented Generation to Agentic and Non-Vector Reasoning Systems in the Financial Domain for Large Language Models\\n   Authors: Elias Lumer, Matt Melich, Olivia Zino, Elena Kim, Sara Dieter, Pradeep Honaganahalli Basavaraju, Vamse Kumar Subbiah, James A. Burke, Roberto Hernandez\\n   ID: 2511.18177v1\\n   Categories: Primary: cs.CL\\n   Published: 2025-11-22T20:06:25Z\\n   Preview: Recent advancements in Retrieval-Augmented Generation (RAG) have enabled Large Language Models to answer financial questions using external knowledge bases of U.S.\\n\\n17. AI-Augmented Bibliometric Framework: A Paradigm Shift with Agentic AI for Dynamic, Snippet-Based Research Analysis\\n   Authors: Adela Bara, Simona-Vasilica Oprea\\n   ID: 2511.21745v1\\n   Categories: Primary: cs.DL\\n   Published: 2025-11-22T08:41:38Z\\n   Preview: Our paper introduces a generative, multiagent AI framework designed to overcome the rigidity, limited flexibility and technical barriers of current bibliometric tools.\\n\\n18. PersonaAgent with GraphRAG: Community-Aware Knowledge Graphs for Personalized LLM\\n   Authors: Siqi Liang, Yudi Zhang, Yue Guo\\n   ID: 2511.17467v2\\n   Categories: Primary: cs.LG, Additional: cs.AI\\n   Published: 2025-11-21T18:15:47Z\\n   Preview: We propose a novel framework for persona-based language model system, motivated by the need for personalized AI agents that adapt to individual user preferences.\\n\\n19. REMSA: An LLM Agent for Foundation Model Selection in Remote Sensing\\n   Authors: Binger Chen, Tacettin Emre B\\u00f6k, Behnood Rasti, Volker Markl, Beg\\u00fcm Demir\\n   ID: 2511.17442v1\\n   Categories: Primary: cs.CV, Additional: cs.AI\\n   Published: 2025-11-21T17:41:26Z\\n   Preview: Foundation Models (FMs) are increasingly used in remote sensing (RS) for tasks such as environmental monitoring, disaster assessment, and land-use mapping.\\n\\n20. Beyond Component Strength: Synergistic Integration and Adaptive Calibration in Multi-Agent RAG Systems\\n   Authors: Jithin Krishnan\\n   ID: 2511.21729v1\\n   Categories: Primary: cs.CL, Additional: cs.AI\\n   Published: 2025-11-21T07:53:56Z\\n   Preview: Building reliable retrieval-augmented generation (RAG) systems requires more than adding powerful components; it requires understanding how they interact.\\n\\n21. LLM and Agent-Driven Data Analysis: A Systematic Approach for Enterprise Applications and System-level Deployment\\n   Authors: Xi Wang, Xianyao Ling, Kun Li, Gang Yin, Liang Zhang, Jiang Wu, Annie Wang, Weizhe Wang\\n   ID: 2511.17676v1\\n   Categories: Primary: cs.DB, Additional: cs.AI, cs.CL\\n   Published: 2025-11-21T07:16:31Z\\n   Preview: The rapid progress in Generative AI and Agent technologies is profoundly transforming enterprise data management and analytics.\\n\\n22. SurvAgent: Hierarchical CoT-Enhanced Case Banking and Dichotomy-Based Multi-Agent System for Multimodal Survival Prediction\\n   Authors: Guolin Huang, Wenting Chen, Jiaqi Yang, Xinheng Lyu, Xiaoling Luo, Sen Yang, Xiaohan Xing, Linlin Shen\\n   ID: 2511.16635v1\\n   Categories: Primary: cs.CV, Additional: cs.CL\\n   Published: 2025-11-20T18:41:44Z\\n   Preview: Survival analysis is critical for cancer prognosis and treatment planning, yet existing methods lack the transparency essential for clinical adoption.\\n\\n23. CorrectHDL: Agentic HDL Design with LLMs Leveraging High-Level Synthesis as Reference\\n   Authors: Kangwei Xu, Grace Li Zhang, Ulf Schlichtmann, Bing Li\\n   ID: 2511.16395v1\\n   Categories: Primary: cs.AI, Additional: cs.PL, cs.SE, eess.SY\\n   Published: 2025-11-20T14:13:38Z\\n   Preview: Large Language Models (LLMs) have demonstrated remarkable potential in hardware front-end design using hardware description languages (HDLs).\\n\\n24. Operon: Incremental Construction of Ragged Data via Named Dimensions\\n   Authors: Sungbin Moon, Jiho Park, Suyoung Hwang, Donghyun Koh, Seunghyun Moon, Minhyeong Lee\\n   ID: 2511.16080v1\\n   Categories: Primary: cs.PL, Additional: cs.AI, cs.LG\\n   Published: 2025-11-20T06:16:31Z\\n   Preview: Modern data processing workflows frequently encounter ragged data: collections with variable-length elements that arise naturally in domains like natural language processing, scientific measurements,...\\n\\n25. KRAL: Knowledge and Reasoning Augmented Learning for LLM-assisted Clinical Antimicrobial Therapy\\n   Authors: Zhe Li, Yehan Qiu, Yujie Chen, Xiang Zhou\\n   ID: 2511.15974v4\\n   Categories: Primary: cs.AI\\n   Published: 2025-11-20T02:04:46Z\\n   Preview: Clinical antimicrobial therapy requires the dynamic integration of pathogen profiles,host factors, pharmacological properties of antimicrobials,and the severity of infection.\\n\\n\"]", "tool_call_id": "call_4aVFbtd5SHoo6jnwSWviCHj0"}, {"role": "tool", "content": "[\"Search Results:\\n\\n1. The Personalization Paradox: Semantic Loss vs. Reasoning Gains in Agentic AI Q&A\\n   Authors: Satyajit Movidi, Stephen Russell\\n   ID: 2512.04343v1\\n   Categories: Primary: cs.IR\\n   Published: 2025-12-04T00:12:41Z\\n   Preview: AIVisor, an agentic retrieval-augmented LLM for student advising, was used to examine how personalization affects system performance across multiple evaluation dimensions.\\n\\n2. A Hierarchical Tree-based approach for creating Configurable and Static Deep Research Agent (Static-DRA)\\n   Authors: Saurav Prateek\\n   ID: 2512.03887v2\\n   Categories: Primary: cs.AI\\n   Published: 2025-12-03T15:37:13Z\\n   Preview: The advancement in Large Language Models has driven the creation of complex agentic systems, such as Deep Research Agents (DRAs), to overcome the limitations of static Retrieval Augmented Generation (...\\n\\n3. OmniDexVLG: Learning Dexterous Grasp Generation from Vision Language Model-Guided Grasp Semantics, Taxonomy and Functional Affordance\\n   Authors: Lei Zhang, Diwen Zheng, Kaixin Bai, Zhenshan Bing, Zoltan-Csaba Marton, Zhaopeng Chen, Alois Christian Knoll, Jianwei Zhang\\n   ID: 2512.03874v1\\n   Categories: Primary: cs.RO, Additional: cs.LG\\n   Published: 2025-12-03T15:28:23Z\\n   Preview: Dexterous grasp generation aims to produce grasp poses that align with task requirements and human interpretable grasp semantics.\\n\\n4. Think Before You Drive: World Model-Inspired Multimodal Grounding for Autonomous Vehicles\\n   Authors: Haicheng Liao, Huanming Shen, Bonan Wang, Yongkang Li, Yihong Tang, Chengyue Wang, Dingyi Zhuang, Kehua Chen, Hai Yang, Chengzhong Xu, Zhenning Li\\n   ID: 2512.03454v1\\n   Categories: Primary: cs.CV, Additional: cs.AI\\n   Published: 2025-12-03T05:14:16Z\\n   Preview: Interpreting natural-language commands to localize target objects is critical for autonomous driving (AD).\\n\\n5. Multi-Aspect Knowledge-Enhanced Medical Vision-Language Pretraining with Multi-Agent Data Generation\\n   Authors: Xieji Li, Siyuan Yan, Yingsheng Liu, H. Peter Soyer, Monika Janda, Victoria Mar, Zongyuan Ge\\n   ID: 2512.03445v1\\n   Categories: Primary: cs.CV, Additional: cs.AI\\n   Published: 2025-12-03T04:55:54Z\\n   Preview: Vision-language pretraining (VLP) has emerged as a powerful paradigm in medical image analysis, enabling representation learning from large-scale image-text pairs without relying on expensive manual a...\\n\\n6. HarnessAgent: Scaling Automatic Fuzzing Harness Construction with Tool-Augmented LLM Pipelines\\n   Authors: Kang Yang, Yunhang Zhang, Zichuan Li, GuanHong Tao, Jun Xu, XiaoJing Liao\\n   ID: 2512.03420v1\\n   Categories: Primary: cs.CR, Additional: cs.SE\\n   Published: 2025-12-03T03:55:09Z\\n   Preview: Large language model (LLM)-based techniques have achieved notable progress in generating harnesses for program fuzzing.\\n\\n7. BookRAG: A Hierarchical Structure-aware Index-based Approach for Retrieval-Augmented Generation on Complex Documents\\n   Authors: Shu Wang, Yingli Zhou, Yixiang Fang\\n   ID: 2512.03413v1\\n   Categories: Primary: cs.IR, Additional: cs.AI\\n   Published: 2025-12-03T03:40:49Z\\n   Preview: As an effective method to boost the performance of Large Language Models (LLMs) on the question answering (QA) task, Retrieval-Augmented Generation (RAG), which queries highly relevant information fro...\\n\\n8. WorldMM: Dynamic Multimodal Memory Agent for Long Video Reasoning\\n   Authors: Woongyeong Yeo, Kangsan Kim, Jaehong Yoon, Sung Ju Hwang\\n   ID: 2512.02425v1\\n   Categories: Primary: cs.CV, Additional: cs.AI, cs.CL, cs.IR, cs.LG\\n   Published: 2025-12-02T05:14:52Z\\n   Preview: Recent advances in video large language models have demonstrated strong capabilities in understanding short clips.\\n\\n9. MMAG: Mixed Memory-Augmented Generation for Large Language Models Applications\\n   Authors: Stefano Zeppieri\\n   ID: 2512.01710v2\\n   Categories: Primary: cs.CL, Additional: cs.IR\\n   Published: 2025-12-01T14:16:57Z\\n   Preview: Large Language Models (LLMs) excel at generating coherent text within a single prompt but fall short in sustaining relevance, personalization, and continuity across extended interactions.\\n\\n10. Memory-Integrated Reconfigurable Adapters: A Unified Framework for Settings with Multiple Tasks\\n   Authors: Susmit Agrawal, Krishn Vishwas Kher, Saksham Mittal, Swarnim Maheshwari, Vineeth N. Balasubramanian\\n   ID: 2512.00940v1\\n   Categories: Primary: cs.LG\\n   Published: 2025-11-30T15:45:55Z\\n   Preview: Organisms constantly pivot between tasks such as evading predators, foraging, traversing rugged terrain, and socializing, often within milliseconds.\\n\\n11. ML-Tool-Bench: Tool-Augmented Planning for ML Tasks\\n   Authors: Yaswanth Chittepu, Raghavendra Addanki, Tung Mai, Anup Rao, Branislav Kveton\\n   ID: 2512.00672v1\\n   Categories: Primary: cs.LG, Additional: cs.AI\\n   Published: 2025-11-29T23:59:40Z\\n   Preview: The development of autonomous machine learning (ML) agents capable of end-to-end data science workflows represents a significant frontier in artificial intelligence.\\n\\n12. CogEvo-Edu: Cognitive Evolution Educational Multi-Agent Collaborative System\\n   Authors: Yefeng Wu, Yuchen Song, Yecheng Zhao, Ling Wu, Shan Wan\\n   ID: 2512.00331v1\\n   Categories: Primary: cs.AI, Additional: cs.MA\\n   Published: 2025-11-29T05:41:57Z\\n   Preview: Large language models (LLMs) are increasingly deployed as conversational tutors in STEM education, yet most systems still rely on a single LLM with a static retrieval-augmented generation (RAG) pipeli...\\n\\n13. Words into World: A Task-Adaptive Agent for Language-Guided Spatial Retrieval in AR\\n   Authors: Lixing Guo, Tobias H\\u00f6llerer\\n   ID: 2512.00294v1\\n   Categories: Primary: cs.CV, Additional: cs.AI, cs.HC\\n   Published: 2025-11-29T03:29:15Z\\n   Preview: Traditional augmented reality (AR) systems predominantly rely on fixed class detectors or fiducial markers, limiting their ability to interpret complex, open-vocabulary natural language queries.\\n\\n14. MegaChat: A Synthetic Persian Q&A Dataset for High-Quality Sales Chatbot Evaluation\\n   Authors: Mahdi Rahmani, AmirHossein Saffari, Reyhane Rahmani\\n   ID: 2511.23397v1\\n   Categories: Primary: cs.CL, Additional: cs.AI, cs.MA\\n   Published: 2025-11-28T17:44:20Z\\n   Preview: Small and medium-sized enterprises (SMEs) in Iran increasingly leverage Telegram for sales, where real-time engagement is essential for conversion.\\n\\n15. MCP vs RAG vs NLWeb vs HTML: A Comparison of the Effectiveness and Efficiency of Different Agent Interfaces to the Web (Technical Report)\\n   Authors: Aaron Steiner, Ralph Peeters, Christian Bizer\\n   ID: 2511.23281v1\\n   Categories: Primary: cs.CL\\n   Published: 2025-11-28T15:32:15Z\\n   Preview: Large language model agents are increasingly used to automate web tasks such as product search, offer comparison, and checkout.\\n\\n16. RAG-Empowered LLM-Driven Dynamic Radio Resource Management in Open 6G RAN\\n   Authors: Onur Salan, Burak \\u00c7\\u0131ra\\u011f, Onur Sever, \\u0130brahim H\\u00f6kelek, Ali G\\u00f6r\\u00e7in, Hakan Ali \\u00c7\\u0131rpan\\n   ID: 2511.22933v1\\n   Categories: Primary: eess.SP\\n   Published: 2025-11-28T07:20:25Z\\n   Preview: Implications of the advancements in the area of artificial intelligence to the wireless communications is extremely significant, especially in terms of resource management.\\n\\n17. Resolving Evidence Sparsity: Agentic Context Engineering for Long-Document Understanding\\n   Authors: Keliang Liu, Zizhi Chen, Mingcheng Li, Jingqun Tang, Dingkang Yang, Lihua Zhang\\n   ID: 2511.22850v1\\n   Categories: Primary: cs.CV\\n   Published: 2025-11-28T03:09:40Z\\n   Preview: Document understanding is a long standing practical task.\\n\\n18. Real-Time Procedural Learning From Experience for AI Agents\\n   Authors: Dasheng Bi, Yubin Hu, Mohammed N. Nasir\\n   ID: 2511.22074v2\\n   Categories: Primary: cs.AI, Additional: cs.IR\\n   Published: 2025-11-27T03:51:49Z\\n   Preview: Learning how to do things from trial and error in real time is a hallmark of biological intelligence, yet most LLM-based agents lack mechanisms to acquire procedural knowledge after deployment.\\n\\n19. Chatty-KG: A Multi-Agent AI System for On-Demand Conversational Question Answering over Knowledge Graphs\\n   Authors: Reham Omar, Abdelghny Orogat, Ibrahim Abdelaziz, Omij Mangukiya, Panos Kalnis, Essam Mansour\\n   ID: 2511.20940v1\\n   Categories: Primary: cs.CL\\n   Published: 2025-11-26T00:18:55Z\\n   Preview: Conversational Question Answering over Knowledge Graphs (KGs) combines the factual grounding of KG-based QA with the interactive nature of dialogue systems.\\n\\n20. LLM-Driven Transient Stability Assessment: From Automated Simulation to Neural Architecture Design\\n   Authors: Lianzhe Hu, Yu Wang, Bikash Pal\\n   ID: 2511.20276v1\\n   Categories: Primary: eess.SY\\n   Published: 2025-11-25T13:05:02Z\\n   Preview: This paper presents an LLM-driven, end-to-end workflow that addresses the lack of automation and intelligence in power system transient stability assessment (TSA).\\n\\n21. HKRAG: Holistic Knowledge Retrieval-Augmented Generation Over Visually-Rich Documents\\n   Authors: Anyang Tong, Xiang Niu, ZhiPing Liu, Chang Tian, Yanyan Wei, Zenglin Shi, Meng Wang\\n   ID: 2511.20227v1\\n   Categories: Primary: cs.IR\\n   Published: 2025-11-25T11:59:52Z\\n   Preview: Existing multimodal Retrieval-Augmented Generation (RAG) methods for visually rich documents (VRD) are often biased towards retrieving salient knowledge(e.g., prominent text and visual elements), whil...\\n\\n22. M$^3$Prune: Hierarchical Communication Graph Pruning for Efficient Multi-Modal Multi-Agent Retrieval-Augmented Generation\\n   Authors: Weizi Shao, Taolin Zhang, Zijie Zhou, Chen Chen, Chengyu Wang, Xiaofeng He\\n   ID: 2511.19969v1\\n   Categories: Primary: cs.AI\\n   Published: 2025-11-25T06:29:13Z\\n   Preview: Recent advancements in multi-modal retrieval-augmented generation (mRAG), which enhance multi-modal large language models (MLLMs) with external knowledge, have demonstrated that the collective intelli...\\n\\n23. AppSelectBench: Application-Level Tool Selection Benchmark\\n   Authors: Tianyi Chen, Michael Solodko, Sen Wang, Jongwoo Ko, Junheng Hao, Colby Banbury, Sara Abdali, Saeed Amizadeh, Qing Xiao, Yinheng Li, Tianyu Ding, Kamran Ghasedi Dizaji, Suzhen Zheng, Hao Fan, Justin Wagle, Pashmina Cameron, Kazuhito Koishida\\n   ID: 2511.19957v2\\n   Categories: Primary: cs.CL\\n   Published: 2025-11-25T06:06:17Z\\n   Preview: Computer Using Agents (CUAs) are increasingly equipped with external tools, enabling them to perform complex and realistic tasks.\\n\\n24. Large Language Model Aided Birt-Hogg-Dube Syndrome Diagnosis with Multimodal Retrieval-Augmented Generation\\n   Authors: Haoqing Li, Jun Shi, Xianmeng Chen, Qiwei Jia, Rui Wang, Wei Wei, Hong An, Xiaowen Hu\\n   ID: 2511.19834v1\\n   Categories: Primary: cs.CV\\n   Published: 2025-11-25T01:55:23Z\\n   Preview: Deep learning methods face dual challenges of limited clinical samples and low inter-class differentiation among Diffuse Cystic Lung Diseases (DCLDs) in advancing Birt-Hogg-Dube syndrome (BHD) diagnos...\\n\\n25. NOEM$^{3}$A: A Neuro-Symbolic Ontology-Enhanced Method for Multi-Intent Understanding in Mobile Agents\\n   Authors: Ioannis Tzachristas, Aifen Sui\\n   ID: 2511.19780v1\\n   Categories: Primary: cs.AI\\n   Published: 2025-11-24T23:14:45Z\\n   Preview: We introduce a neuro-symbolic framework for multi-intent understanding in mobile AI agents by integrating a structured intent ontology with compact language models.\\n\\n\"]", "tool_call_id": "call_BiDHkzDWmSntNwWBNIlWKkQ5"}, {"role": "tool", "content": "[\"Search Results:\\n\\n1. ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning\\n   Authors: Shengyuan Ding, Xinyu Fang, Ziyu Liu, Yuhang Zang, Yuhang Cao, Xiangyu Zhao, Haodong Duan, Xiaoyi Dong, Jianze Liang, Bin Wang, Conghui He, Dahua Lin, Jiaqi Wang\\n   ID: 2512.05111v1\\n   Categories: Primary: cs.CV\\n   Published: 2025-12-04T18:59:52Z\\n   Preview: Reward models are critical for aligning vision-language systems with human preferences, yet current approaches suffer from hallucination, weak visual grounding, and an inability to use tools for verif...\\n\\n2. The Personalization Paradox: Semantic Loss vs. Reasoning Gains in Agentic AI Q&A\\n   Authors: Satyajit Movidi, Stephen Russell\\n   ID: 2512.04343v1\\n   Categories: Primary: cs.IR\\n   Published: 2025-12-04T00:12:41Z\\n   Preview: AIVisor, an agentic retrieval-augmented LLM for student advising, was used to examine how personalization affects system performance across multiple evaluation dimensions.\\n\\n3. Evaluating Long-Context Reasoning in LLM-Based WebAgents\\n   Authors: Andy Chung, Yichi Zhang, Kaixiang Lin, Aditya Rawal, Qiaozi Gao, Joyce Chai\\n   ID: 2512.04307v1\\n   Categories: Primary: cs.LG, Additional: cs.AI\\n   Published: 2025-12-03T22:53:10Z\\n   Preview: As large language model (LLM)-based agents become increasingly integrated into daily digital interactions, their ability to reason across long interaction histories becomes crucial for providing perso...\\n\\n4. SQuARE: Structured Query & Adaptive Retrieval Engine For Tabular Formats\\n   Authors: Chinmay Gondhalekar, Urjitkumar Patel, Fang-Chun Yeh\\n   ID: 2512.04292v1\\n   Categories: Primary: cs.CL\\n   Published: 2025-12-03T22:11:45Z\\n   Preview: Accurate question answering over real spreadsheets remains difficult due to multirow headers, merged cells, and unit annotations that disrupt naive chunking, while rigid SQL views fail on files lackin...\\n\\n5. A Hierarchical Tree-based approach for creating Configurable and Static Deep Research Agent (Static-DRA)\\n   Authors: Saurav Prateek\\n   ID: 2512.03887v2\\n   Categories: Primary: cs.AI\\n   Published: 2025-12-03T15:37:13Z\\n   Preview: The advancement in Large Language Models has driven the creation of complex agentic systems, such as Deep Research Agents (DRAs), to overcome the limitations of static Retrieval Augmented Generation (...\\n\\n6. OmniDexVLG: Learning Dexterous Grasp Generation from Vision Language Model-Guided Grasp Semantics, Taxonomy and Functional Affordance\\n   Authors: Lei Zhang, Diwen Zheng, Kaixin Bai, Zhenshan Bing, Zoltan-Csaba Marton, Zhaopeng Chen, Alois Christian Knoll, Jianwei Zhang\\n   ID: 2512.03874v1\\n   Categories: Primary: cs.RO, Additional: cs.LG\\n   Published: 2025-12-03T15:28:23Z\\n   Preview: Dexterous grasp generation aims to produce grasp poses that align with task requirements and human interpretable grasp semantics.\\n\\n7. MemVerse: Multimodal Memory for Lifelong Learning Agents\\n   Authors: Junming Liu, Yifei Sun, Weihua Cheng, Haodong Lei, Yirong Chen, Licheng Wen, Xuemeng Yang, Daocheng Fu, Pinlong Cai, Nianchen Deng, Yi Yu, Shuyue Hu, Botian Shi, Ding Wang\\n   ID: 2512.03627v1\\n   Categories: Primary: cs.AI\\n   Published: 2025-12-03T10:06:14Z\\n   Preview: Despite rapid progress in large-scale language and vision models, AI agents still suffer from a fundamental limitation: they cannot remember.\\n\\n8. Towards Object-centric Understanding for Instructional Videos\\n   Authors: Wenliang Guo, Yu Kong\\n   ID: 2512.03479v1\\n   Categories: Primary: cs.CV\\n   Published: 2025-12-03T06:14:26Z\\n   Preview: Understanding procedural activities is crucial for developing future assistive AI that can reason about complex real-world tasks.\\n\\n9. Think Before You Drive: World Model-Inspired Multimodal Grounding for Autonomous Vehicles\\n   Authors: Haicheng Liao, Huanming Shen, Bonan Wang, Yongkang Li, Yihong Tang, Chengyue Wang, Dingyi Zhuang, Kehua Chen, Hai Yang, Chengzhong Xu, Zhenning Li\\n   ID: 2512.03454v1\\n   Categories: Primary: cs.CV, Additional: cs.AI\\n   Published: 2025-12-03T05:14:16Z\\n   Preview: Interpreting natural-language commands to localize target objects is critical for autonomous driving (AD).\\n\\n10. Multi-Aspect Knowledge-Enhanced Medical Vision-Language Pretraining with Multi-Agent Data Generation\\n   Authors: Xieji Li, Siyuan Yan, Yingsheng Liu, H. Peter Soyer, Monika Janda, Victoria Mar, Zongyuan Ge\\n   ID: 2512.03445v1\\n   Categories: Primary: cs.CV, Additional: cs.AI\\n   Published: 2025-12-03T04:55:54Z\\n   Preview: Vision-language pretraining (VLP) has emerged as a powerful paradigm in medical image analysis, enabling representation learning from large-scale image-text pairs without relying on expensive manual a...\\n\\n11. HarnessAgent: Scaling Automatic Fuzzing Harness Construction with Tool-Augmented LLM Pipelines\\n   Authors: Kang Yang, Yunhang Zhang, Zichuan Li, GuanHong Tao, Jun Xu, XiaoJing Liao\\n   ID: 2512.03420v1\\n   Categories: Primary: cs.CR, Additional: cs.SE\\n   Published: 2025-12-03T03:55:09Z\\n   Preview: Large language model (LLM)-based techniques have achieved notable progress in generating harnesses for program fuzzing.\\n\\n12. BookRAG: A Hierarchical Structure-aware Index-based Approach for Retrieval-Augmented Generation on Complex Documents\\n   Authors: Shu Wang, Yingli Zhou, Yixiang Fang\\n   ID: 2512.03413v1\\n   Categories: Primary: cs.IR, Additional: cs.AI\\n   Published: 2025-12-03T03:40:49Z\\n   Preview: As an effective method to boost the performance of Large Language Models (LLMs) on the question answering (QA) task, Retrieval-Augmented Generation (RAG), which queries highly relevant information fro...\\n\\n13. Spoken Conversational Agents with Large Language Models\\n   Authors: Chao-Han Huck Yang, Andreas Stolcke, Larry Heck\\n   ID: 2512.02593v1\\n   Categories: Primary: cs.CL, Additional: cs.MA, cs.NE, cs.SD, eess.AS\\n   Published: 2025-12-02T10:02:10Z\\n   Preview: Spoken conversational agents are converging toward voice-native LLMs.\\n\\n14. EZYer: A simulacrum of high school with generative agent\\n   Authors: Jinming Yang, Zimu Ji, Weiqi Luo, Gaoxi Wang, Bin Ma, Yueling Deng\\n   ID: 2512.02561v1\\n   Categories: Primary: cs.MA, Additional: cs.AI\\n   Published: 2025-12-02T09:28:59Z\\n   Preview: With the rapid development of the online education and large language model, the existing educational tools still suffer from incomplete service, insufficient performance and weak interactivity in ter...\\n\\n15. In-Context Distillation with Self-Consistency Cascades: A Simple, Training-Free Way to Reduce LLM Agent Costs\\n   Authors: Vishnu Sarukkai, Asanshay Gupta, James Hong, Micha\\u00ebl Gharbi, Kayvon Fatahalian\\n   ID: 2512.02543v1\\n   Categories: Primary: cs.LG\\n   Published: 2025-12-02T09:11:05Z\\n   Preview: The world currently has an abundance of ideas for how to use new LLM agents, and developers seek to rapidly prototype and test new agentic designs.\\n\\n16. Aetheria: A multimodal interpretable content safety framework based on multi-agent debate and collaboration\\n   Authors: Yuxiang He, Jian Zhao, Yuchen Yuan, Tianle Zhang, Wei Cai, Haojie Cheng, Ziyan Shi, Ming Zhu, Haichuan Tang, Chi Zhang, Xuelong Li\\n   ID: 2512.02530v1\\n   Categories: Primary: cs.AI\\n   Published: 2025-12-02T08:49:54Z\\n   Preview: The exponential growth of digital content presents significant challenges for content safety.\\n\\n17. WorldMM: Dynamic Multimodal Memory Agent for Long Video Reasoning\\n   Authors: Woongyeong Yeo, Kangsan Kim, Jaehong Yoon, Sung Ju Hwang\\n   ID: 2512.02425v1\\n   Categories: Primary: cs.CV, Additional: cs.AI, cs.CL, cs.IR, cs.LG\\n   Published: 2025-12-02T05:14:52Z\\n   Preview: Recent advances in video large language models have demonstrated strong capabilities in understanding short clips.\\n\\n18. Skywork-R1V4: Toward Agentic Multimodal Intelligence through Interleaved Thinking with Images and DeepResearch\\n   Authors: Yifan Zhang, Liang Hu, Haofeng Sun, Peiyu Wang, Yichen Wei, Shukang Yin, Jiangbo Pei, Wei Shen, Peng Xia, Yi Peng, Tianyidan Xie, Eric Li, Yang Liu, Xuchen Song, Yahui Zhou\\n   ID: 2512.02395v1\\n   Categories: Primary: cs.CV\\n   Published: 2025-12-02T04:12:57Z\\n   Preview: Despite recent progress in multimodal agentic systems, existing approaches often treat image manipulation and web search as disjoint capabilities, rely heavily on costly reinforcement learning, and la...\\n\\n19. Benchmarking LLM Agents for Wealth-Management Workflows\\n   Authors: Rory Milsom\\n   ID: 2512.02230v1\\n   Categories: Primary: cs.AI\\n   Published: 2025-12-01T21:56:21Z\\n   Preview: Modern work relies on an assortment of digital collaboration tools, yet routine processes continue to suffer from human error and delay.\\n\\n20. How Far Are We from Genuinely Useful Deep Research Agents?\\n   Authors: Dingling Zhang, He Zhu, Jincheng Ren, Kangqi Song, Xinran Zhou, Boyu Feng, Shudong Liu, Jiabin Luo, Weihao Xie, Zhaohui Wang, Tianrui Qin, King Zhu, Yuqing Wang, Qianben Chen, Yuchen Eleanor Jiang, Wei Wang, Jiaheng Liu, Wangchunshu Zhou\\n   ID: 2512.01948v1\\n   Categories: Primary: cs.CL\\n   Published: 2025-12-01T17:58:59Z\\n   Preview: Deep Research Agents (DRAs) aim to automatically produce analyst-level reports through iterative information retrieval and synthesis.\\n\\n21. Agentic Policy Optimization via Instruction-Policy Co-Evolution\\n   Authors: Han Zhou, Xingchen Wan, Ivan Vuli\\u0107, Anna Korhonen\\n   ID: 2512.01945v1\\n   Categories: Primary: cs.LG, Additional: cs.AI, cs.CL\\n   Published: 2025-12-01T17:56:29Z\\n   Preview: Reinforcement Learning with Verifiable Rewards (RLVR) has advanced the reasoning capability of large language models (LLMs), enabling autonomous agents that can conduct effective multi-turn and tool-i...\\n\\n22. MMAG: Mixed Memory-Augmented Generation for Large Language Models Applications\\n   Authors: Stefano Zeppieri\\n   ID: 2512.01710v2\\n   Categories: Primary: cs.CL, Additional: cs.IR\\n   Published: 2025-12-01T14:16:57Z\\n   Preview: Large Language Models (LLMs) excel at generating coherent text within a single prompt but fall short in sustaining relevance, personalization, and continuity across extended interactions.\\n\\n23. Dr.Mi-Bench: A Modular-integrated Benchmark for Scientific Deep Research Agent\\n   Authors: Zhihan Guo, Feiyang Xu, Yifan Li, Muzhi Li, Shuai Zou, Jiele Wu, Han Shi, Haoli Bai, Ho-fung Leung, Irwin King\\n   ID: 2512.00986v1\\n   Categories: Primary: cs.CL\\n   Published: 2025-11-30T17:16:47Z\\n   Preview: The explosive growth in academic literature necessitates automated deep research (DR) agents, yet their evaluation remains a significant challenge.\\n\\n24. Memory-Integrated Reconfigurable Adapters: A Unified Framework for Settings with Multiple Tasks\\n   Authors: Susmit Agrawal, Krishn Vishwas Kher, Saksham Mittal, Swarnim Maheshwari, Vineeth N. Balasubramanian\\n   ID: 2512.00940v1\\n   Categories: Primary: cs.LG\\n   Published: 2025-11-30T15:45:55Z\\n   Preview: Organisms constantly pivot between tasks such as evading predators, foraging, traversing rugged terrain, and socializing, often within milliseconds.\\n\\n25. ML-Tool-Bench: Tool-Augmented Planning for ML Tasks\\n   Authors: Yaswanth Chittepu, Raghavendra Addanki, Tung Mai, Anup Rao, Branislav Kveton\\n   ID: 2512.00672v1\\n   Categories: Primary: cs.LG, Additional: cs.AI\\n   Published: 2025-11-29T23:59:40Z\\n   Preview: The development of autonomous machine learning (ML) agents capable of end-to-end data science workflows represents a significant frontier in artificial intelligence.\\n\\n\"]", "tool_call_id": "call_wxFZHag4GpUWkgJCVqzkRRbO"}, {"role": "assistant", "content": "", "tool_calls": [{"id": "call_Zzixlq2O2pJMx8RRYSbtj7u4", "type": "function", "function": {"name": "get_paper_data", "arguments": "{\"paper_id\": \"2512.01948v1\"}"}}, {"id": "call_RKnT5QfH3UzBlWl7LU8URriM", "type": "function", "function": {"name": "get_paper_data", "arguments": "{\"paper_id\": \"2512.03887v2\"}"}}, {"id": "call_ywekqaFuZapHMGrU0LKPidzx", "type": "function", "function": {"name": "get_paper_data", "arguments": "{\"paper_id\": \"2511.21729v1\"}"}}, {"id": "call_FTkWipLI05JXDaTgYS0xtgbw", "type": "function", "function": {"name": "get_paper_data", "arguments": "{\"paper_id\": \"2511.19969v1\"}"}}, {"id": "call_m5Y8yOLnylwIyOsWTFAZgkoA", "type": "function", "function": {"name": "get_paper_data", "arguments": "{\"paper_id\": \"2512.04343v1\"}"}}, {"id": "call_ybHuNDTnP3LzgUeZNHiX6l8d", "type": "function", "function": {"name": "get_paper_data", "arguments": "{\"paper_id\": \"2511.18177v1\"}"}}, {"id": "call_jSyZSv8qD7K1nh0OfnmB3JiS", "type": "function", "function": {"name": "get_paper_data", "arguments": "{\"paper_id\": \"2511.22850v1\"}"}}, {"id": "call_6Rpnt58eeZnN2OAFlAG3L2Ai", "type": "function", "function": {"name": "get_paper_data", "arguments": "{\"paper_id\": \"2511.17467v2\"}"}}, {"id": "call_bcLbKIRxHOLwSkC7yEVM7BoG", "type": "function", "function": {"name": "get_paper_data", "arguments": "{\"paper_id\": \"2511.20227v1\"}"}}, {"id": "call_pdVijcKxLM4X9KXaYlGq50Pw", "type": "function", "function": {"name": "get_paper_data", "arguments": "{\"paper_id\": \"2512.00986v1\"}"}}]}, {"role": "tool", "content": "[\"Title: How Far Are We from Genuinely Useful Deep Research Agents?\\n\\nMetadata:\\n- Authors: Dingling Zhang, He Zhu, Jincheng Ren, Kangqi Song, Xinran Zhou, Boyu Feng, Shudong Liu, Jiabin Luo, Weihao Xie, Zhaohui Wang, Tianrui Qin, King Zhu, Yuqing Wang, Qianben Chen, Yuchen Eleanor Jiang, Wei Wang, Jiaheng Liu, Wangchunshu Zhou\\n- Published: 2025-12-01T17:58:59Z\\n- Last Updated: 2025-12-01T17:58:59Z\\n- Categories: Primary: cs.CL\\n\\nAbstract:\\nDeep Research Agents (DRAs) aim to automatically produce analyst-level reports through iterative information retrieval and synthesis. However, most existing DRAs were validated on question-answering benchmarks, while research on generating comprehensive reports remains overlooked. Worse, current benchmarks for report synthesis suffer from task complexity and subjective metrics -- this fails to reflect user demands and limits the practical utility of generated reports. To address these gaps, we present Fine-grained DEepResearch bench (FINDER), an enhanced benchmark consisting of 100 human-curated research tasks with 419 structured checklist items that standardize report structure, analytical depth, and factual grounding. Based on approximately 1,000 reports produced by mainstream DRAs, we further propose Deep rEsearch Failure Taxonomy (DEFT), the first failure taxonomy for deep research agents. DEFT contains 14 fine-grained failure modes across reasoning, retrieval, and generation, and is built upon grounded theory with human-LLM co-annotating and inter-annotator reliability validation. Our experimental findings reveal that current DRAs struggle not with task comprehension but with evidence integration, verification, and reasoning-resilient planning.\\n\\nAccess Options:\\n- Abstract page: https://arxiv.org/abs/2512.01948v1\\n- Full text HTML version: https://arxiv.org/html/2512.01948v1\\n- PDF version: https://arxiv.org/pdf/2512.01948v1\\n\\nAdditional Information:\\n- Comment: 34 pages\\n\"]", "tool_call_id": "call_Zzixlq2O2pJMx8RRYSbtj7u4"}, {"role": "tool", "content": "[\"Title: A Hierarchical Tree-based approach for creating Configurable and Static Deep Research Agent (Static-DRA)\\n\\nMetadata:\\n- Authors: Saurav Prateek\\n- Published: 2025-12-03T15:37:13Z\\n- Last Updated: 2025-12-04T05:41:20Z\\n- Categories: Primary: cs.AI\\n\\nAbstract:\\nThe advancement in Large Language Models has driven the creation of complex agentic systems, such as Deep Research Agents (DRAs), to overcome the limitations of static Retrieval Augmented Generation (RAG) pipelines in handling complex, multi-turn research tasks. This paper introduces the Static Deep Research Agent (Static-DRA), a novel solution built upon a configurable and hierarchical Tree-based static workflow. The core contribution is the integration of two user-tunable parameters, Depth and Breadth, which provide granular control over the research intensity. This design allows end-users to consciously balance the desired quality and comprehensiveness of the research report against the associated computational cost of Large Language Model (LLM) interactions. The agent's architecture, comprising Supervisor, Independent, and Worker agents, facilitates effective multi-hop information retrieval and parallel sub-topic investigation. We evaluate the Static-DRA against the established DeepResearch Bench using the RACE (Reference-based Adaptive Criteria-driven Evaluation) framework. Configured with a depth of 2 and a breadth of 5, and powered by the gemini-2.5-pro model, the agent achieved an overall score of 34.72. Our experiments validate that increasing the configured Depth and Breadth parameters results in a more in-depth research process and a correspondingly higher evaluation score. The Static-DRA offers a pragmatic and resource-aware solution, empowering users with transparent control over the deep research process. The entire source code, outputs and benchmark results are open-sourced at https://github.com/SauravP97/Static-Deep-Research/\\n\\nAccess Options:\\n- Abstract page: https://arxiv.org/abs/2512.03887v2\\n- Full text HTML version: https://arxiv.org/html/2512.03887v2\\n- PDF version: https://arxiv.org/pdf/2512.03887v2\\n\\nAdditional Information:\\n- Comment: 16 pages, 6 figures, 4 tables. Code available at: https://github.com/SauravP97/Static-Deep-Research\\n\"]", "tool_call_id": "call_RKnT5QfH3UzBlWl7LU8URriM"}, {"role": "tool", "content": "[\"Title: Beyond Component Strength: Synergistic Integration and Adaptive Calibration in Multi-Agent RAG Systems\\n\\nMetadata:\\n- Authors: Jithin Krishnan\\n- Published: 2025-11-21T07:53:56Z\\n- Last Updated: 2025-11-21T07:53:56Z\\n- Categories: Primary: cs.CL, Additional: cs.AI\\n\\nAbstract:\\nBuilding reliable retrieval-augmented generation (RAG) systems requires more than adding powerful components; it requires understanding how they interact. Using ablation studies on 50 queries (15 answerable, 10 edge cases, and 25 adversarial), we show that enhancements such as hybrid retrieval, ensemble verification, and adaptive thresholding provide almost no benefit when used in isolation, yet together achieve a 95% reduction in abstention (from 40% to 2%) without increasing hallucinations. We also identify a measurement challenge: different verification strategies can behave safely but assign inconsistent labels (for example, \\\"abstained\\\" versus \\\"unsupported\\\"), creating apparent hallucination rates that are actually artifacts of labeling. Our results show that synergistic integration matters more than the strength of any single component, that standardized metrics and labels are essential for correctly interpreting performance, and that adaptive calibration is needed to prevent overconfident over-answering even when retrieval quality is high.\\n\\nAccess Options:\\n- Abstract page: https://arxiv.org/abs/2511.21729v1\\n- Full text HTML version: https://arxiv.org/html/2511.21729v1\\n- PDF version: https://arxiv.org/pdf/2511.21729v1\\n\\nAdditional Information:\\n- Comment: 10 pages, 4 figures\\n\"]", "tool_call_id": "call_ywekqaFuZapHMGrU0LKPidzx"}, {"role": "tool", "content": "[\"Title: M$^3$Prune: Hierarchical Communication Graph Pruning for Efficient Multi-Modal Multi-Agent Retrieval-Augmented Generation\\n\\nMetadata:\\n- Authors: Weizi Shao, Taolin Zhang, Zijie Zhou, Chen Chen, Chengyu Wang, Xiaofeng He\\n- Published: 2025-11-25T06:29:13Z\\n- Last Updated: 2025-11-25T06:29:13Z\\n- Categories: Primary: cs.AI\\n\\nAbstract:\\nRecent advancements in multi-modal retrieval-augmented generation (mRAG), which enhance multi-modal large language models (MLLMs) with external knowledge, have demonstrated that the collective intelligence of multiple agents can significantly outperform a single model through effective communication. Despite impressive performance, existing multi-agent systems inherently incur substantial token overhead and increased computational costs, posing challenges for large-scale deployment. To address these issues, we propose a novel Multi-Modal Multi-agent hierarchical communication graph PRUNING framework, termed M$^3$Prune. Our framework eliminates redundant edges across different modalities, achieving an optimal balance between task performance and token overhead. Specifically, M$^3$Prune first applies intra-modal graph sparsification to textual and visual modalities, identifying the edges most critical for solving the task. Subsequently, we construct a dynamic communication topology using these key edges for inter-modal graph sparsification. Finally, we progressively prune redundant edges to obtain a more efficient and hierarchical topology. Extensive experiments on both general and domain-specific mRAG benchmarks demonstrate that our method consistently outperforms both single-agent and robust multi-agent mRAG systems while significantly reducing token consumption.\\n\\nAccess Options:\\n- Abstract page: https://arxiv.org/abs/2511.19969v1\\n- Full text HTML version: https://arxiv.org/html/2511.19969v1\\n- PDF version: https://arxiv.org/pdf/2511.19969v1\\n\"]", "tool_call_id": "call_FTkWipLI05JXDaTgYS0xtgbw"}, {"role": "tool", "content": "[\"Title: The Personalization Paradox: Semantic Loss vs. Reasoning Gains in Agentic AI Q&A\\n\\nMetadata:\\n- Authors: Satyajit Movidi, Stephen Russell\\n- Published: 2025-12-04T00:12:41Z\\n- Last Updated: 2025-12-04T00:12:41Z\\n- Categories: Primary: cs.IR\\n\\nAbstract:\\nAIVisor, an agentic retrieval-augmented LLM for student advising, was used to examine how personalization affects system performance across multiple evaluation dimensions. Using twelve authentic advising questions intentionally designed to stress lexical precision, we compared ten personalized and non-personalized system configurations and analyzed outcomes with a Linear Mixed-Effects Model across lexical (BLEU, ROUGE-L), semantic (METEOR, BERTScore), and grounding (RAGAS) metrics. Results showed a consistent trade-off: personalization reliably improved reasoning quality and grounding, yet introduced a significant negative interaction on semantic similarity, driven not by poorer answers but by the limits of current metrics, which penalize meaningful personalized deviations from generic reference texts. This reveals a structural flaw in prevailing LLM evaluation methods, which are ill-suited for assessing user-specific responses. The fully integrated personalized configuration produced the highest overall gains, suggesting that personalization can enhance system effectiveness when evaluated with appropriate multidimensional metrics. Overall, the study demonstrates that personalization produces metric-dependent shifts rather than uniform improvements and provides a methodological foundation for more transparent and robust personalization in agentic AI.\\n\\nAccess Options:\\n- Abstract page: https://arxiv.org/abs/2512.04343v1\\n- Full text HTML version: https://arxiv.org/html/2512.04343v1\\n- PDF version: https://arxiv.org/pdf/2512.04343v1\\n\"]", "tool_call_id": "call_m5Y8yOLnylwIyOsWTFAZgkoA"}, {"role": "tool", "content": "[\"Title: Rethinking Retrieval: From Traditional Retrieval Augmented Generation to Agentic and Non-Vector Reasoning Systems in the Financial Domain for Large Language Models\\n\\nMetadata:\\n- Authors: Elias Lumer, Matt Melich, Olivia Zino, Elena Kim, Sara Dieter, Pradeep Honaganahalli Basavaraju, Vamse Kumar Subbiah, James A. Burke, Roberto Hernandez\\n- Published: 2025-11-22T20:06:25Z\\n- Last Updated: 2025-11-22T20:06:25Z\\n- Categories: Primary: cs.CL\\n\\nAbstract:\\nRecent advancements in Retrieval-Augmented Generation (RAG) have enabled Large Language Models to answer financial questions using external knowledge bases of U.S. SEC filings, earnings reports, and regulatory documents. However, existing work lacks systematic comparison of vector-based and non-vector RAG architectures for financial documents, and the empirical impact of advanced RAG techniques on retrieval accuracy, answer quality, latency, and cost remain unclear. We present the first systematic evaluation comparing vector-based agentic RAG using hybrid search and metadata filtering against hierarchical node-based systems that traverse document structure without embeddings. We evaluate two enhancement techniques applied to the vector-based architecture, i) cross-encoder reranking for retrieval precision, and ii) small-to-big chunk retrieval for context completeness. Across 1,200 SEC 10-K, 10-Q, and 8-K filings on a 150-question benchmark, we measure retrieval metrics (MRR, Recall@5), answer quality through LLM-as-a-judge pairwise comparisons, latency, and preprocessing costs. Vector-based agentic RAG achieves a 68% win rate over hierarchical node-based systems with comparable latency (5.2 compared to 5.98 seconds). Cross-encoder reranking achieves a 59% absolute improvement at optimal parameters (10, 5) for MRR@5. Small-to-big retrieval achieves a 65% win rate over baseline chunking with only 0.2 seconds additional latency. Our findings reveal that applying advanced RAG techniques to financial Q&A systems improves retrieval accuracy, answer quality, and has cost-performance tradeoffs to be considered in production.\\n\\nAccess Options:\\n- Abstract page: https://arxiv.org/abs/2511.18177v1\\n- Full text HTML version: https://arxiv.org/html/2511.18177v1\\n- PDF version: https://arxiv.org/pdf/2511.18177v1\\n\\nAdditional Information:\\n- Comment: 8 pages, 2 figures\\n\"]", "tool_call_id": "call_ybHuNDTnP3LzgUeZNHiX6l8d"}, {"role": "tool", "content": "[\"Title: Resolving Evidence Sparsity: Agentic Context Engineering for Long-Document Understanding\\n\\nMetadata:\\n- Authors: Keliang Liu, Zizhi Chen, Mingcheng Li, Jingqun Tang, Dingkang Yang, Lihua Zhang\\n- Published: 2025-11-28T03:09:40Z\\n- Last Updated: 2025-11-28T03:09:40Z\\n- Categories: Primary: cs.CV\\n\\nAbstract:\\nDocument understanding is a long standing practical task. Vision Language Models (VLMs) have gradually become a primary approach in this domain, demonstrating effective performance on single page tasks. However, their effectiveness diminishes when handling long documents. In such scenarios, clues are often scattered across multiple pages and modalities, and redundancy from lengthy inputs can impair the models judgment. While retrieval augmented generation mitigates this issue by filtering for question relevant content, the retrieved results still contain substantial redundancy. To address these limitations, we propose SLEUTH, a multi agent framework. Concretely, SLEUTH orchestrates a retriever and four collaborative agents in a coarse to fine process. The framework identifies key textual and visual clues within the retrieved pages, filters for salient visual evidence such as tables and charts, and analyzes the query to devise a reasoning strategy. It ultimately synthesizes a distilled, evidence dense multimodal context to generate the final prediction. SLEUTH is model agnostic and scalable. When paired with advanced VLM backbones, it consistently improves performance on multiple long document benchmarks, achieving state of the art results. Ablation studies verify each modules effectiveness and confirm the benefits of our hierarchical refinement paradigm.\\n\\nAccess Options:\\n- Abstract page: https://arxiv.org/abs/2511.22850v1\\n- Full text HTML version: https://arxiv.org/html/2511.22850v1\\n- PDF version: https://arxiv.org/pdf/2511.22850v1\\n\"]", "tool_call_id": "call_jSyZSv8qD7K1nh0OfnmB3JiS"}, {"role": "tool", "content": "[\"Title: PersonaAgent with GraphRAG: Community-Aware Knowledge Graphs for Personalized LLM\\n\\nMetadata:\\n- Authors: Siqi Liang, Yudi Zhang, Yue Guo\\n- Published: 2025-11-21T18:15:47Z\\n- Last Updated: 2025-12-02T04:45:19Z\\n- Categories: Primary: cs.LG, Additional: cs.AI\\n\\nAbstract:\\nWe propose a novel framework for persona-based language model system, motivated by the need for personalized AI agents that adapt to individual user preferences. In our approach, the agent embodies the user's \\\"persona\\\" (e.g. user profile or taste) and is powered by a large language model (LLM). To enable the agent to leverage rich contextual information, we introduce a Knowledge-Graph-enhanced Retrieval-Augmented Generation (Graph RAG) mechanism that constructs an LLM-derived graph index of relevant documents and summarizes communities of related information. Our framework generates personalized prompts by combining: (1) a summary of the user's historical behaviors and preferences extracted from the knowledge graph, and (2) relevant global interaction patterns identified through graph-based community detection. This dynamic prompt engineering approach allows the agent to maintain consistent persona-aligned behaviors while benefiting from collective knowledge. On the LaMP benchmark, our method improves news categorization F1 by 11.1%, movie tagging F1 by 56.1%, and reduces product rating MAE by 10.4% over prior methods. Our code is available at https://anonymous.4open.science/r/PersonaAgentwGraphRAG-DE6F\\n\\nAccess Options:\\n- Abstract page: https://arxiv.org/abs/2511.17467v2\\n- Full text HTML version: https://arxiv.org/html/2511.17467v2\\n- PDF version: https://arxiv.org/pdf/2511.17467v2\\n\"]", "tool_call_id": "call_6Rpnt58eeZnN2OAFlAG3L2Ai"}, {"role": "tool", "content": "[\"Title: HKRAG: Holistic Knowledge Retrieval-Augmented Generation Over Visually-Rich Documents\\n\\nMetadata:\\n- Authors: Anyang Tong, Xiang Niu, ZhiPing Liu, Chang Tian, Yanyan Wei, Zenglin Shi, Meng Wang\\n- Published: 2025-11-25T11:59:52Z\\n- Last Updated: 2025-11-25T11:59:52Z\\n- Categories: Primary: cs.IR\\n\\nAbstract:\\nExisting multimodal Retrieval-Augmented Generation (RAG) methods for visually rich documents (VRD) are often biased towards retrieving salient knowledge(e.g., prominent text and visual elements), while largely neglecting the critical fine-print knowledge(e.g., small text, contextual details). This limitation leads to incomplete retrieval and compromises the generator's ability to produce accurate and comprehensive answers. To bridge this gap, we propose HKRAG, a new holistic RAG framework designed to explicitly capture and integrate both knowledge types. Our framework features two key components: (1) a Hybrid Masking-based Holistic Retriever that employs explicit masking strategies to separately model salient and fine-print knowledge, ensuring a query-relevant holistic information retrieval; and (2) an Uncertainty-guided Agentic Generator that dynamically assesses the uncertainty of initial answers and actively decides how to integrate the two distinct knowledge streams for optimal response generation. Extensive experiments on open-domain visual question answering benchmarks show that HKRAG consistently outperforms existing methods in both zero-shot and supervised settings, demonstrating the critical importance of holistic knowledge retrieval for VRD understanding.\\n\\nAccess Options:\\n- Abstract page: https://arxiv.org/abs/2511.20227v1\\n- Full text HTML version: https://arxiv.org/html/2511.20227v1\\n- PDF version: https://arxiv.org/pdf/2511.20227v1\\n\"]", "tool_call_id": "call_bcLbKIRxHOLwSkC7yEVM7BoG"}, {"role": "tool", "content": "[\"Title: Dr.Mi-Bench: A Modular-integrated Benchmark for Scientific Deep Research Agent\\n\\nMetadata:\\n- Authors: Zhihan Guo, Feiyang Xu, Yifan Li, Muzhi Li, Shuai Zou, Jiele Wu, Han Shi, Haoli Bai, Ho-fung Leung, Irwin King\\n- Published: 2025-11-30T17:16:47Z\\n- Last Updated: 2025-11-30T17:16:47Z\\n- Categories: Primary: cs.CL\\n\\nAbstract:\\nThe explosive growth in academic literature necessitates automated deep research (DR) agents, yet their evaluation remains a significant challenge. First, existing benchmarks often focus narrowly on retrieval while neglecting high-level planning and reasoning. Second, existing benchmarks favor general domains over the scientific domains that are the core application for DR agents. To address these gaps, we introduce Dr.Mi-Bench, a Modular-integrated benchmark for scientific DR agents. Grounded in academic literature, our benchmark uses a human-annotated dataset of 200 instances across 10 scientific domains, including both research and review papers. Besides, we also propose a Modular-integrated Evaluation Paradigm for DR Agents (Dr.Mi-Eval), a novel modular-integrated evaluation paradigm, which leverages the rich structure of academic papers to assess the core competencies of planning, retrieval, and reasoning through two complementary modes: an end-to-end evaluation for DR agents and an isolated evaluation for foundational LLMs as potential backbones. Experimental results reveal a fragmented performance landscape: agents exhibit specialized strengths but share critical weaknesses, most notably in performing the multi-source retrieval required for review-style tasks and performing consistently across diverse scientific fields. Moreover, improving high-level planning capability is the crucial factor for unlocking the reasoning potential of foundational LLMs as backbones. By exposing these actionable failure modes, Dr.Mi-Bench provides a diagnostic tool to guide the development of more reliable academic research assistants.\\n\\nAccess Options:\\n- Abstract page: https://arxiv.org/abs/2512.00986v1\\n- Full text HTML version: https://arxiv.org/html/2512.00986v1\\n- PDF version: https://arxiv.org/pdf/2512.00986v1\\n\"]", "tool_call_id": "call_pdVijcKxLM4X9KXaYlGq50Pw"}], "tool_calls": [{"id": "call_bGDLswMpW4E1p1IYRyyZXmVV", "name": "notion-create-pages", "arguments": "{\"pages\":[{\"properties\":{\"title\":\"近期 RAG Agent 论文综述（arXiv，2025-12-06）\"},\"content\":\"# 一句话结论\\nRAG 正在从“静态检索+生成”演进为“具身化、可规划、可验证的 Agent 流水线”。近期工作集中在多智能体协同、个性化与不确定性建模、长文档证据蒸馏、以及工程侧的效率/评测体系，强调“系统集成与自适应校准”比单一组件升级更关键。\\n<empty-block/>\\n## 关键观点与趋势\\n- 多智能体协同 > 单点增强（Krishnan, 2025）\\n  - 混合检索、集成式验证与自适应阈值如果孤立使用收益有限，但“协同集成”可显著降低拒答率且不增幻觉；同时呼吁统一标签与指标，避免评测偏差。\\n- 通信与令牌效率成为多模态/多智能体 RAG 的核心工程挑战（Shao et al., 2025）\\n  - 通过分层通信图剪枝，跨模态/模态内稀疏化，能在保持或提升效果的同时显著降低 token 成本。\\n- 个性化带来“评测悖论”（Movidi & Russell, 2025）\\n  - 个性化 Agent 往往提升推理与事实扎根，但会在 BERTScore/METEOR 等“语义相似度”指标上显得更差，原因是指标惩罚偏离标准答案的“合理个性化”。需采用多维度指标（含 grounding）与以任务为中心的评测。\\n- Agentic Context Engineering 解决长文档“证据稀疏与冗余”难题（Liu et al., 2025）\\n  - 以多代理分工粗到细筛选关键线索、图表与策略，蒸馏出高密度证据上下文，显著提升长文档理解性能。\\n- 垂直领域实践：向“工程化的 Agentic RAG”迈进（Lumer et al., 2025）\\n  - 金融文档中，基于向量的 agentic RAG（+ 交叉编码器重排 + small-to-big 检索）在质量与时延上优于节点/层级式非向量系统，但需权衡预处理与在线成本。\\n- 图谱与社区摘要用于个性化提示（Liang et al., 2025）\\n  - GraphRAG 通过图社区检测和用户行为摘要，生成 persona 对齐提示，实现个性化与群体知识的结合。\\n- 面向视觉富文本的“整体式”检索与不确定性驱动生成（Tong et al., 2025）\\n  - 显性遮罩分别建模“显著知识/细粒度知识”，再由不确定性引导整合，提升多模态 RAG 的完整性与稳健性。\\n- 深度研究代理：评测先行，失败点在“证据整合、验证与抗噪规划”（Zhang et al., 2025；Guo et al., 2025）\\n  - FINDER 与 Dr.Mi-Bench 等基准暴露：多源检索与高层规划是瓶颈；提出 DEFT 失败分类学与模块化评测范式，指向更可靠的研究 Agent 设计。\\n- 可控的层次化静态工作流（Prateek, 2025）\\n  - 通过“深度/广度”参数让用户在研究质量与成本间可调，树状多代理结构便于并行子课题探索。\\n<empty-block/>\\n## 对落地系统的建议\\n- 优先做“系统级集成与校准”：混合检索 + 验证集成 + 动态阈值，而非仅换更强的检索/模型。\\n- 在多智能体/多模态场景下做通信图稀疏化与会话预算控制，显著降本。\\n- 采用多维评测：将 grounding、拒答/不确定性、任务完成度与个性化收益纳入指标；避免只看语义相似度。\\n- 长文档任务采用“证据蒸馏链”：检索→线索抽取→结构化证据→不确定性引导的回答整合。\\n- 垂直领域（如金融）结合重排与分层检索策略，小步验证线上/离线成本与延迟。\\n- 个性化用图谱/社区摘要构造提示，配合不确定性感知与可追溯引用，兼顾体验与合规。\\n<empty-block/>\\n## 参考论文（arXiv）\\n- How Far Are We from Genuinely Useful Deep Research Agents? (Zhang et al., 2025) — https://arxiv.org/abs/2512.01948\\n- A Hierarchical Tree-based approach for creating Configurable and Static Deep Research Agent (Prateek, 2025) — https://arxiv.org/abs/2512.03887\\n- Beyond Component Strength: Synergistic Integration and Adaptive Calibration in Multi-Agent RAG Systems (Krishnan, 2025) — https://arxiv.org/abs/2511.21729\\n- M^3Prune: Hierarchical Communication Graph Pruning for Efficient Multi-Modal Multi-Agent RAG (Shao et al., 2025) — https://arxiv.org/abs/2511.19969\\n- The Personalization Paradox: Semantic Loss vs. Reasoning Gains in Agentic AI Q&A (Movidi & Russell, 2025) — https://arxiv.org/abs/2512.04343\\n- Rethinking Retrieval: From Traditional RAG to Agentic and Non-Vector Reasoning in Finance (Lumer et al., 2025) — https://arxiv.org/abs/2511.18177\\n- Resolving Evidence Sparsity: Agentic Context Engineering for Long-Document Understanding (Liu et al., 2025) — https://arxiv.org/abs/2511.22850\\n- PersonaAgent with GraphRAG: Community-Aware Knowledge Graphs for Personalized LLM (Liang et al., 2025) — https://arxiv.org/abs/2511.17467\\n- HKRAG: Holistic Knowledge RAG Over Visually-Rich Documents (Tong et al., 2025) — https://arxiv.org/abs/2511.20227\\n- Dr.Mi-Bench: A Modular-integrated Benchmark for Scientific Deep Research Agent (Guo et al., 2025) — https://arxiv.org/abs/2512.00986\"}]}"}], "response_content": ""}
{"timestamp": "2025-12-06T08:29:49.183408Z", "type": "tool_call", "tool": "notion-create-pages", "args": {"pages": [{"properties": {"title": "近期 RAG Agent 论文综述（arXiv，2025-12-06）"}, "content": "# 一句话结论\nRAG 正在从“静态检索+生成”演进为“具身化、可规划、可验证的 Agent 流水线”。近期工作集中在多智能体协同、个性化与不确定性建模、长文档证据蒸馏、以及工程侧的效率/评测体系，强调“系统集成与自适应校准”比单一组件升级更关键。\n<empty-block/>\n## 关键观点与趋势\n- 多智能体协同 > 单点增强（Krishnan, 2025）\n  - 混合检索、集成式验证与自适应阈值如果孤立使用收益有限，但“协同集成”可显著降低拒答率且不增幻觉；同时呼吁统一标签与指标，避免评测偏差。\n- 通信与令牌效率成为多模态/多智能体 RAG 的核心工程挑战（Shao et al., 2025）\n  - 通过分层通信图剪枝，跨模态/模态内稀疏化，能在保持或提升效果的同时显著降低 token 成本。\n- 个性化带来“评测悖论”（Movidi & Russell, 2025）\n  - 个性化 Agent 往往提升推理与事实扎根，但会在 BERTScore/METEOR 等“语义相似度”指标上显得更差，原因是指标惩罚偏离标准答案的“合理个性化”。需采用多维度指标（含 grounding）与以任务为中心的评测。\n- Agentic Context Engineering 解决长文档“证据稀疏与冗余”难题（Liu et al., 2025）\n  - 以多代理分工粗到细筛选关键线索、图表与策略，蒸馏出高密度证据上下文，显著提升长文档理解性能。\n- 垂直领域实践：向“工程化的 Agentic RAG”迈进（Lumer et al., 2025）\n  - 金融文档中，基于向量的 agentic RAG（+ 交叉编码器重排 + small-to-big 检索）在质量与时延上优于节点/层级式非向量系统，但需权衡预处理与在线成本。\n- 图谱与社区摘要用于个性化提示（Liang et al., 2025）\n  - GraphRAG 通过图社区检测和用户行为摘要，生成 persona 对齐提示，实现个性化与群体知识的结合。\n- 面向视觉富文本的“整体式”检索与不确定性驱动生成（Tong et al., 2025）\n  - 显性遮罩分别建模“显著知识/细粒度知识”，再由不确定性引导整合，提升多模态 RAG 的完整性与稳健性。\n- 深度研究代理：评测先行，失败点在“证据整合、验证与抗噪规划”（Zhang et al., 2025；Guo et al., 2025）\n  - FINDER 与 Dr.Mi-Bench 等基准暴露：多源检索与高层规划是瓶颈；提出 DEFT 失败分类学与模块化评测范式，指向更可靠的研究 Agent 设计。\n- 可控的层次化静态工作流（Prateek, 2025）\n  - 通过“深度/广度”参数让用户在研究质量与成本间可调，树状多代理结构便于并行子课题探索。\n<empty-block/>\n## 对落地系统的建议\n- 优先做“系统级集成与校准”：混合检索 + 验证集成 + 动态阈值，而非仅换更强的检索/模型。\n- 在多智能体/多模态场景下做通信图稀疏化与会话预算控制，显著降本。\n- 采用多维评测：将 grounding、拒答/不确定性、任务完成度与个性化收益纳入指标；避免只看语义相似度。\n- 长文档任务采用“证据蒸馏链”：检索→线索抽取→结构化证据→不确定性引导的回答整合。\n- 垂直领域（如金融）结合重排与分层检索策略，小步验证线上/离线成本与延迟。\n- 个性化用图谱/社区摘要构造提示，配合不确定性感知与可追溯引用，兼顾体验与合规。\n<empty-block/>\n## 参考论文（arXiv）\n- How Far Are We from Genuinely Useful Deep Research Agents? (Zhang et al., 2025) — https://arxiv.org/abs/2512.01948\n- A Hierarchical Tree-based approach for creating Configurable and Static Deep Research Agent (Prateek, 2025) — https://arxiv.org/abs/2512.03887\n- Beyond Component Strength: Synergistic Integration and Adaptive Calibration in Multi-Agent RAG Systems (Krishnan, 2025) — https://arxiv.org/abs/2511.21729\n- M^3Prune: Hierarchical Communication Graph Pruning for Efficient Multi-Modal Multi-Agent RAG (Shao et al., 2025) — https://arxiv.org/abs/2511.19969\n- The Personalization Paradox: Semantic Loss vs. Reasoning Gains in Agentic AI Q&A (Movidi & Russell, 2025) — https://arxiv.org/abs/2512.04343\n- Rethinking Retrieval: From Traditional RAG to Agentic and Non-Vector Reasoning in Finance (Lumer et al., 2025) — https://arxiv.org/abs/2511.18177\n- Resolving Evidence Sparsity: Agentic Context Engineering for Long-Document Understanding (Liu et al., 2025) — https://arxiv.org/abs/2511.22850\n- PersonaAgent with GraphRAG: Community-Aware Knowledge Graphs for Personalized LLM (Liang et al., 2025) — https://arxiv.org/abs/2511.17467\n- HKRAG: Holistic Knowledge RAG Over Visually-Rich Documents (Tong et al., 2025) — https://arxiv.org/abs/2511.20227\n- Dr.Mi-Bench: A Modular-integrated Benchmark for Scientific Deep Research Agent (Guo et al., 2025) — https://arxiv.org/abs/2512.00986"}]}, "result": ["{\"pages\":[{\"id\":\"2c1b93bb-159c-816b-b55d-faeb7d4c49c0\",\"url\":\"https://www.notion.so/2c1b93bb159c816bb55dfaeb7d4c49c0\",\"properties\":{\"title\":\"近期 RAG Agent 论文综述（arXiv，2025-12-06）\"}}]}"]}
{"timestamp": "2025-12-06T08:29:53.392182Z", "type": "llm_call", "model": "gpt-5", "messages": [{"role": "system", "content": "You are an intelligent assistant equipped with specific tools.\n\n### WORKFLOW\n1. **Analyze Request**: Determine if the user's request requires performing actions (e.g., file I/O, web fetching) that your tools can handle.\n2. **Select Tool**: If an action is needed, select the appropriate tool from your available list.\n3. **Execute via Function Call**:\n   - You MUST use the native **function calling** mechanism to invoke the tool.\n   - Do NOT output the JSON arguments inside the conversation text (content).\n   - The system will only execute tools passed through the dedicated `tool_calls` field, NOT those written in the message body.\n\n### STRICT RULES\n- **NO JSON IN TEXT**: Never output raw JSON or code blocks containing tool arguments in your response text.\n- **Context Awareness**: Rely on the provided context. Do not invent file paths or data that do not exist.\n- **Fabrication Prohibited**: Do not make up file paths or information not present in the context.\n\nYou are a stateful assistant with access to prior conversation history. Use it to stay consistent, avoid repetition, and reference past context when helpful."}, {"role": "user", "content": "帮我查一下最近关于 RAG Agent 的 arXiv 论文，总结一下他们的核心观点。然后把总结写到 Notion 的页面里去。"}, {"role": "assistant", "content": "", "tool_calls": [{"id": "call_4aVFbtd5SHoo6jnwSWviCHj0", "type": "function", "function": {"name": "search_papers", "arguments": "{\"query\": \"ti:\\\"RAG agent\\\" OR abs:\\\"RAG agent\\\"\", \"max_results\": 25}"}}, {"id": "call_BiDHkzDWmSntNwWBNIlWKkQ5", "type": "function", "function": {"name": "search_papers", "arguments": "{\"query\": \"(ti:\\\"retrieval-augmented\\\" AND (ti:agent OR abs:agent)) OR abs:\\\"retrieval-augmented agent\\\"\", \"max_results\": 25}"}}, {"id": "call_wxFZHag4GpUWkgJCVqzkRRbO", "type": "function", "function": {"name": "search_papers", "arguments": "{\"query\": \"ti:\\\"agentic RAG\\\" OR abs:\\\"agentic RAG\\\" OR ti:\\\"agentic retrieval\\\" OR abs:\\\"agentic retrieval\\\"\", \"max_results\": 25}"}}]}, {"role": "tool", "content": "[\"Search Results:\\n\\n1. Evaluating Long-Context Reasoning in LLM-Based WebAgents\\n   Authors: Andy Chung, Yichi Zhang, Kaixiang Lin, Aditya Rawal, Qiaozi Gao, Joyce Chai\\n   ID: 2512.04307v1\\n   Categories: Primary: cs.LG, Additional: cs.AI\\n   Published: 2025-12-03T22:53:10Z\\n   Preview: As large language model (LLM)-based agents become increasingly integrated into daily digital interactions, their ability to reason across long interaction histories becomes crucial for providing perso...\\n\\n2. A Hierarchical Tree-based approach for creating Configurable and Static Deep Research Agent (Static-DRA)\\n   Authors: Saurav Prateek\\n   ID: 2512.03887v2\\n   Categories: Primary: cs.AI\\n   Published: 2025-12-03T15:37:13Z\\n   Preview: The advancement in Large Language Models has driven the creation of complex agentic systems, such as Deep Research Agents (DRAs), to overcome the limitations of static Retrieval Augmented Generation (...\\n\\n3. Think Before You Drive: World Model-Inspired Multimodal Grounding for Autonomous Vehicles\\n   Authors: Haicheng Liao, Huanming Shen, Bonan Wang, Yongkang Li, Yihong Tang, Chengyue Wang, Dingyi Zhuang, Kehua Chen, Hai Yang, Chengzhong Xu, Zhenning Li\\n   ID: 2512.03454v1\\n   Categories: Primary: cs.CV, Additional: cs.AI\\n   Published: 2025-12-03T05:14:16Z\\n   Preview: Interpreting natural-language commands to localize target objects is critical for autonomous driving (AD).\\n\\n4. BookRAG: A Hierarchical Structure-aware Index-based Approach for Retrieval-Augmented Generation on Complex Documents\\n   Authors: Shu Wang, Yingli Zhou, Yixiang Fang\\n   ID: 2512.03413v1\\n   Categories: Primary: cs.IR, Additional: cs.AI\\n   Published: 2025-12-03T03:40:49Z\\n   Preview: As an effective method to boost the performance of Large Language Models (LLMs) on the question answering (QA) task, Retrieval-Augmented Generation (RAG), which queries highly relevant information fro...\\n\\n5. Aetheria: A multimodal interpretable content safety framework based on multi-agent debate and collaboration\\n   Authors: Yuxiang He, Jian Zhao, Yuchen Yuan, Tianle Zhang, Wei Cai, Haojie Cheng, Ziyan Shi, Ming Zhu, Haichuan Tang, Chi Zhang, Xuelong Li\\n   ID: 2512.02530v1\\n   Categories: Primary: cs.AI\\n   Published: 2025-12-02T08:49:54Z\\n   Preview: The exponential growth of digital content presents significant challenges for content safety.\\n\\n6. CogEvo-Edu: Cognitive Evolution Educational Multi-Agent Collaborative System\\n   Authors: Yefeng Wu, Yuchen Song, Yecheng Zhao, Ling Wu, Shan Wan\\n   ID: 2512.00331v1\\n   Categories: Primary: cs.AI, Additional: cs.MA\\n   Published: 2025-11-29T05:41:57Z\\n   Preview: Large language models (LLMs) are increasingly deployed as conversational tutors in STEM education, yet most systems still rely on a single LLM with a static retrieval-augmented generation (RAG) pipeli...\\n\\n7. MegaChat: A Synthetic Persian Q&A Dataset for High-Quality Sales Chatbot Evaluation\\n   Authors: Mahdi Rahmani, AmirHossein Saffari, Reyhane Rahmani\\n   ID: 2511.23397v1\\n   Categories: Primary: cs.CL, Additional: cs.AI, cs.MA\\n   Published: 2025-11-28T17:44:20Z\\n   Preview: Small and medium-sized enterprises (SMEs) in Iran increasingly leverage Telegram for sales, where real-time engagement is essential for conversion.\\n\\n8. MCP vs RAG vs NLWeb vs HTML: A Comparison of the Effectiveness and Efficiency of Different Agent Interfaces to the Web (Technical Report)\\n   Authors: Aaron Steiner, Ralph Peeters, Christian Bizer\\n   ID: 2511.23281v1\\n   Categories: Primary: cs.CL\\n   Published: 2025-11-28T15:32:15Z\\n   Preview: Large language model agents are increasingly used to automate web tasks such as product search, offer comparison, and checkout.\\n\\n9. RAG-Empowered LLM-Driven Dynamic Radio Resource Management in Open 6G RAN\\n   Authors: Onur Salan, Burak \\u00c7\\u0131ra\\u011f, Onur Sever, \\u0130brahim H\\u00f6kelek, Ali G\\u00f6r\\u00e7in, Hakan Ali \\u00c7\\u0131rpan\\n   ID: 2511.22933v1\\n   Categories: Primary: eess.SP\\n   Published: 2025-11-28T07:20:25Z\\n   Preview: Implications of the advancements in the area of artificial intelligence to the wireless communications is extremely significant, especially in terms of resource management.\\n\\n10. Advancing Automated In-Isolation Validation in Repository-Level Code Translation\\n   Authors: Kaiyao Ke, Ali Reza Ibrahimzada, Rangeet Pan, Saurabh Sinha, Reyhaneh Jabbarvand\\n   ID: 2511.21878v1\\n   Categories: Primary: cs.SE, Additional: cs.PL\\n   Published: 2025-11-26T19:53:46Z\\n   Preview: Repository-level code translation aims to migrate entire repositories across programming languages while preserving functionality automatically.\\n\\n11. Chatty-KG: A Multi-Agent AI System for On-Demand Conversational Question Answering over Knowledge Graphs\\n   Authors: Reham Omar, Abdelghny Orogat, Ibrahim Abdelaziz, Omij Mangukiya, Panos Kalnis, Essam Mansour\\n   ID: 2511.20940v1\\n   Categories: Primary: cs.CL\\n   Published: 2025-11-26T00:18:55Z\\n   Preview: Conversational Question Answering over Knowledge Graphs (KGs) combines the factual grounding of KG-based QA with the interactive nature of dialogue systems.\\n\\n12. HKRAG: Holistic Knowledge Retrieval-Augmented Generation Over Visually-Rich Documents\\n   Authors: Anyang Tong, Xiang Niu, ZhiPing Liu, Chang Tian, Yanyan Wei, Zenglin Shi, Meng Wang\\n   ID: 2511.20227v1\\n   Categories: Primary: cs.IR\\n   Published: 2025-11-25T11:59:52Z\\n   Preview: Existing multimodal Retrieval-Augmented Generation (RAG) methods for visually rich documents (VRD) are often biased towards retrieving salient knowledge(e.g., prominent text and visual elements), whil...\\n\\n13. Large Language Model Aided Birt-Hogg-Dube Syndrome Diagnosis with Multimodal Retrieval-Augmented Generation\\n   Authors: Haoqing Li, Jun Shi, Xianmeng Chen, Qiwei Jia, Rui Wang, Wei Wei, Hong An, Xiaowen Hu\\n   ID: 2511.19834v1\\n   Categories: Primary: cs.CV\\n   Published: 2025-11-25T01:55:23Z\\n   Preview: Deep learning methods face dual challenges of limited clinical samples and low inter-class differentiation among Diffuse Cystic Lung Diseases (DCLDs) in advancing Birt-Hogg-Dube syndrome (BHD) diagnos...\\n\\n14. Beyond Protein Language Models: An Agentic LLM Framework for Mechanistic Enzyme Design\\n   Authors: Bruno Jacob, Khushbu Agarwal, Marcel Baer, Peter Rice, Simone Raugei\\n   ID: 2511.19423v1\\n   Categories: Primary: q-bio.QM, Additional: cs.AI\\n   Published: 2025-11-24T18:57:07Z\\n   Preview: We present Genie-CAT, a tool-augmented large-language-model (LLM) system designed to accelerate scientific hypothesis generation in protein design.\\n\\n15. Cross-Disciplinary Knowledge Retrieval and Synthesis: A Compound AI Architecture for Scientific Discovery\\n   Authors: Svitlana Volkova, Peter Bautista, Avinash Hiriyanna, Gabriel Ganberg, Isabel Erickson, Zachary Klinefelter, Nick Abele, Hsien-Te Kao, Grant Engberson\\n   ID: 2511.18298v1\\n   Categories: Primary: cs.AI\\n   Published: 2025-11-23T05:33:11Z\\n   Preview: The exponential growth of scientific knowledge has created significant barriers to cross-disciplinary knowledge discovery, synthesis and research collaboration.\\n\\n16. Rethinking Retrieval: From Traditional Retrieval Augmented Generation to Agentic and Non-Vector Reasoning Systems in the Financial Domain for Large Language Models\\n   Authors: Elias Lumer, Matt Melich, Olivia Zino, Elena Kim, Sara Dieter, Pradeep Honaganahalli Basavaraju, Vamse Kumar Subbiah, James A. Burke, Roberto Hernandez\\n   ID: 2511.18177v1\\n   Categories: Primary: cs.CL\\n   Published: 2025-11-22T20:06:25Z\\n   Preview: Recent advancements in Retrieval-Augmented Generation (RAG) have enabled Large Language Models to answer financial questions using external knowledge bases of U.S.\\n\\n17. AI-Augmented Bibliometric Framework: A Paradigm Shift with Agentic AI for Dynamic, Snippet-Based Research Analysis\\n   Authors: Adela Bara, Simona-Vasilica Oprea\\n   ID: 2511.21745v1\\n   Categories: Primary: cs.DL\\n   Published: 2025-11-22T08:41:38Z\\n   Preview: Our paper introduces a generative, multiagent AI framework designed to overcome the rigidity, limited flexibility and technical barriers of current bibliometric tools.\\n\\n18. PersonaAgent with GraphRAG: Community-Aware Knowledge Graphs for Personalized LLM\\n   Authors: Siqi Liang, Yudi Zhang, Yue Guo\\n   ID: 2511.17467v2\\n   Categories: Primary: cs.LG, Additional: cs.AI\\n   Published: 2025-11-21T18:15:47Z\\n   Preview: We propose a novel framework for persona-based language model system, motivated by the need for personalized AI agents that adapt to individual user preferences.\\n\\n19. REMSA: An LLM Agent for Foundation Model Selection in Remote Sensing\\n   Authors: Binger Chen, Tacettin Emre B\\u00f6k, Behnood Rasti, Volker Markl, Beg\\u00fcm Demir\\n   ID: 2511.17442v1\\n   Categories: Primary: cs.CV, Additional: cs.AI\\n   Published: 2025-11-21T17:41:26Z\\n   Preview: Foundation Models (FMs) are increasingly used in remote sensing (RS) for tasks such as environmental monitoring, disaster assessment, and land-use mapping.\\n\\n20. Beyond Component Strength: Synergistic Integration and Adaptive Calibration in Multi-Agent RAG Systems\\n   Authors: Jithin Krishnan\\n   ID: 2511.21729v1\\n   Categories: Primary: cs.CL, Additional: cs.AI\\n   Published: 2025-11-21T07:53:56Z\\n   Preview: Building reliable retrieval-augmented generation (RAG) systems requires more than adding powerful components; it requires understanding how they interact.\\n\\n21. LLM and Agent-Driven Data Analysis: A Systematic Approach for Enterprise Applications and System-level Deployment\\n   Authors: Xi Wang, Xianyao Ling, Kun Li, Gang Yin, Liang Zhang, Jiang Wu, Annie Wang, Weizhe Wang\\n   ID: 2511.17676v1\\n   Categories: Primary: cs.DB, Additional: cs.AI, cs.CL\\n   Published: 2025-11-21T07:16:31Z\\n   Preview: The rapid progress in Generative AI and Agent technologies is profoundly transforming enterprise data management and analytics.\\n\\n22. SurvAgent: Hierarchical CoT-Enhanced Case Banking and Dichotomy-Based Multi-Agent System for Multimodal Survival Prediction\\n   Authors: Guolin Huang, Wenting Chen, Jiaqi Yang, Xinheng Lyu, Xiaoling Luo, Sen Yang, Xiaohan Xing, Linlin Shen\\n   ID: 2511.16635v1\\n   Categories: Primary: cs.CV, Additional: cs.CL\\n   Published: 2025-11-20T18:41:44Z\\n   Preview: Survival analysis is critical for cancer prognosis and treatment planning, yet existing methods lack the transparency essential for clinical adoption.\\n\\n23. CorrectHDL: Agentic HDL Design with LLMs Leveraging High-Level Synthesis as Reference\\n   Authors: Kangwei Xu, Grace Li Zhang, Ulf Schlichtmann, Bing Li\\n   ID: 2511.16395v1\\n   Categories: Primary: cs.AI, Additional: cs.PL, cs.SE, eess.SY\\n   Published: 2025-11-20T14:13:38Z\\n   Preview: Large Language Models (LLMs) have demonstrated remarkable potential in hardware front-end design using hardware description languages (HDLs).\\n\\n24. Operon: Incremental Construction of Ragged Data via Named Dimensions\\n   Authors: Sungbin Moon, Jiho Park, Suyoung Hwang, Donghyun Koh, Seunghyun Moon, Minhyeong Lee\\n   ID: 2511.16080v1\\n   Categories: Primary: cs.PL, Additional: cs.AI, cs.LG\\n   Published: 2025-11-20T06:16:31Z\\n   Preview: Modern data processing workflows frequently encounter ragged data: collections with variable-length elements that arise naturally in domains like natural language processing, scientific measurements,...\\n\\n25. KRAL: Knowledge and Reasoning Augmented Learning for LLM-assisted Clinical Antimicrobial Therapy\\n   Authors: Zhe Li, Yehan Qiu, Yujie Chen, Xiang Zhou\\n   ID: 2511.15974v4\\n   Categories: Primary: cs.AI\\n   Published: 2025-11-20T02:04:46Z\\n   Preview: Clinical antimicrobial therapy requires the dynamic integration of pathogen profiles,host factors, pharmacological properties of antimicrobials,and the severity of infection.\\n\\n\"]", "tool_call_id": "call_4aVFbtd5SHoo6jnwSWviCHj0"}, {"role": "tool", "content": "[\"Search Results:\\n\\n1. The Personalization Paradox: Semantic Loss vs. Reasoning Gains in Agentic AI Q&A\\n   Authors: Satyajit Movidi, Stephen Russell\\n   ID: 2512.04343v1\\n   Categories: Primary: cs.IR\\n   Published: 2025-12-04T00:12:41Z\\n   Preview: AIVisor, an agentic retrieval-augmented LLM for student advising, was used to examine how personalization affects system performance across multiple evaluation dimensions.\\n\\n2. A Hierarchical Tree-based approach for creating Configurable and Static Deep Research Agent (Static-DRA)\\n   Authors: Saurav Prateek\\n   ID: 2512.03887v2\\n   Categories: Primary: cs.AI\\n   Published: 2025-12-03T15:37:13Z\\n   Preview: The advancement in Large Language Models has driven the creation of complex agentic systems, such as Deep Research Agents (DRAs), to overcome the limitations of static Retrieval Augmented Generation (...\\n\\n3. OmniDexVLG: Learning Dexterous Grasp Generation from Vision Language Model-Guided Grasp Semantics, Taxonomy and Functional Affordance\\n   Authors: Lei Zhang, Diwen Zheng, Kaixin Bai, Zhenshan Bing, Zoltan-Csaba Marton, Zhaopeng Chen, Alois Christian Knoll, Jianwei Zhang\\n   ID: 2512.03874v1\\n   Categories: Primary: cs.RO, Additional: cs.LG\\n   Published: 2025-12-03T15:28:23Z\\n   Preview: Dexterous grasp generation aims to produce grasp poses that align with task requirements and human interpretable grasp semantics.\\n\\n4. Think Before You Drive: World Model-Inspired Multimodal Grounding for Autonomous Vehicles\\n   Authors: Haicheng Liao, Huanming Shen, Bonan Wang, Yongkang Li, Yihong Tang, Chengyue Wang, Dingyi Zhuang, Kehua Chen, Hai Yang, Chengzhong Xu, Zhenning Li\\n   ID: 2512.03454v1\\n   Categories: Primary: cs.CV, Additional: cs.AI\\n   Published: 2025-12-03T05:14:16Z\\n   Preview: Interpreting natural-language commands to localize target objects is critical for autonomous driving (AD).\\n\\n5. Multi-Aspect Knowledge-Enhanced Medical Vision-Language Pretraining with Multi-Agent Data Generation\\n   Authors: Xieji Li, Siyuan Yan, Yingsheng Liu, H. Peter Soyer, Monika Janda, Victoria Mar, Zongyuan Ge\\n   ID: 2512.03445v1\\n   Categories: Primary: cs.CV, Additional: cs.AI\\n   Published: 2025-12-03T04:55:54Z\\n   Preview: Vision-language pretraining (VLP) has emerged as a powerful paradigm in medical image analysis, enabling representation learning from large-scale image-text pairs without relying on expensive manual a...\\n\\n6. HarnessAgent: Scaling Automatic Fuzzing Harness Construction with Tool-Augmented LLM Pipelines\\n   Authors: Kang Yang, Yunhang Zhang, Zichuan Li, GuanHong Tao, Jun Xu, XiaoJing Liao\\n   ID: 2512.03420v1\\n   Categories: Primary: cs.CR, Additional: cs.SE\\n   Published: 2025-12-03T03:55:09Z\\n   Preview: Large language model (LLM)-based techniques have achieved notable progress in generating harnesses for program fuzzing.\\n\\n7. BookRAG: A Hierarchical Structure-aware Index-based Approach for Retrieval-Augmented Generation on Complex Documents\\n   Authors: Shu Wang, Yingli Zhou, Yixiang Fang\\n   ID: 2512.03413v1\\n   Categories: Primary: cs.IR, Additional: cs.AI\\n   Published: 2025-12-03T03:40:49Z\\n   Preview: As an effective method to boost the performance of Large Language Models (LLMs) on the question answering (QA) task, Retrieval-Augmented Generation (RAG), which queries highly relevant information fro...\\n\\n8. WorldMM: Dynamic Multimodal Memory Agent for Long Video Reasoning\\n   Authors: Woongyeong Yeo, Kangsan Kim, Jaehong Yoon, Sung Ju Hwang\\n   ID: 2512.02425v1\\n   Categories: Primary: cs.CV, Additional: cs.AI, cs.CL, cs.IR, cs.LG\\n   Published: 2025-12-02T05:14:52Z\\n   Preview: Recent advances in video large language models have demonstrated strong capabilities in understanding short clips.\\n\\n9. MMAG: Mixed Memory-Augmented Generation for Large Language Models Applications\\n   Authors: Stefano Zeppieri\\n   ID: 2512.01710v2\\n   Categories: Primary: cs.CL, Additional: cs.IR\\n   Published: 2025-12-01T14:16:57Z\\n   Preview: Large Language Models (LLMs) excel at generating coherent text within a single prompt but fall short in sustaining relevance, personalization, and continuity across extended interactions.\\n\\n10. Memory-Integrated Reconfigurable Adapters: A Unified Framework for Settings with Multiple Tasks\\n   Authors: Susmit Agrawal, Krishn Vishwas Kher, Saksham Mittal, Swarnim Maheshwari, Vineeth N. Balasubramanian\\n   ID: 2512.00940v1\\n   Categories: Primary: cs.LG\\n   Published: 2025-11-30T15:45:55Z\\n   Preview: Organisms constantly pivot between tasks such as evading predators, foraging, traversing rugged terrain, and socializing, often within milliseconds.\\n\\n11. ML-Tool-Bench: Tool-Augmented Planning for ML Tasks\\n   Authors: Yaswanth Chittepu, Raghavendra Addanki, Tung Mai, Anup Rao, Branislav Kveton\\n   ID: 2512.00672v1\\n   Categories: Primary: cs.LG, Additional: cs.AI\\n   Published: 2025-11-29T23:59:40Z\\n   Preview: The development of autonomous machine learning (ML) agents capable of end-to-end data science workflows represents a significant frontier in artificial intelligence.\\n\\n12. CogEvo-Edu: Cognitive Evolution Educational Multi-Agent Collaborative System\\n   Authors: Yefeng Wu, Yuchen Song, Yecheng Zhao, Ling Wu, Shan Wan\\n   ID: 2512.00331v1\\n   Categories: Primary: cs.AI, Additional: cs.MA\\n   Published: 2025-11-29T05:41:57Z\\n   Preview: Large language models (LLMs) are increasingly deployed as conversational tutors in STEM education, yet most systems still rely on a single LLM with a static retrieval-augmented generation (RAG) pipeli...\\n\\n13. Words into World: A Task-Adaptive Agent for Language-Guided Spatial Retrieval in AR\\n   Authors: Lixing Guo, Tobias H\\u00f6llerer\\n   ID: 2512.00294v1\\n   Categories: Primary: cs.CV, Additional: cs.AI, cs.HC\\n   Published: 2025-11-29T03:29:15Z\\n   Preview: Traditional augmented reality (AR) systems predominantly rely on fixed class detectors or fiducial markers, limiting their ability to interpret complex, open-vocabulary natural language queries.\\n\\n14. MegaChat: A Synthetic Persian Q&A Dataset for High-Quality Sales Chatbot Evaluation\\n   Authors: Mahdi Rahmani, AmirHossein Saffari, Reyhane Rahmani\\n   ID: 2511.23397v1\\n   Categories: Primary: cs.CL, Additional: cs.AI, cs.MA\\n   Published: 2025-11-28T17:44:20Z\\n   Preview: Small and medium-sized enterprises (SMEs) in Iran increasingly leverage Telegram for sales, where real-time engagement is essential for conversion.\\n\\n15. MCP vs RAG vs NLWeb vs HTML: A Comparison of the Effectiveness and Efficiency of Different Agent Interfaces to the Web (Technical Report)\\n   Authors: Aaron Steiner, Ralph Peeters, Christian Bizer\\n   ID: 2511.23281v1\\n   Categories: Primary: cs.CL\\n   Published: 2025-11-28T15:32:15Z\\n   Preview: Large language model agents are increasingly used to automate web tasks such as product search, offer comparison, and checkout.\\n\\n16. RAG-Empowered LLM-Driven Dynamic Radio Resource Management in Open 6G RAN\\n   Authors: Onur Salan, Burak \\u00c7\\u0131ra\\u011f, Onur Sever, \\u0130brahim H\\u00f6kelek, Ali G\\u00f6r\\u00e7in, Hakan Ali \\u00c7\\u0131rpan\\n   ID: 2511.22933v1\\n   Categories: Primary: eess.SP\\n   Published: 2025-11-28T07:20:25Z\\n   Preview: Implications of the advancements in the area of artificial intelligence to the wireless communications is extremely significant, especially in terms of resource management.\\n\\n17. Resolving Evidence Sparsity: Agentic Context Engineering for Long-Document Understanding\\n   Authors: Keliang Liu, Zizhi Chen, Mingcheng Li, Jingqun Tang, Dingkang Yang, Lihua Zhang\\n   ID: 2511.22850v1\\n   Categories: Primary: cs.CV\\n   Published: 2025-11-28T03:09:40Z\\n   Preview: Document understanding is a long standing practical task.\\n\\n18. Real-Time Procedural Learning From Experience for AI Agents\\n   Authors: Dasheng Bi, Yubin Hu, Mohammed N. Nasir\\n   ID: 2511.22074v2\\n   Categories: Primary: cs.AI, Additional: cs.IR\\n   Published: 2025-11-27T03:51:49Z\\n   Preview: Learning how to do things from trial and error in real time is a hallmark of biological intelligence, yet most LLM-based agents lack mechanisms to acquire procedural knowledge after deployment.\\n\\n19. Chatty-KG: A Multi-Agent AI System for On-Demand Conversational Question Answering over Knowledge Graphs\\n   Authors: Reham Omar, Abdelghny Orogat, Ibrahim Abdelaziz, Omij Mangukiya, Panos Kalnis, Essam Mansour\\n   ID: 2511.20940v1\\n   Categories: Primary: cs.CL\\n   Published: 2025-11-26T00:18:55Z\\n   Preview: Conversational Question Answering over Knowledge Graphs (KGs) combines the factual grounding of KG-based QA with the interactive nature of dialogue systems.\\n\\n20. LLM-Driven Transient Stability Assessment: From Automated Simulation to Neural Architecture Design\\n   Authors: Lianzhe Hu, Yu Wang, Bikash Pal\\n   ID: 2511.20276v1\\n   Categories: Primary: eess.SY\\n   Published: 2025-11-25T13:05:02Z\\n   Preview: This paper presents an LLM-driven, end-to-end workflow that addresses the lack of automation and intelligence in power system transient stability assessment (TSA).\\n\\n21. HKRAG: Holistic Knowledge Retrieval-Augmented Generation Over Visually-Rich Documents\\n   Authors: Anyang Tong, Xiang Niu, ZhiPing Liu, Chang Tian, Yanyan Wei, Zenglin Shi, Meng Wang\\n   ID: 2511.20227v1\\n   Categories: Primary: cs.IR\\n   Published: 2025-11-25T11:59:52Z\\n   Preview: Existing multimodal Retrieval-Augmented Generation (RAG) methods for visually rich documents (VRD) are often biased towards retrieving salient knowledge(e.g., prominent text and visual elements), whil...\\n\\n22. M$^3$Prune: Hierarchical Communication Graph Pruning for Efficient Multi-Modal Multi-Agent Retrieval-Augmented Generation\\n   Authors: Weizi Shao, Taolin Zhang, Zijie Zhou, Chen Chen, Chengyu Wang, Xiaofeng He\\n   ID: 2511.19969v1\\n   Categories: Primary: cs.AI\\n   Published: 2025-11-25T06:29:13Z\\n   Preview: Recent advancements in multi-modal retrieval-augmented generation (mRAG), which enhance multi-modal large language models (MLLMs) with external knowledge, have demonstrated that the collective intelli...\\n\\n23. AppSelectBench: Application-Level Tool Selection Benchmark\\n   Authors: Tianyi Chen, Michael Solodko, Sen Wang, Jongwoo Ko, Junheng Hao, Colby Banbury, Sara Abdali, Saeed Amizadeh, Qing Xiao, Yinheng Li, Tianyu Ding, Kamran Ghasedi Dizaji, Suzhen Zheng, Hao Fan, Justin Wagle, Pashmina Cameron, Kazuhito Koishida\\n   ID: 2511.19957v2\\n   Categories: Primary: cs.CL\\n   Published: 2025-11-25T06:06:17Z\\n   Preview: Computer Using Agents (CUAs) are increasingly equipped with external tools, enabling them to perform complex and realistic tasks.\\n\\n24. Large Language Model Aided Birt-Hogg-Dube Syndrome Diagnosis with Multimodal Retrieval-Augmented Generation\\n   Authors: Haoqing Li, Jun Shi, Xianmeng Chen, Qiwei Jia, Rui Wang, Wei Wei, Hong An, Xiaowen Hu\\n   ID: 2511.19834v1\\n   Categories: Primary: cs.CV\\n   Published: 2025-11-25T01:55:23Z\\n   Preview: Deep learning methods face dual challenges of limited clinical samples and low inter-class differentiation among Diffuse Cystic Lung Diseases (DCLDs) in advancing Birt-Hogg-Dube syndrome (BHD) diagnos...\\n\\n25. NOEM$^{3}$A: A Neuro-Symbolic Ontology-Enhanced Method for Multi-Intent Understanding in Mobile Agents\\n   Authors: Ioannis Tzachristas, Aifen Sui\\n   ID: 2511.19780v1\\n   Categories: Primary: cs.AI\\n   Published: 2025-11-24T23:14:45Z\\n   Preview: We introduce a neuro-symbolic framework for multi-intent understanding in mobile AI agents by integrating a structured intent ontology with compact language models.\\n\\n\"]", "tool_call_id": "call_BiDHkzDWmSntNwWBNIlWKkQ5"}, {"role": "tool", "content": "[\"Search Results:\\n\\n1. ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning\\n   Authors: Shengyuan Ding, Xinyu Fang, Ziyu Liu, Yuhang Zang, Yuhang Cao, Xiangyu Zhao, Haodong Duan, Xiaoyi Dong, Jianze Liang, Bin Wang, Conghui He, Dahua Lin, Jiaqi Wang\\n   ID: 2512.05111v1\\n   Categories: Primary: cs.CV\\n   Published: 2025-12-04T18:59:52Z\\n   Preview: Reward models are critical for aligning vision-language systems with human preferences, yet current approaches suffer from hallucination, weak visual grounding, and an inability to use tools for verif...\\n\\n2. The Personalization Paradox: Semantic Loss vs. Reasoning Gains in Agentic AI Q&A\\n   Authors: Satyajit Movidi, Stephen Russell\\n   ID: 2512.04343v1\\n   Categories: Primary: cs.IR\\n   Published: 2025-12-04T00:12:41Z\\n   Preview: AIVisor, an agentic retrieval-augmented LLM for student advising, was used to examine how personalization affects system performance across multiple evaluation dimensions.\\n\\n3. Evaluating Long-Context Reasoning in LLM-Based WebAgents\\n   Authors: Andy Chung, Yichi Zhang, Kaixiang Lin, Aditya Rawal, Qiaozi Gao, Joyce Chai\\n   ID: 2512.04307v1\\n   Categories: Primary: cs.LG, Additional: cs.AI\\n   Published: 2025-12-03T22:53:10Z\\n   Preview: As large language model (LLM)-based agents become increasingly integrated into daily digital interactions, their ability to reason across long interaction histories becomes crucial for providing perso...\\n\\n4. SQuARE: Structured Query & Adaptive Retrieval Engine For Tabular Formats\\n   Authors: Chinmay Gondhalekar, Urjitkumar Patel, Fang-Chun Yeh\\n   ID: 2512.04292v1\\n   Categories: Primary: cs.CL\\n   Published: 2025-12-03T22:11:45Z\\n   Preview: Accurate question answering over real spreadsheets remains difficult due to multirow headers, merged cells, and unit annotations that disrupt naive chunking, while rigid SQL views fail on files lackin...\\n\\n5. A Hierarchical Tree-based approach for creating Configurable and Static Deep Research Agent (Static-DRA)\\n   Authors: Saurav Prateek\\n   ID: 2512.03887v2\\n   Categories: Primary: cs.AI\\n   Published: 2025-12-03T15:37:13Z\\n   Preview: The advancement in Large Language Models has driven the creation of complex agentic systems, such as Deep Research Agents (DRAs), to overcome the limitations of static Retrieval Augmented Generation (...\\n\\n6. OmniDexVLG: Learning Dexterous Grasp Generation from Vision Language Model-Guided Grasp Semantics, Taxonomy and Functional Affordance\\n   Authors: Lei Zhang, Diwen Zheng, Kaixin Bai, Zhenshan Bing, Zoltan-Csaba Marton, Zhaopeng Chen, Alois Christian Knoll, Jianwei Zhang\\n   ID: 2512.03874v1\\n   Categories: Primary: cs.RO, Additional: cs.LG\\n   Published: 2025-12-03T15:28:23Z\\n   Preview: Dexterous grasp generation aims to produce grasp poses that align with task requirements and human interpretable grasp semantics.\\n\\n7. MemVerse: Multimodal Memory for Lifelong Learning Agents\\n   Authors: Junming Liu, Yifei Sun, Weihua Cheng, Haodong Lei, Yirong Chen, Licheng Wen, Xuemeng Yang, Daocheng Fu, Pinlong Cai, Nianchen Deng, Yi Yu, Shuyue Hu, Botian Shi, Ding Wang\\n   ID: 2512.03627v1\\n   Categories: Primary: cs.AI\\n   Published: 2025-12-03T10:06:14Z\\n   Preview: Despite rapid progress in large-scale language and vision models, AI agents still suffer from a fundamental limitation: they cannot remember.\\n\\n8. Towards Object-centric Understanding for Instructional Videos\\n   Authors: Wenliang Guo, Yu Kong\\n   ID: 2512.03479v1\\n   Categories: Primary: cs.CV\\n   Published: 2025-12-03T06:14:26Z\\n   Preview: Understanding procedural activities is crucial for developing future assistive AI that can reason about complex real-world tasks.\\n\\n9. Think Before You Drive: World Model-Inspired Multimodal Grounding for Autonomous Vehicles\\n   Authors: Haicheng Liao, Huanming Shen, Bonan Wang, Yongkang Li, Yihong Tang, Chengyue Wang, Dingyi Zhuang, Kehua Chen, Hai Yang, Chengzhong Xu, Zhenning Li\\n   ID: 2512.03454v1\\n   Categories: Primary: cs.CV, Additional: cs.AI\\n   Published: 2025-12-03T05:14:16Z\\n   Preview: Interpreting natural-language commands to localize target objects is critical for autonomous driving (AD).\\n\\n10. Multi-Aspect Knowledge-Enhanced Medical Vision-Language Pretraining with Multi-Agent Data Generation\\n   Authors: Xieji Li, Siyuan Yan, Yingsheng Liu, H. Peter Soyer, Monika Janda, Victoria Mar, Zongyuan Ge\\n   ID: 2512.03445v1\\n   Categories: Primary: cs.CV, Additional: cs.AI\\n   Published: 2025-12-03T04:55:54Z\\n   Preview: Vision-language pretraining (VLP) has emerged as a powerful paradigm in medical image analysis, enabling representation learning from large-scale image-text pairs without relying on expensive manual a...\\n\\n11. HarnessAgent: Scaling Automatic Fuzzing Harness Construction with Tool-Augmented LLM Pipelines\\n   Authors: Kang Yang, Yunhang Zhang, Zichuan Li, GuanHong Tao, Jun Xu, XiaoJing Liao\\n   ID: 2512.03420v1\\n   Categories: Primary: cs.CR, Additional: cs.SE\\n   Published: 2025-12-03T03:55:09Z\\n   Preview: Large language model (LLM)-based techniques have achieved notable progress in generating harnesses for program fuzzing.\\n\\n12. BookRAG: A Hierarchical Structure-aware Index-based Approach for Retrieval-Augmented Generation on Complex Documents\\n   Authors: Shu Wang, Yingli Zhou, Yixiang Fang\\n   ID: 2512.03413v1\\n   Categories: Primary: cs.IR, Additional: cs.AI\\n   Published: 2025-12-03T03:40:49Z\\n   Preview: As an effective method to boost the performance of Large Language Models (LLMs) on the question answering (QA) task, Retrieval-Augmented Generation (RAG), which queries highly relevant information fro...\\n\\n13. Spoken Conversational Agents with Large Language Models\\n   Authors: Chao-Han Huck Yang, Andreas Stolcke, Larry Heck\\n   ID: 2512.02593v1\\n   Categories: Primary: cs.CL, Additional: cs.MA, cs.NE, cs.SD, eess.AS\\n   Published: 2025-12-02T10:02:10Z\\n   Preview: Spoken conversational agents are converging toward voice-native LLMs.\\n\\n14. EZYer: A simulacrum of high school with generative agent\\n   Authors: Jinming Yang, Zimu Ji, Weiqi Luo, Gaoxi Wang, Bin Ma, Yueling Deng\\n   ID: 2512.02561v1\\n   Categories: Primary: cs.MA, Additional: cs.AI\\n   Published: 2025-12-02T09:28:59Z\\n   Preview: With the rapid development of the online education and large language model, the existing educational tools still suffer from incomplete service, insufficient performance and weak interactivity in ter...\\n\\n15. In-Context Distillation with Self-Consistency Cascades: A Simple, Training-Free Way to Reduce LLM Agent Costs\\n   Authors: Vishnu Sarukkai, Asanshay Gupta, James Hong, Micha\\u00ebl Gharbi, Kayvon Fatahalian\\n   ID: 2512.02543v1\\n   Categories: Primary: cs.LG\\n   Published: 2025-12-02T09:11:05Z\\n   Preview: The world currently has an abundance of ideas for how to use new LLM agents, and developers seek to rapidly prototype and test new agentic designs.\\n\\n16. Aetheria: A multimodal interpretable content safety framework based on multi-agent debate and collaboration\\n   Authors: Yuxiang He, Jian Zhao, Yuchen Yuan, Tianle Zhang, Wei Cai, Haojie Cheng, Ziyan Shi, Ming Zhu, Haichuan Tang, Chi Zhang, Xuelong Li\\n   ID: 2512.02530v1\\n   Categories: Primary: cs.AI\\n   Published: 2025-12-02T08:49:54Z\\n   Preview: The exponential growth of digital content presents significant challenges for content safety.\\n\\n17. WorldMM: Dynamic Multimodal Memory Agent for Long Video Reasoning\\n   Authors: Woongyeong Yeo, Kangsan Kim, Jaehong Yoon, Sung Ju Hwang\\n   ID: 2512.02425v1\\n   Categories: Primary: cs.CV, Additional: cs.AI, cs.CL, cs.IR, cs.LG\\n   Published: 2025-12-02T05:14:52Z\\n   Preview: Recent advances in video large language models have demonstrated strong capabilities in understanding short clips.\\n\\n18. Skywork-R1V4: Toward Agentic Multimodal Intelligence through Interleaved Thinking with Images and DeepResearch\\n   Authors: Yifan Zhang, Liang Hu, Haofeng Sun, Peiyu Wang, Yichen Wei, Shukang Yin, Jiangbo Pei, Wei Shen, Peng Xia, Yi Peng, Tianyidan Xie, Eric Li, Yang Liu, Xuchen Song, Yahui Zhou\\n   ID: 2512.02395v1\\n   Categories: Primary: cs.CV\\n   Published: 2025-12-02T04:12:57Z\\n   Preview: Despite recent progress in multimodal agentic systems, existing approaches often treat image manipulation and web search as disjoint capabilities, rely heavily on costly reinforcement learning, and la...\\n\\n19. Benchmarking LLM Agents for Wealth-Management Workflows\\n   Authors: Rory Milsom\\n   ID: 2512.02230v1\\n   Categories: Primary: cs.AI\\n   Published: 2025-12-01T21:56:21Z\\n   Preview: Modern work relies on an assortment of digital collaboration tools, yet routine processes continue to suffer from human error and delay.\\n\\n20. How Far Are We from Genuinely Useful Deep Research Agents?\\n   Authors: Dingling Zhang, He Zhu, Jincheng Ren, Kangqi Song, Xinran Zhou, Boyu Feng, Shudong Liu, Jiabin Luo, Weihao Xie, Zhaohui Wang, Tianrui Qin, King Zhu, Yuqing Wang, Qianben Chen, Yuchen Eleanor Jiang, Wei Wang, Jiaheng Liu, Wangchunshu Zhou\\n   ID: 2512.01948v1\\n   Categories: Primary: cs.CL\\n   Published: 2025-12-01T17:58:59Z\\n   Preview: Deep Research Agents (DRAs) aim to automatically produce analyst-level reports through iterative information retrieval and synthesis.\\n\\n21. Agentic Policy Optimization via Instruction-Policy Co-Evolution\\n   Authors: Han Zhou, Xingchen Wan, Ivan Vuli\\u0107, Anna Korhonen\\n   ID: 2512.01945v1\\n   Categories: Primary: cs.LG, Additional: cs.AI, cs.CL\\n   Published: 2025-12-01T17:56:29Z\\n   Preview: Reinforcement Learning with Verifiable Rewards (RLVR) has advanced the reasoning capability of large language models (LLMs), enabling autonomous agents that can conduct effective multi-turn and tool-i...\\n\\n22. MMAG: Mixed Memory-Augmented Generation for Large Language Models Applications\\n   Authors: Stefano Zeppieri\\n   ID: 2512.01710v2\\n   Categories: Primary: cs.CL, Additional: cs.IR\\n   Published: 2025-12-01T14:16:57Z\\n   Preview: Large Language Models (LLMs) excel at generating coherent text within a single prompt but fall short in sustaining relevance, personalization, and continuity across extended interactions.\\n\\n23. Dr.Mi-Bench: A Modular-integrated Benchmark for Scientific Deep Research Agent\\n   Authors: Zhihan Guo, Feiyang Xu, Yifan Li, Muzhi Li, Shuai Zou, Jiele Wu, Han Shi, Haoli Bai, Ho-fung Leung, Irwin King\\n   ID: 2512.00986v1\\n   Categories: Primary: cs.CL\\n   Published: 2025-11-30T17:16:47Z\\n   Preview: The explosive growth in academic literature necessitates automated deep research (DR) agents, yet their evaluation remains a significant challenge.\\n\\n24. Memory-Integrated Reconfigurable Adapters: A Unified Framework for Settings with Multiple Tasks\\n   Authors: Susmit Agrawal, Krishn Vishwas Kher, Saksham Mittal, Swarnim Maheshwari, Vineeth N. Balasubramanian\\n   ID: 2512.00940v1\\n   Categories: Primary: cs.LG\\n   Published: 2025-11-30T15:45:55Z\\n   Preview: Organisms constantly pivot between tasks such as evading predators, foraging, traversing rugged terrain, and socializing, often within milliseconds.\\n\\n25. ML-Tool-Bench: Tool-Augmented Planning for ML Tasks\\n   Authors: Yaswanth Chittepu, Raghavendra Addanki, Tung Mai, Anup Rao, Branislav Kveton\\n   ID: 2512.00672v1\\n   Categories: Primary: cs.LG, Additional: cs.AI\\n   Published: 2025-11-29T23:59:40Z\\n   Preview: The development of autonomous machine learning (ML) agents capable of end-to-end data science workflows represents a significant frontier in artificial intelligence.\\n\\n\"]", "tool_call_id": "call_wxFZHag4GpUWkgJCVqzkRRbO"}, {"role": "assistant", "content": "", "tool_calls": [{"id": "call_Zzixlq2O2pJMx8RRYSbtj7u4", "type": "function", "function": {"name": "get_paper_data", "arguments": "{\"paper_id\": \"2512.01948v1\"}"}}, {"id": "call_RKnT5QfH3UzBlWl7LU8URriM", "type": "function", "function": {"name": "get_paper_data", "arguments": "{\"paper_id\": \"2512.03887v2\"}"}}, {"id": "call_ywekqaFuZapHMGrU0LKPidzx", "type": "function", "function": {"name": "get_paper_data", "arguments": "{\"paper_id\": \"2511.21729v1\"}"}}, {"id": "call_FTkWipLI05JXDaTgYS0xtgbw", "type": "function", "function": {"name": "get_paper_data", "arguments": "{\"paper_id\": \"2511.19969v1\"}"}}, {"id": "call_m5Y8yOLnylwIyOsWTFAZgkoA", "type": "function", "function": {"name": "get_paper_data", "arguments": "{\"paper_id\": \"2512.04343v1\"}"}}, {"id": "call_ybHuNDTnP3LzgUeZNHiX6l8d", "type": "function", "function": {"name": "get_paper_data", "arguments": "{\"paper_id\": \"2511.18177v1\"}"}}, {"id": "call_jSyZSv8qD7K1nh0OfnmB3JiS", "type": "function", "function": {"name": "get_paper_data", "arguments": "{\"paper_id\": \"2511.22850v1\"}"}}, {"id": "call_6Rpnt58eeZnN2OAFlAG3L2Ai", "type": "function", "function": {"name": "get_paper_data", "arguments": "{\"paper_id\": \"2511.17467v2\"}"}}, {"id": "call_bcLbKIRxHOLwSkC7yEVM7BoG", "type": "function", "function": {"name": "get_paper_data", "arguments": "{\"paper_id\": \"2511.20227v1\"}"}}, {"id": "call_pdVijcKxLM4X9KXaYlGq50Pw", "type": "function", "function": {"name": "get_paper_data", "arguments": "{\"paper_id\": \"2512.00986v1\"}"}}]}, {"role": "tool", "content": "[\"Title: How Far Are We from Genuinely Useful Deep Research Agents?\\n\\nMetadata:\\n- Authors: Dingling Zhang, He Zhu, Jincheng Ren, Kangqi Song, Xinran Zhou, Boyu Feng, Shudong Liu, Jiabin Luo, Weihao Xie, Zhaohui Wang, Tianrui Qin, King Zhu, Yuqing Wang, Qianben Chen, Yuchen Eleanor Jiang, Wei Wang, Jiaheng Liu, Wangchunshu Zhou\\n- Published: 2025-12-01T17:58:59Z\\n- Last Updated: 2025-12-01T17:58:59Z\\n- Categories: Primary: cs.CL\\n\\nAbstract:\\nDeep Research Agents (DRAs) aim to automatically produce analyst-level reports through iterative information retrieval and synthesis. However, most existing DRAs were validated on question-answering benchmarks, while research on generating comprehensive reports remains overlooked. Worse, current benchmarks for report synthesis suffer from task complexity and subjective metrics -- this fails to reflect user demands and limits the practical utility of generated reports. To address these gaps, we present Fine-grained DEepResearch bench (FINDER), an enhanced benchmark consisting of 100 human-curated research tasks with 419 structured checklist items that standardize report structure, analytical depth, and factual grounding. Based on approximately 1,000 reports produced by mainstream DRAs, we further propose Deep rEsearch Failure Taxonomy (DEFT), the first failure taxonomy for deep research agents. DEFT contains 14 fine-grained failure modes across reasoning, retrieval, and generation, and is built upon grounded theory with human-LLM co-annotating and inter-annotator reliability validation. Our experimental findings reveal that current DRAs struggle not with task comprehension but with evidence integration, verification, and reasoning-resilient planning.\\n\\nAccess Options:\\n- Abstract page: https://arxiv.org/abs/2512.01948v1\\n- Full text HTML version: https://arxiv.org/html/2512.01948v1\\n- PDF version: https://arxiv.org/pdf/2512.01948v1\\n\\nAdditional Information:\\n- Comment: 34 pages\\n\"]", "tool_call_id": "call_Zzixlq2O2pJMx8RRYSbtj7u4"}, {"role": "tool", "content": "[\"Title: A Hierarchical Tree-based approach for creating Configurable and Static Deep Research Agent (Static-DRA)\\n\\nMetadata:\\n- Authors: Saurav Prateek\\n- Published: 2025-12-03T15:37:13Z\\n- Last Updated: 2025-12-04T05:41:20Z\\n- Categories: Primary: cs.AI\\n\\nAbstract:\\nThe advancement in Large Language Models has driven the creation of complex agentic systems, such as Deep Research Agents (DRAs), to overcome the limitations of static Retrieval Augmented Generation (RAG) pipelines in handling complex, multi-turn research tasks. This paper introduces the Static Deep Research Agent (Static-DRA), a novel solution built upon a configurable and hierarchical Tree-based static workflow. The core contribution is the integration of two user-tunable parameters, Depth and Breadth, which provide granular control over the research intensity. This design allows end-users to consciously balance the desired quality and comprehensiveness of the research report against the associated computational cost of Large Language Model (LLM) interactions. The agent's architecture, comprising Supervisor, Independent, and Worker agents, facilitates effective multi-hop information retrieval and parallel sub-topic investigation. We evaluate the Static-DRA against the established DeepResearch Bench using the RACE (Reference-based Adaptive Criteria-driven Evaluation) framework. Configured with a depth of 2 and a breadth of 5, and powered by the gemini-2.5-pro model, the agent achieved an overall score of 34.72. Our experiments validate that increasing the configured Depth and Breadth parameters results in a more in-depth research process and a correspondingly higher evaluation score. The Static-DRA offers a pragmatic and resource-aware solution, empowering users with transparent control over the deep research process. The entire source code, outputs and benchmark results are open-sourced at https://github.com/SauravP97/Static-Deep-Research/\\n\\nAccess Options:\\n- Abstract page: https://arxiv.org/abs/2512.03887v2\\n- Full text HTML version: https://arxiv.org/html/2512.03887v2\\n- PDF version: https://arxiv.org/pdf/2512.03887v2\\n\\nAdditional Information:\\n- Comment: 16 pages, 6 figures, 4 tables. Code available at: https://github.com/SauravP97/Static-Deep-Research\\n\"]", "tool_call_id": "call_RKnT5QfH3UzBlWl7LU8URriM"}, {"role": "tool", "content": "[\"Title: Beyond Component Strength: Synergistic Integration and Adaptive Calibration in Multi-Agent RAG Systems\\n\\nMetadata:\\n- Authors: Jithin Krishnan\\n- Published: 2025-11-21T07:53:56Z\\n- Last Updated: 2025-11-21T07:53:56Z\\n- Categories: Primary: cs.CL, Additional: cs.AI\\n\\nAbstract:\\nBuilding reliable retrieval-augmented generation (RAG) systems requires more than adding powerful components; it requires understanding how they interact. Using ablation studies on 50 queries (15 answerable, 10 edge cases, and 25 adversarial), we show that enhancements such as hybrid retrieval, ensemble verification, and adaptive thresholding provide almost no benefit when used in isolation, yet together achieve a 95% reduction in abstention (from 40% to 2%) without increasing hallucinations. We also identify a measurement challenge: different verification strategies can behave safely but assign inconsistent labels (for example, \\\"abstained\\\" versus \\\"unsupported\\\"), creating apparent hallucination rates that are actually artifacts of labeling. Our results show that synergistic integration matters more than the strength of any single component, that standardized metrics and labels are essential for correctly interpreting performance, and that adaptive calibration is needed to prevent overconfident over-answering even when retrieval quality is high.\\n\\nAccess Options:\\n- Abstract page: https://arxiv.org/abs/2511.21729v1\\n- Full text HTML version: https://arxiv.org/html/2511.21729v1\\n- PDF version: https://arxiv.org/pdf/2511.21729v1\\n\\nAdditional Information:\\n- Comment: 10 pages, 4 figures\\n\"]", "tool_call_id": "call_ywekqaFuZapHMGrU0LKPidzx"}, {"role": "tool", "content": "[\"Title: M$^3$Prune: Hierarchical Communication Graph Pruning for Efficient Multi-Modal Multi-Agent Retrieval-Augmented Generation\\n\\nMetadata:\\n- Authors: Weizi Shao, Taolin Zhang, Zijie Zhou, Chen Chen, Chengyu Wang, Xiaofeng He\\n- Published: 2025-11-25T06:29:13Z\\n- Last Updated: 2025-11-25T06:29:13Z\\n- Categories: Primary: cs.AI\\n\\nAbstract:\\nRecent advancements in multi-modal retrieval-augmented generation (mRAG), which enhance multi-modal large language models (MLLMs) with external knowledge, have demonstrated that the collective intelligence of multiple agents can significantly outperform a single model through effective communication. Despite impressive performance, existing multi-agent systems inherently incur substantial token overhead and increased computational costs, posing challenges for large-scale deployment. To address these issues, we propose a novel Multi-Modal Multi-agent hierarchical communication graph PRUNING framework, termed M$^3$Prune. Our framework eliminates redundant edges across different modalities, achieving an optimal balance between task performance and token overhead. Specifically, M$^3$Prune first applies intra-modal graph sparsification to textual and visual modalities, identifying the edges most critical for solving the task. Subsequently, we construct a dynamic communication topology using these key edges for inter-modal graph sparsification. Finally, we progressively prune redundant edges to obtain a more efficient and hierarchical topology. Extensive experiments on both general and domain-specific mRAG benchmarks demonstrate that our method consistently outperforms both single-agent and robust multi-agent mRAG systems while significantly reducing token consumption.\\n\\nAccess Options:\\n- Abstract page: https://arxiv.org/abs/2511.19969v1\\n- Full text HTML version: https://arxiv.org/html/2511.19969v1\\n- PDF version: https://arxiv.org/pdf/2511.19969v1\\n\"]", "tool_call_id": "call_FTkWipLI05JXDaTgYS0xtgbw"}, {"role": "tool", "content": "[\"Title: The Personalization Paradox: Semantic Loss vs. Reasoning Gains in Agentic AI Q&A\\n\\nMetadata:\\n- Authors: Satyajit Movidi, Stephen Russell\\n- Published: 2025-12-04T00:12:41Z\\n- Last Updated: 2025-12-04T00:12:41Z\\n- Categories: Primary: cs.IR\\n\\nAbstract:\\nAIVisor, an agentic retrieval-augmented LLM for student advising, was used to examine how personalization affects system performance across multiple evaluation dimensions. Using twelve authentic advising questions intentionally designed to stress lexical precision, we compared ten personalized and non-personalized system configurations and analyzed outcomes with a Linear Mixed-Effects Model across lexical (BLEU, ROUGE-L), semantic (METEOR, BERTScore), and grounding (RAGAS) metrics. Results showed a consistent trade-off: personalization reliably improved reasoning quality and grounding, yet introduced a significant negative interaction on semantic similarity, driven not by poorer answers but by the limits of current metrics, which penalize meaningful personalized deviations from generic reference texts. This reveals a structural flaw in prevailing LLM evaluation methods, which are ill-suited for assessing user-specific responses. The fully integrated personalized configuration produced the highest overall gains, suggesting that personalization can enhance system effectiveness when evaluated with appropriate multidimensional metrics. Overall, the study demonstrates that personalization produces metric-dependent shifts rather than uniform improvements and provides a methodological foundation for more transparent and robust personalization in agentic AI.\\n\\nAccess Options:\\n- Abstract page: https://arxiv.org/abs/2512.04343v1\\n- Full text HTML version: https://arxiv.org/html/2512.04343v1\\n- PDF version: https://arxiv.org/pdf/2512.04343v1\\n\"]", "tool_call_id": "call_m5Y8yOLnylwIyOsWTFAZgkoA"}, {"role": "tool", "content": "[\"Title: Rethinking Retrieval: From Traditional Retrieval Augmented Generation to Agentic and Non-Vector Reasoning Systems in the Financial Domain for Large Language Models\\n\\nMetadata:\\n- Authors: Elias Lumer, Matt Melich, Olivia Zino, Elena Kim, Sara Dieter, Pradeep Honaganahalli Basavaraju, Vamse Kumar Subbiah, James A. Burke, Roberto Hernandez\\n- Published: 2025-11-22T20:06:25Z\\n- Last Updated: 2025-11-22T20:06:25Z\\n- Categories: Primary: cs.CL\\n\\nAbstract:\\nRecent advancements in Retrieval-Augmented Generation (RAG) have enabled Large Language Models to answer financial questions using external knowledge bases of U.S. SEC filings, earnings reports, and regulatory documents. However, existing work lacks systematic comparison of vector-based and non-vector RAG architectures for financial documents, and the empirical impact of advanced RAG techniques on retrieval accuracy, answer quality, latency, and cost remain unclear. We present the first systematic evaluation comparing vector-based agentic RAG using hybrid search and metadata filtering against hierarchical node-based systems that traverse document structure without embeddings. We evaluate two enhancement techniques applied to the vector-based architecture, i) cross-encoder reranking for retrieval precision, and ii) small-to-big chunk retrieval for context completeness. Across 1,200 SEC 10-K, 10-Q, and 8-K filings on a 150-question benchmark, we measure retrieval metrics (MRR, Recall@5), answer quality through LLM-as-a-judge pairwise comparisons, latency, and preprocessing costs. Vector-based agentic RAG achieves a 68% win rate over hierarchical node-based systems with comparable latency (5.2 compared to 5.98 seconds). Cross-encoder reranking achieves a 59% absolute improvement at optimal parameters (10, 5) for MRR@5. Small-to-big retrieval achieves a 65% win rate over baseline chunking with only 0.2 seconds additional latency. Our findings reveal that applying advanced RAG techniques to financial Q&A systems improves retrieval accuracy, answer quality, and has cost-performance tradeoffs to be considered in production.\\n\\nAccess Options:\\n- Abstract page: https://arxiv.org/abs/2511.18177v1\\n- Full text HTML version: https://arxiv.org/html/2511.18177v1\\n- PDF version: https://arxiv.org/pdf/2511.18177v1\\n\\nAdditional Information:\\n- Comment: 8 pages, 2 figures\\n\"]", "tool_call_id": "call_ybHuNDTnP3LzgUeZNHiX6l8d"}, {"role": "tool", "content": "[\"Title: Resolving Evidence Sparsity: Agentic Context Engineering for Long-Document Understanding\\n\\nMetadata:\\n- Authors: Keliang Liu, Zizhi Chen, Mingcheng Li, Jingqun Tang, Dingkang Yang, Lihua Zhang\\n- Published: 2025-11-28T03:09:40Z\\n- Last Updated: 2025-11-28T03:09:40Z\\n- Categories: Primary: cs.CV\\n\\nAbstract:\\nDocument understanding is a long standing practical task. Vision Language Models (VLMs) have gradually become a primary approach in this domain, demonstrating effective performance on single page tasks. However, their effectiveness diminishes when handling long documents. In such scenarios, clues are often scattered across multiple pages and modalities, and redundancy from lengthy inputs can impair the models judgment. While retrieval augmented generation mitigates this issue by filtering for question relevant content, the retrieved results still contain substantial redundancy. To address these limitations, we propose SLEUTH, a multi agent framework. Concretely, SLEUTH orchestrates a retriever and four collaborative agents in a coarse to fine process. The framework identifies key textual and visual clues within the retrieved pages, filters for salient visual evidence such as tables and charts, and analyzes the query to devise a reasoning strategy. It ultimately synthesizes a distilled, evidence dense multimodal context to generate the final prediction. SLEUTH is model agnostic and scalable. When paired with advanced VLM backbones, it consistently improves performance on multiple long document benchmarks, achieving state of the art results. Ablation studies verify each modules effectiveness and confirm the benefits of our hierarchical refinement paradigm.\\n\\nAccess Options:\\n- Abstract page: https://arxiv.org/abs/2511.22850v1\\n- Full text HTML version: https://arxiv.org/html/2511.22850v1\\n- PDF version: https://arxiv.org/pdf/2511.22850v1\\n\"]", "tool_call_id": "call_jSyZSv8qD7K1nh0OfnmB3JiS"}, {"role": "tool", "content": "[\"Title: PersonaAgent with GraphRAG: Community-Aware Knowledge Graphs for Personalized LLM\\n\\nMetadata:\\n- Authors: Siqi Liang, Yudi Zhang, Yue Guo\\n- Published: 2025-11-21T18:15:47Z\\n- Last Updated: 2025-12-02T04:45:19Z\\n- Categories: Primary: cs.LG, Additional: cs.AI\\n\\nAbstract:\\nWe propose a novel framework for persona-based language model system, motivated by the need for personalized AI agents that adapt to individual user preferences. In our approach, the agent embodies the user's \\\"persona\\\" (e.g. user profile or taste) and is powered by a large language model (LLM). To enable the agent to leverage rich contextual information, we introduce a Knowledge-Graph-enhanced Retrieval-Augmented Generation (Graph RAG) mechanism that constructs an LLM-derived graph index of relevant documents and summarizes communities of related information. Our framework generates personalized prompts by combining: (1) a summary of the user's historical behaviors and preferences extracted from the knowledge graph, and (2) relevant global interaction patterns identified through graph-based community detection. This dynamic prompt engineering approach allows the agent to maintain consistent persona-aligned behaviors while benefiting from collective knowledge. On the LaMP benchmark, our method improves news categorization F1 by 11.1%, movie tagging F1 by 56.1%, and reduces product rating MAE by 10.4% over prior methods. Our code is available at https://anonymous.4open.science/r/PersonaAgentwGraphRAG-DE6F\\n\\nAccess Options:\\n- Abstract page: https://arxiv.org/abs/2511.17467v2\\n- Full text HTML version: https://arxiv.org/html/2511.17467v2\\n- PDF version: https://arxiv.org/pdf/2511.17467v2\\n\"]", "tool_call_id": "call_6Rpnt58eeZnN2OAFlAG3L2Ai"}, {"role": "tool", "content": "[\"Title: HKRAG: Holistic Knowledge Retrieval-Augmented Generation Over Visually-Rich Documents\\n\\nMetadata:\\n- Authors: Anyang Tong, Xiang Niu, ZhiPing Liu, Chang Tian, Yanyan Wei, Zenglin Shi, Meng Wang\\n- Published: 2025-11-25T11:59:52Z\\n- Last Updated: 2025-11-25T11:59:52Z\\n- Categories: Primary: cs.IR\\n\\nAbstract:\\nExisting multimodal Retrieval-Augmented Generation (RAG) methods for visually rich documents (VRD) are often biased towards retrieving salient knowledge(e.g., prominent text and visual elements), while largely neglecting the critical fine-print knowledge(e.g., small text, contextual details). This limitation leads to incomplete retrieval and compromises the generator's ability to produce accurate and comprehensive answers. To bridge this gap, we propose HKRAG, a new holistic RAG framework designed to explicitly capture and integrate both knowledge types. Our framework features two key components: (1) a Hybrid Masking-based Holistic Retriever that employs explicit masking strategies to separately model salient and fine-print knowledge, ensuring a query-relevant holistic information retrieval; and (2) an Uncertainty-guided Agentic Generator that dynamically assesses the uncertainty of initial answers and actively decides how to integrate the two distinct knowledge streams for optimal response generation. Extensive experiments on open-domain visual question answering benchmarks show that HKRAG consistently outperforms existing methods in both zero-shot and supervised settings, demonstrating the critical importance of holistic knowledge retrieval for VRD understanding.\\n\\nAccess Options:\\n- Abstract page: https://arxiv.org/abs/2511.20227v1\\n- Full text HTML version: https://arxiv.org/html/2511.20227v1\\n- PDF version: https://arxiv.org/pdf/2511.20227v1\\n\"]", "tool_call_id": "call_bcLbKIRxHOLwSkC7yEVM7BoG"}, {"role": "tool", "content": "[\"Title: Dr.Mi-Bench: A Modular-integrated Benchmark for Scientific Deep Research Agent\\n\\nMetadata:\\n- Authors: Zhihan Guo, Feiyang Xu, Yifan Li, Muzhi Li, Shuai Zou, Jiele Wu, Han Shi, Haoli Bai, Ho-fung Leung, Irwin King\\n- Published: 2025-11-30T17:16:47Z\\n- Last Updated: 2025-11-30T17:16:47Z\\n- Categories: Primary: cs.CL\\n\\nAbstract:\\nThe explosive growth in academic literature necessitates automated deep research (DR) agents, yet their evaluation remains a significant challenge. First, existing benchmarks often focus narrowly on retrieval while neglecting high-level planning and reasoning. Second, existing benchmarks favor general domains over the scientific domains that are the core application for DR agents. To address these gaps, we introduce Dr.Mi-Bench, a Modular-integrated benchmark for scientific DR agents. Grounded in academic literature, our benchmark uses a human-annotated dataset of 200 instances across 10 scientific domains, including both research and review papers. Besides, we also propose a Modular-integrated Evaluation Paradigm for DR Agents (Dr.Mi-Eval), a novel modular-integrated evaluation paradigm, which leverages the rich structure of academic papers to assess the core competencies of planning, retrieval, and reasoning through two complementary modes: an end-to-end evaluation for DR agents and an isolated evaluation for foundational LLMs as potential backbones. Experimental results reveal a fragmented performance landscape: agents exhibit specialized strengths but share critical weaknesses, most notably in performing the multi-source retrieval required for review-style tasks and performing consistently across diverse scientific fields. Moreover, improving high-level planning capability is the crucial factor for unlocking the reasoning potential of foundational LLMs as backbones. By exposing these actionable failure modes, Dr.Mi-Bench provides a diagnostic tool to guide the development of more reliable academic research assistants.\\n\\nAccess Options:\\n- Abstract page: https://arxiv.org/abs/2512.00986v1\\n- Full text HTML version: https://arxiv.org/html/2512.00986v1\\n- PDF version: https://arxiv.org/pdf/2512.00986v1\\n\"]", "tool_call_id": "call_pdVijcKxLM4X9KXaYlGq50Pw"}, {"role": "assistant", "content": "", "tool_calls": [{"id": "call_bGDLswMpW4E1p1IYRyyZXmVV", "type": "function", "function": {"name": "notion-create-pages", "arguments": "{\"pages\":[{\"properties\":{\"title\":\"近期 RAG Agent 论文综述（arXiv，2025-12-06）\"},\"content\":\"# 一句话结论\\nRAG 正在从“静态检索+生成”演进为“具身化、可规划、可验证的 Agent 流水线”。近期工作集中在多智能体协同、个性化与不确定性建模、长文档证据蒸馏、以及工程侧的效率/评测体系，强调“系统集成与自适应校准”比单一组件升级更关键。\\n<empty-block/>\\n## 关键观点与趋势\\n- 多智能体协同 > 单点增强（Krishnan, 2025）\\n  - 混合检索、集成式验证与自适应阈值如果孤立使用收益有限，但“协同集成”可显著降低拒答率且不增幻觉；同时呼吁统一标签与指标，避免评测偏差。\\n- 通信与令牌效率成为多模态/多智能体 RAG 的核心工程挑战（Shao et al., 2025）\\n  - 通过分层通信图剪枝，跨模态/模态内稀疏化，能在保持或提升效果的同时显著降低 token 成本。\\n- 个性化带来“评测悖论”（Movidi & Russell, 2025）\\n  - 个性化 Agent 往往提升推理与事实扎根，但会在 BERTScore/METEOR 等“语义相似度”指标上显得更差，原因是指标惩罚偏离标准答案的“合理个性化”。需采用多维度指标（含 grounding）与以任务为中心的评测。\\n- Agentic Context Engineering 解决长文档“证据稀疏与冗余”难题（Liu et al., 2025）\\n  - 以多代理分工粗到细筛选关键线索、图表与策略，蒸馏出高密度证据上下文，显著提升长文档理解性能。\\n- 垂直领域实践：向“工程化的 Agentic RAG”迈进（Lumer et al., 2025）\\n  - 金融文档中，基于向量的 agentic RAG（+ 交叉编码器重排 + small-to-big 检索）在质量与时延上优于节点/层级式非向量系统，但需权衡预处理与在线成本。\\n- 图谱与社区摘要用于个性化提示（Liang et al., 2025）\\n  - GraphRAG 通过图社区检测和用户行为摘要，生成 persona 对齐提示，实现个性化与群体知识的结合。\\n- 面向视觉富文本的“整体式”检索与不确定性驱动生成（Tong et al., 2025）\\n  - 显性遮罩分别建模“显著知识/细粒度知识”，再由不确定性引导整合，提升多模态 RAG 的完整性与稳健性。\\n- 深度研究代理：评测先行，失败点在“证据整合、验证与抗噪规划”（Zhang et al., 2025；Guo et al., 2025）\\n  - FINDER 与 Dr.Mi-Bench 等基准暴露：多源检索与高层规划是瓶颈；提出 DEFT 失败分类学与模块化评测范式，指向更可靠的研究 Agent 设计。\\n- 可控的层次化静态工作流（Prateek, 2025）\\n  - 通过“深度/广度”参数让用户在研究质量与成本间可调，树状多代理结构便于并行子课题探索。\\n<empty-block/>\\n## 对落地系统的建议\\n- 优先做“系统级集成与校准”：混合检索 + 验证集成 + 动态阈值，而非仅换更强的检索/模型。\\n- 在多智能体/多模态场景下做通信图稀疏化与会话预算控制，显著降本。\\n- 采用多维评测：将 grounding、拒答/不确定性、任务完成度与个性化收益纳入指标；避免只看语义相似度。\\n- 长文档任务采用“证据蒸馏链”：检索→线索抽取→结构化证据→不确定性引导的回答整合。\\n- 垂直领域（如金融）结合重排与分层检索策略，小步验证线上/离线成本与延迟。\\n- 个性化用图谱/社区摘要构造提示，配合不确定性感知与可追溯引用，兼顾体验与合规。\\n<empty-block/>\\n## 参考论文（arXiv）\\n- How Far Are We from Genuinely Useful Deep Research Agents? (Zhang et al., 2025) — https://arxiv.org/abs/2512.01948\\n- A Hierarchical Tree-based approach for creating Configurable and Static Deep Research Agent (Prateek, 2025) — https://arxiv.org/abs/2512.03887\\n- Beyond Component Strength: Synergistic Integration and Adaptive Calibration in Multi-Agent RAG Systems (Krishnan, 2025) — https://arxiv.org/abs/2511.21729\\n- M^3Prune: Hierarchical Communication Graph Pruning for Efficient Multi-Modal Multi-Agent RAG (Shao et al., 2025) — https://arxiv.org/abs/2511.19969\\n- The Personalization Paradox: Semantic Loss vs. Reasoning Gains in Agentic AI Q&A (Movidi & Russell, 2025) — https://arxiv.org/abs/2512.04343\\n- Rethinking Retrieval: From Traditional RAG to Agentic and Non-Vector Reasoning in Finance (Lumer et al., 2025) — https://arxiv.org/abs/2511.18177\\n- Resolving Evidence Sparsity: Agentic Context Engineering for Long-Document Understanding (Liu et al., 2025) — https://arxiv.org/abs/2511.22850\\n- PersonaAgent with GraphRAG: Community-Aware Knowledge Graphs for Personalized LLM (Liang et al., 2025) — https://arxiv.org/abs/2511.17467\\n- HKRAG: Holistic Knowledge RAG Over Visually-Rich Documents (Tong et al., 2025) — https://arxiv.org/abs/2511.20227\\n- Dr.Mi-Bench: A Modular-integrated Benchmark for Scientific Deep Research Agent (Guo et al., 2025) — https://arxiv.org/abs/2512.00986\"}]}"}}]}, {"role": "tool", "content": "[\"{\\\"pages\\\":[{\\\"id\\\":\\\"2c1b93bb-159c-816b-b55d-faeb7d4c49c0\\\",\\\"url\\\":\\\"https://www.notion.so/2c1b93bb159c816bb55dfaeb7d4c49c0\\\",\\\"properties\\\":{\\\"title\\\":\\\"\\u8fd1\\u671f RAG Agent \\u8bba\\u6587\\u7efc\\u8ff0\\uff08arXiv\\uff0c2025-12-06\\uff09\\\"}}]}\"]", "tool_call_id": "call_bGDLswMpW4E1p1IYRyyZXmVV"}], "tool_calls": [], "response_content": "我已：\n- 在 arXiv 上检索并筛选了最近与 RAG Agent/Agentic RAG 相关的论文，阅读其摘要与要点。\n- 将一页中文综述创建到了你的 Notion 中，标题为“近期 RAG Agent 论文综述（arXiv，2025-12-06）”。\n\n你可以在这里查看页面：\nhttps://www.notion.so/2c1b93bb159c816bb55dfaeb7d4c49c0\n\n需要我后续在页面中补充每篇论文的关键表格（方法、数据集、指标、复现实用性），或加入示意图/流程图吗？"}
